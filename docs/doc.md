
Logo
Search
/
Sign In

Getting started
Overview
Models
Transcribe a pre-recorded audio file
Transcribe streaming audio

Build a voice agent
Introducing Slam-1
Build with AssemblyAI
Cookbooks
Integrations
Deployment
Speech-to-text
Pre-recorded audio
Streaming audio
Speech Understanding

LeMUR - LLMs for Speech

Audio Intelligence
Getting started
AssemblyAI Documentation

Copy page

Build with our leading Speech AI models
Industry-leading models on a developer-first API
Your AI product strategy depends on the foundation that powers it. Make sure you build on the best.


Quickstart
Transcribe an audio file
Learn how to transcribe audio files with our SDK

Transcribe streaming audio
Learn how to transcribe live audio from a microphone

Apply LLMs to audio
Learn how to analyze audio content with LLMs
Cookbooks
Get started quickly with our use-case specific cookbooks

Products

Speech-to-Text
Models for converting audio files, video files, and live speech into text.


LeMUR
LeMUR is a framework for applying Large Language Models (LLMs) to spoken data.


Audio Intelligence
Models for interpreting audio for business and personal workflows.

Need help? Talk to our Support team.

·
Join our Discord community
·
Check status page
·
See changelog
Models
Next
Built with



Logo
Search
/
Sign In

Getting started
Overview
Models
Transcribe a pre-recorded audio file
Transcribe streaming audio

Build a voice agent
Introducing Slam-1
Build with AssemblyAI
Cookbooks
Integrations
Deployment
Speech-to-text
Pre-recorded audio
Streaming audio
Speech Understanding

LeMUR - LLMs for Speech

Audio Intelligence
Getting started
Models

Copy page

AssemblyAI offers several state-of-the-art speech recognition models, each optimized for different use cases. Choose the model that best fits your needs based on accuracy, latency, cost, and language requirements.

Slam-1
Highest accuracy for transcribing English pre-recorded audio with fine-tuning support and customization via prompting

Universal
Best for out-of-the-box transcription of pre-recorded audio with multi-lingual support, excellent accuracy, and low latency

Universal-Streaming
Streaming audio transcription optimized for voice agents and real-time applications

Choosing the right model
Slam-1
Best for: English content requiring highest accuracy
Key benefits:
Superior accuracy for English content
Fine-tuning support
Ideal for domain-specific terminology
Universal
Best for: Production-ready transcription out of the box
Key benefits:
Excellent accuracy-to-latency ratio
Multi-language support
No configuration needed
Ideal for conversational intelligence
Breakdown of Universal language support
High accuracy (≤ 10% WER)
Good accuracy (>10% to ≤25% WER)
Moderate accuracy (>25% to ≤50% WER)
Fair accuracy (>50% WER)
Streaming
Best for: Voice agents and real-time voice applications
Key benefits:
~300ms immutable transcripts
Continuous speech recognition
Intelligent endpointing
Ideal for voice agents and interactive applications
Pricing
For detailed pricing information, visit our pricing page.

Model	Price per Hour	Volume discounts
Universal	$0.27/hr	Available
Slam-1	$0.27/hr	Available
Streaming	$0.15/hr	Available
For volume discounts, please reach out to sales@assemblyai.com.

Next steps
For pre-recorded audio, see how to select your model
For real-time transcription, check out our streaming documentation
Built with
Models | AssemblyAI | Documentation


Logo
Search
/
Sign In

Getting started
Overview
Models
Transcribe a pre-recorded audio file
Transcribe streaming audio

Build a voice agent
Introducing Slam-1
Build with AssemblyAI
Cookbooks
Integrations
Deployment
Speech-to-text
Pre-recorded audio
Streaming audio
Speech Understanding

LeMUR - LLMs for Speech

Audio Intelligence
Getting started
Transcribe a pre-recorded audio file


Copy page

Learn how to transcribe and analyze an audio file.
Overview
By the end of this tutorial, you’ll be able to:

Transcribe a pre-recorded audio file.
Select the speech model for your request.
Here’s the full sample code for what you’ll build in this tutorial:

Python SDK
Python
JavaScript SDK
JavaScript
import assemblyai as aai
aai.settings.api_key = "<YOUR_API_KEY>"
transcriber = aai.Transcriber()
# You can use a local filepath:
# audio_file = "./example.mp3"
# Or use a publicly-accessible URL:
audio_file = "https://assembly.ai/sports_injuries.mp3"
config = aai.TranscriptionConfig(speech_model=aai.SpeechModel.slam_1)
transcript = transcriber.transcribe(audio_file, config)
if transcript.status == aai.TranscriptStatus.error:
    print(f"Transcription failed: {transcript.error}")
    exit(1)
print(f" \nFull Transcript: \n\n{transcript.text}")

Before you begin
To complete this tutorial, you need:

If opting to use the Python or Python SDK code, you will need Python installed.
A free AssemblyAI account.
Step 1: Install the necessary libraries
Python SDK
Python
JavaScript SDK
JavaScript
1
Install our Python SDK via pip:

pip install assemblyai

2
Create a new file and import the assemblyai package.

import assemblyai as aai

Step 2: Configure your request
Python SDK
Python
JavaScript SDK
JavaScript
In this step, you ‘ll create an SDK client and configure it to use your API key.

1
Browse to API Keys in your dashboard, and then copy your API key.

2
Create a new Transcriber and configure it to use your API key. Replace YOUR_API_KEY with your copied API key.

aai.settings.api_key = "<YOUR_API_KEY>"
transcriber = aai.Transcriber()

3
Specify a URL to the audio you want to transcribe. The URL needs to be accessible from AssemblyAI’s servers. For a list of supported formats, see FAQ.

audio_file = "https://assembly.ai/sports_injuries.mp3"

Creating self hosted audio URLs
You can use a service like Amazon S3, Google Cloud Storage, or any platform that supports direct file access to generate a shareable audio file URL. Check out this cookbook on how to transcribe from an S3 bucket.

Local audio files
If you want to use a local file, you can also specify a local path, for example:

audio_file = "./example.mp3"

YouTube
YouTube URLs are not supported. If you want to transcribe a YouTube video, you need to download the audio first.

4
Select the speech model: Create a TranscriptionConfig object and set the speech_model to aai.SpeechModel.slam_1.

config = aai.TranscriptionConfig(speech_model=aai.SpeechModel.slam_1)

Selecting the right speech model for your use-case
This example shows our latest prompt-based speech model, Slam-1. You can select the class of models to use in order to make cost-performance tradeoffs best suited for your application. See Models for more information about our available models.

Step 3: Submit for transcription
Python SDK
Python
JavaScript SDK
JavaScript
1
To generate the transcript, pass the audio_file or file to transcriber.transcribe(). This may take a minute while we’re processing the audio.

transcript = transcriber.transcribe(audio_file, config=config)

2
If the transcription failed, the status of the transcription will be set to error. To see why it failed you can print the value of error.

if transcript.error:
  print(transcript.error)
  exit(1)

3
Print the complete transcript.

print(transcript.text)

4
Run the application and wait for it to finish.

Next steps
In this tutorial, you’ve learned how to generate a transcript for an audio file and how to set the speech model.

Want to learn more?

For more ways to analyze your audio data, explore our Audio Intelligence models.
If you want to transcribe audio in real-time, see Transcribe streaming audio from a microphone.
To search, summarize, and ask questions on your transcripts with LLMs, see LeMUR.
Need some help?
If you get stuck, or have any other questions, we’d love to help you out. Contact our support team at support@assemblyai.com or create a support ticket.

Was this page helpful?
Yes
No
Built with
Transcribe a pre-recorded audio file | AssemblyAI | Documentation


Logo
Search
/
Sign In

Getting started
Overview
Models
Transcribe a pre-recorded audio file
Transcribe streaming audio

Build a voice agent
Introducing Slam-1
Build with AssemblyAI
Cookbooks
Integrations
Deployment
Speech-to-text
Pre-recorded audio
Streaming audio
Speech Understanding

LeMUR - LLMs for Speech

Audio Intelligence
Getting started
Transcribe streaming audio

Copy page

Learn how to transcribe streaming audio.
Overview
By the end of this tutorial, you’ll be able to transcribe audio from your microphone.

Supported languages
Streaming Speech-to-Text is only available for English.

Before you begin
To complete this tutorial, you need:

Python or Node.
Here’s the full sample code of what you’ll build in this tutorial:

Python SDK
Python
JavaScript SDK
JavaScript
import logging
from typing import Type
import assemblyai as aai
from assemblyai.streaming.v3 import (
    BeginEvent,
    StreamingClient,
    StreamingClientOptions,
    StreamingError,
    StreamingEvents,
    StreamingParameters,
    StreamingSessionParameters,
    TerminationEvent,
    TurnEvent,
)
api_key = "<YOUR_API_KEY>"
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
def on_begin(self: Type[StreamingClient], event: BeginEvent):
    print(f"Session started: {event.id}")
def on_turn(self: Type[StreamingClient], event: TurnEvent):
    print(f"{event.transcript} ({event.end_of_turn})")
    if event.end_of_turn and not event.turn_is_formatted:
        params = StreamingSessionParameters(
            format_turns=True,
        )
        self.set_params(params)
def on_terminated(self: Type[StreamingClient], event: TerminationEvent):
    print(
        f"Session terminated: {event.audio_duration_seconds} seconds of audio processed"
    )
def on_error(self: Type[StreamingClient], error: StreamingError):
    print(f"Error occurred: {error}")
def main():
    client = StreamingClient(
        StreamingClientOptions(
            api_key=api_key,
            api_host="streaming.assemblyai.com",
        )
    )
    client.on(StreamingEvents.Begin, on_begin)
    client.on(StreamingEvents.Turn, on_turn)
    client.on(StreamingEvents.Termination, on_terminated)
    client.on(StreamingEvents.Error, on_error)
    client.connect(
        StreamingParameters(
            sample_rate=16000,
            format_turns=True,
        )
    )
    try:
        client.stream(
          aai.extras.MicrophoneStream(sample_rate=16000)
        )
    finally:
        client.disconnect(terminate=True)
if __name__ == "__main__":
    main()

Step 1: Install and import dependencies
Python SDK
Python
JavaScript SDK
JavaScript
1
Install the AssemblyAI Python SDK via PIP:

pip install assemblyai

2
Create a file called main.py and import the following packages at the top of your file:

import logging
from typing import Type
import assemblyai as aai
from assemblyai.streaming.v3 import (
    BeginEvent,
    StreamingClient,
    StreamingClientOptions,
    StreamingError,
    StreamingEvents,
    StreamingParameters,
    StreamingSessionParameters,
    TerminationEvent,
    TurnEvent,
)

Step 2: Configure the API key
In this step, you’ll configure your AssemblyAI API key to authenticate your application and enable access to the streaming transcription service.

1
Browse to API Keys in your dashboard, and then copy your API key.

2
Python SDK
Python
JavaScript SDK
JavaScript
Configure the SDK to use your API key. Replace <YOUR_API_KEY> with your copied API key.

api_key = "<YOUR_API_KEY>"

Authenticate with a temporary token
If you need to authenticate on the client, you can avoid exposing your API key by using temporary authentication tokens.

Step 3: Set up audio and websocket configuration
Python SDK
Python
JavaScript SDK
JavaScript
The Python SDK handles audio configuration automatically. You’ll specify the sample rate when connecting to the transcriber. If you don’t set a sample rate, it defaults to 16 kHz.

Step 4: Create event handlers
In this step, you’ll define event handlers to manage the different types of events emitted during the streaming session. The handlers will respond to session lifecycle events, transcription turns, errors, and session termination.

Python SDK
Python
JavaScript SDK
JavaScript
Implement basic event handlers. These handlers let your app respond to key streaming events:

on_begin – Logs when the session starts.
on_turn – Handles each transcription turn and optionally enables formatted turns.
on_terminated – Logs when the session ends and how much audio was processed.
on_error – Captures and prints any errors during streaming.
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
def on_begin(self: Type[StreamingClient], event: BeginEvent):
    print(f"Session started: {event.id}")
def on_turn(self: Type[StreamingClient], event: TurnEvent):
    print(f"{event.transcript} ({event.end_of_turn})")
    if event.end_of_turn and not event.turn_is_formatted:
        params = StreamingSessionParameters(
            format_turns=True,
        )
        self.set_params(params)
def on_terminated(self: Type[StreamingClient], event: TerminationEvent):
    print(
        f"Session terminated: {event.audio_duration_seconds} seconds of audio processed"
    )
def on_error(self: Type[StreamingClient], error: StreamingError):
    print(f"Error occurred: {error}")

Message sequence and turn events
To get a better understanding of the turn event and the message sequences, check out our Message Sequence Breakdown page. This object is how you’ll receive your transcripts.

Step 5: Connect and start transcription
Streaming Speech-to-Text uses WebSockets to stream audio to AssemblyAI. This requires first establishing a connection to the API.

Python SDK
Python
JavaScript SDK
JavaScript
1
In the main function create a client and connect to the streaming service:

    client = StreamingClient(
        StreamingClientOptions(
            api_key=api_key,
            api_host="streaming.assemblyai.com",
        )
    )
    client.on(StreamingEvents.Begin, on_begin)
    client.on(StreamingEvents.Turn, on_turn)
    client.on(StreamingEvents.Termination, on_terminated)
    client.on(StreamingEvents.Error, on_error)
    client.connect(
        StreamingParameters(
            sample_rate=16000,
            format_turns=True,
        )
    )

2
Next, create a microphone stream and begin transcribing audio. Make sure the sample_rate matches the value you specified in the StreamingParameters when initializing the streaming client.

    try:
        client.stream(
          aai.extras.MicrophoneStream(sample_rate=16000)
        )

Step 6: Close the connection
Python SDK
Python
JavaScript SDK
JavaScript
Disconnect the client when you’re done:

    finally:
        client.disconnect(terminate=True)

The connection will also close automatically when you press Ctrl+C. In both cases, the .disconnect() handler will clean up the audio resources.

Note: Pricing is based on session duration so it is very important to close sessions properly to avoid unexpected usage and cost.
Next steps
To learn more about Streaming Speech-to-Text, see the following resources:

Streaming Speech-to-Text
WebSocket API reference
Need some help?
If you get stuck, or have any other questions, we’d love to help you out. Contact our support team at support@assemblyai.com or create a support ticket.

Was this page helpful?
Yes
No
Built with
Transcribe streaming audio | AssemblyAI | Documentation

---
title: Building a Voice Agent with LiveKit and AssemblyAI
description: >-
  Complete guide to building a voice agent from scratch using LiveKit and
  AssemblyAI
---

## Overview

Build a complete voice agent from scratch using LiveKit's Agents framework and AssemblyAI's streaming speech-to-text with advanced turn detection. This guide walks you through creating a fully functional voice agent that can have natural conversations with users in real-time.

**LiveKit** is an open-source platform for building real-time audio and video applications. It provides the infrastructure for live streaming, video conferencing, and interactive media experiences.

**LiveKit Agents** is a framework within LiveKit specifically designed for building AI-powered voice and video agents. It handles the complex orchestration of speech-to-text (STT), large language models (LLM), and text-to-speech (TTS) services, allowing you to focus on your agent's behavior rather than the underlying real-time media processing.

<Note>
  New to LiveKit? This guide assumes no prior LiveKit experience and walks you through everything from setup to deployment.
</Note>

<Card
    title="LiveKit Agents"
    icon={<img src="https://assemblyaiassets.com/images/Livekit.svg" alt="LiveKit logo"/>}
    href="https://docs.livekit.io/agents/"
>
    Learn more about LiveKit's voice agent framework.
</Card>

## What you'll build

By the end of this guide, you'll have:
- A real-time voice agent with sub-second response times
- Natural conversation flow with AssemblyAI's advanced turn detection model
- Voice Activity Detection based interruption handling for responsive interactions
- A complete setup ready for production deployment

## Prerequisites

- Python 3.9 or higher
- A microphone and speakers/headphones
- API keys for AssemblyAI, OpenAI, and Cartesia

## Step 1: Installation

Create a new Python environment and install the required packages:

Create virtual environment
```bash
python -m venv voice-agent
source voice-agent/bin/activate  # On Windows: voice-agent\\Scripts\\activate
```
Install LiveKit Agents with all required plugins and python-dotenv
```bash
pip install "livekit-agents[assemblyai,openai,cartesia,silero]" livekit-plugins-noise-cancellation python-dotenv
```

## Step 2: Get API Keys

To build your voice agent, you'll use:

- **AssemblyAI** for **STT** (speech-to-text)
- **GPT-4o mini** from OpenAI for the **LLM** (language model)
- **Cartesia** for **TTS** (text-to-speech)

You'll need API keys for each:

<Tip>
  **AssemblyAI (STT)**
  - Sign up: [assemblyai.com/signup](https://www.assemblyai.com/dashboard/signup)
  - API Key: [assemblyai.com/api-keys](https://www.assemblyai.com/dashboard/api-keys)
</Tip>

<Tip>
  **OpenAI (LLM)**
  - Sign up: [auth.openai.com/create-account](https://auth.openai.com/create-account)
  - API Key: [platform.openai.com/api-keys](https://platform.openai.com/api-keys)
</Tip>

<Tip>
  **Cartesia (TTS)**
  - Sign up: [cartesia.ai/sign-up](https://play.cartesia.ai/sign-up)
  - API Key: [cartesia.ai/keys](https://play.cartesia.ai/keys)
</Tip>

<Note>
  Looking for alternatives? LiveKit Agents supports multiple TTS and LLM providers.
  Explore the full plugin list [here](https://docs.livekit.io/agents/plugins/).
</Note>

## Step 3: Set Up LiveKit Account

You'll need a LiveKit Cloud account to use the Agents Playground for testing your voice agent.

<Tip>
  **LiveKit Cloud**
  - Sign up: [cloud.livekit.io](https://cloud.livekit.io/)
  - It's free to get started with generous usage limits
</Tip>

**Getting Your LiveKit Credentials:**

1. **Create a Project**: After signing up, you'll see your dashboard. In the bottom left corner, click on "Projects" and create a new project for your voice agent (e.g., "voice-agent-dev").

2. **Get API Credentials**: Once your project is created, navigate to **Settings > API Keys** in your project dashboard. You'll find all three credentials you need in this section:
   - **WebSocket URL** (e.g., `wss://your-project.livekit.cloud`)
   - **API Key** (e.g., `APIaBcDeFgHiJkLm`)
   - **API Secret** (e.g., `SecretXyZ123AbC456DeF789GhI012JkL345MnO678PqR`)

**Project Environments:**

It's recommended to use separate LiveKit projects for different environments:
- **Development**: For local testing and development
- **Staging**: For testing before production
- **Production**: For live user traffic

Each project has unique credentials, ensuring you won't accidentally process real user traffic during development.

**Testing Options:**

LiveKit provides multiple ways to test your voice agent (web SDKs, mobile apps, custom frontends), but for this guide we'll use the [**LiveKit Agents Playground**](https://agents-playground.livekit.io/) - a ready-to-use web interface that makes testing quick and easy.

<Note>
  While you can run LiveKit locally for development, you need LiveKit Cloud credentials to access the Agents Playground for testing. The playground provides an easy web interface to test your voice agent without building a custom frontend.
</Note>

<Note>
  Keep your API credentials secure and never commit them to version control. We'll add them to your `.env` file in the next step.
</Note>

## Step 4: Environment Setup

Create a `.env` file in your project directory with your API keys and LiveKit credentials:

```env
# API Keys
ASSEMBLYAI_API_KEY=your_assemblyai_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
CARTESIA_API_KEY=your_cartesia_api_key_here

# LiveKit Cloud Credentials (from Step 3 - Development Project)
LIVEKIT_URL=wss://your-dev-project.livekit.cloud
LIVEKIT_API_KEY=APIaBcDeFgHiJkLm
LIVEKIT_API_SECRET=SecretXyZ123AbC456DeF789GhI012JkL345MnO678PqR
```

Replace the placeholder values with your actual credentials:
- Use the API keys you obtained in Step 2
- Use the LiveKit credentials from your **development project's** **Settings > API Keys** section

<Note>
  For this tutorial, use your development project credentials. When you're ready for production, create separate projects for staging and production environments, each with their own `.env` configuration.
</Note>

## Step 5: Create Your Voice Agent

Create a file called `voice_agent.py`:

```python
from dotenv import load_dotenv
from livekit import agents
from livekit.agents import AgentSession, Agent, RoomInputOptions
from livekit.plugins import (
    openai,
    cartesia,
    assemblyai,
    noise_cancellation,
    silero,
)

load_dotenv()


class Assistant(Agent):
    def __init__(self) -> None:
        super().__init__(instructions="You are a helpful AI assistant. Keep your responses concise and conversational. You're having a real-time voice conversation, so avoid long explanations unless asked.")


async def entrypoint(ctx: agents.JobContext):
    await ctx.connect()

    # Create agent session with AssemblyAI's advanced turn detection
    session = AgentSession(
        stt=assemblyai.STT(
            end_of_turn_confidence_threshold=0.7,
            min_end_of_turn_silence_when_confident=160,
            max_turn_silence=2400,
        ),
        llm=openai.LLM(
            model="gpt-4o-mini",
            temperature=0.7,
        ),
        tts=cartesia.TTS(),
        vad=silero.VAD.load(),  # Voice Activity Detection for interruptions
        turn_detection="stt",  # Use AssemblyAI's STT-based turn detection
    )

    await session.start(
        room=ctx.room,
        agent=Assistant(),
        room_input_options=RoomInputOptions(
            noise_cancellation=noise_cancellation.BVC(),
        ),
    )

    # Greet the user when they join
    await session.generate_reply(
        instructions="Greet the user and offer your assistance."
    )


if __name__ == "__main__":
    agents.cli.run_app(agents.WorkerOptions(entrypoint_fnc=entrypoint))
```

## Step 6: Run Your Voice Agent

Start your voice agent:

```bash
python voice_agent.py dev
```

You should see output indicating the agent is ready and waiting for connections.

## Step 7: Test Your Agent

Open [LiveKit Agents Playground](https://agents-playground.livekit.io/) in your browser:

1. **Select your project**: If you're logged in to your LiveKit Cloud account, you should see your project listed. Click on your development project.

2. **Connect**: Click "Connect" and start talking to your voice agent!

<Note>
  Make sure you're logged in to the same LiveKit Cloud account where you created your project. The playground will automatically use your project's credentials.
</Note>

## Configuration

### Turn Detection (Key Feature)

AssemblyAI's new turn detection model was built specifically for voice agents and you can tweak it to fit your use case. It processes both audio and linguistic information to determine an end of turn confidence score on every inference, and if that confidence score is past the set threshold, it triggers end of turn.

This custom model was designed to address 2 major issues with voice agents. With traditional VAD (voice activity detection) approaches based on silence alone, there are situations where the agent wouldn't wait for a user to finish their turn even if the audio data suggested it. Think of a situation like "My credit card number is____" - if someone is looking that up, traditional VAD may not wait for the user, where our turn detection model is far better in these situations.

Additionally, in situations where we are certain that the user is done speaking like "What is my credit score?", a high end of turn confidence is returned, greater than the threshold, and triggering end of turn, allowing for minimal turnaround latency in those scenarios.

```python
# STT-based turn detection (recommended)
turn_detection="stt"

stt=assemblyai.STT(
    end_of_turn_confidence_threshold=0.7,
    min_end_of_turn_silence_when_confident=160,  # in ms
    max_turn_silence=2400,  # in ms
)
```

**Parameter tuning:**
- **end_of_turn_confidence_threshold**: Raise or lower the threshold based on how confident you'd like us to be before triggering end of turn based on confidence score
- **min_end_of_turn_silence_when_confident**: Increase or decrease the amount of time we wait to trigger end of turn when confident
- **max_turn_silence**: Lower or raise the amount of time needed to trigger end of turn when end of turn isn't triggered by a high confidence score

<Tip>
  You can also set `turn_detection="vad"` if you'd like turn detection to be based on Silero VAD instead of our advanced turn detection model.
</Tip>

For more information, see our [Universal-Streaming end-of-turn detection guide](https://www.assemblyai.com/docs/speech-to-text/universal-streaming#end-of-turn-detection) and [message-by-message breakdown](https://www.assemblyai.com/docs/speech-to-text/universal-streaming/message-sequence).

**Customizing your agent:**

Modify the agent's instructions to change behavior:

```python
class Assistant(Agent):
    def __init__(self) -> None:
        super().__init__(instructions="You are a friendly customer service representative. Help users with technical questions and maintain a professional tone. Keep responses under 30 seconds.")
```

For customizing Cartesia TTS, see the [LiveKit Cartesia TTS documentation](https://docs.livekit.io/agents/integrations/tts/cartesia/).

For configuring OpenAI models and parameters, see the [LiveKit OpenAI LLM documentation](https://docs.livekit.io/agents/integrations/llm/openai/).

For complete details on all AssemblyAI parameters, see the [AssemblyAI Universal-Streaming API Reference](https://www.assemblyai.com/docs/speech-to-text/universal-streaming#reference).

<Tip>
  **Want to build more advanced voice agents?** LiveKit has tons of guides on building custom voice agents and workflows. Explore their [documentation](https://docs.livekit.io/agents/build/) to see how you can do things like [function calling with tools](https://docs.livekit.io/agents/build/tools/) or [integrate RAG](https://docs.livekit.io/agents/build/external-data/).
</Tip>

## Production Deployment

When your voice agent is working well in development, it's time to deploy it to production. Head over to the [LiveKit Agents Deployment Guide](https://docs.livekit.io/agents/ops/deployment/) to learn about deploying your voice agent to production using LiveKit Cloud.

Since LiveKit is open source, you have the flexibility to deploy the infrastructure yourself for complete control over your deployment and data. However, LiveKit Cloud makes it really easy to deploy with managed infrastructure that includes auto-scaling, global edge locations for low latency, built-in monitoring and analytics, zero-downtime deployments, and enterprise-grade security.

For production, make sure to create separate LiveKit projects for staging and production environments, each with their own API credentials and `.env` configurations to keep your environments isolated.

## More Questions?

If you get stuck, or have any other questions, we'd love to help you out. Contact our support team at support@assemblyai.com or create a [support ticket](https://www.assemblyai.com/contact/support).


---
title: Building a Voice Agent with Pipecat and AssemblyAI
description: >-
  Complete guide to building a voice agent from scratch using Pipecat and
  AssemblyAI
---

## Overview

Build a complete voice agent from scratch using Pipecat and AssemblyAI's streaming speech-to-text with advanced turn detection. This guide walks you through creating a fully functional voice agent that can have natural conversations with users in real-time.

**Pipecat** is an open-source framework for building conversational AI applications, created by Daily.co. Daily.co is a platform that provides real-time video and audio APIs, and they built Pipecat to make it easier for developers to create AI-powered voice experiences. Pipecat provides the infrastructure for real-time voice interactions, handling the complex orchestration of speech-to-text (STT), large language models (LLM), and text-to-speech (TTS) services.

**Pipecat** specializes in building AI-powered voice agents and handles the real-time media processing pipeline, allowing you to focus on your agent's behavior rather than the underlying technical complexity.

<Note>
  New to Pipecat? This guide assumes no prior Pipecat experience and walks you through everything from setup to deployment.
</Note>

<Card 
    title="Pipecat" 
    icon={<img src="https://assemblyaiassets.com/images/Pipecat.svg" alt="Pipecat logo"/>} 
    href="https://docs.pipecat.ai/"
>
    Learn more about Pipecat's voice agent framework.
</Card>

## YouTube video guide

<Frame>
<iframe
  width="100%"
  height="350px"
  src="https://www.youtube.com/embed/h5E2GMudS5E?si=-ssi5hmNYX4bPRUb"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerpolicy="strict-origin-when-cross-origin"
  allowfullscreen
></iframe>
</Frame>

## What you'll build

By the end of this guide, you'll have:
- A real-time voice agent with sub-second response times
- Natural conversation flow with AssemblyAI's advanced turn detection model
- Voice Activity Detection based interruption handling for responsive interactions
- A complete setup ready for production deployment

## Prerequisites

- Python 3.10 or higher
- A microphone and speakers/headphones
- API keys for AssemblyAI, OpenAI, and Cartesia

## Step 1: Installation

Create a new Python environment and install the required packages:

Create virtual environment
```bash
python -m venv voice-agent
source voice-agent/bin/activate  # On Windows: voice-agent\\Scripts\\activate
```
Install Pipecat with all required plugins
```bash
pip install "pipecat-ai[assemblyai,openai,cartesia,silero,daily,webrtc]" python-dotenv fastapi uvicorn pipecat-ai-small-webrtc-prebuilt
```

Download the Pipecat run helper file:

<Tabs>
<Tab title="Linux/macOS">
```bash
curl -O https://raw.githubusercontent.com/pipecat-ai/pipecat/9f223442c2799d22aac8a552c0af1d0ae7ff42c2/src/pipecat/examples/run.py
```
</Tab>
<Tab title="Windows (PowerShell)">
```bash
curl.exe -O https://raw.githubusercontent.com/pipecat-ai/pipecat/9f223442c2799d22aac8a552c0af1d0ae7ff42c2/src/pipecat/examples/run.py
```
</Tab>
</Tabs>

## Step 2: Get API Keys

To build your voice agent, you'll use:

- **AssemblyAI** for **STT** (speech-to-text)
- **GPT-4o mini** from OpenAI for the **LLM** (language model)
- **Cartesia** for **TTS** (text-to-speech)

You'll need API keys for each:

<Tip>
  **AssemblyAI (STT)**  
  - Sign up: [assemblyai.com/signup](https://www.assemblyai.com/dashboard/signup)  
  - API Key: [assemblyai.com/api-keys](https://www.assemblyai.com/dashboard/api-keys)
</Tip>

<Tip>
  **OpenAI (LLM)**  
  - Sign up: [auth.openai.com/create-account](https://auth.openai.com/create-account)  
  - API Key: [platform.openai.com/api-keys](https://platform.openai.com/api-keys)
</Tip>

<Tip>
  **Cartesia (TTS)**  
  - Sign up: [cartesia.ai/sign-up](https://play.cartesia.ai/sign-up)  
  - API Key: [cartesia.ai/keys](https://play.cartesia.ai/keys)
</Tip>

<Note>
  Looking for alternatives? Pipecat supports multiple TTS and LLM providers.  
  Explore the full plugin list [here](https://docs.pipecat.ai/server/services/).
</Note>

## Step 3: Environment Setup

Create a `.env` file in your project directory with your API keys:

```env
# API Keys
ASSEMBLYAI_API_KEY=your_assemblyai_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
CARTESIA_API_KEY=your_cartesia_api_key_here
```

Replace the placeholder values with your actual API keys from Step 2.

## Step 4: Create Your Voice Agent

Pipecat has many examples for testing, which you can see in their [quickstart guide](https://docs.pipecat.ai/getting-started/quickstart) and all their examples on [GitHub](https://github.com/pipecat-ai/pipecat/tree/main/examples).

You can utilize AssemblyAI within any of these Pipecat examples as long as you use us in the Pipecat Pipeline (Pipecat's code system for voice agents) shown here:

```python
from pipecat.services.assemblyai.stt import AssemblyAISTTService

# Configure service
stt = AssemblyAISTTService(
    connection_params=AssemblyAIConnectionParams(
        end_of_turn_confidence_threshold=0.7,
        min_end_of_turn_silence_when_confident=160,
        max_turn_silence=2400,
    ),
    api_key=os.getenv("ASSEMBLYAI_API_KEY"),
    vad_force_turn_endpoint=False
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    llm,
    ...
])
```

Below is a code snippet that uses our STT within Pipecat's [basic voice agent example](https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07-interruptible.py) structure. This example is great because it comes with a great UI.

Please create a file called `voice_agent.py` and copy and paste this code:

```python
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import argparse
import os

from dotenv import load_dotenv
from loguru import logger

from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.assemblyai.stt import AssemblyAISTTService, AssemblyAIConnectionParams
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.network.fastapi_websocket import FastAPIWebsocketParams
from pipecat.transports.services.daily import DailyParams

load_dotenv(override=True)

# We store functions so objects (e.g. SileroVADAnalyzer) don't get
# instantiated. The function will be called when the desired transport gets
# selected.
transport_params = {
    "daily": lambda: DailyParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
    "twilio": lambda: FastAPIWebsocketParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
    "webrtc": lambda: TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
}


async def run_example(transport: BaseTransport, _: argparse.Namespace, handle_sigint: bool):
    logger.info(f"Starting bot")

    # Configure AssemblyAI STT with advanced turn detection
    stt = AssemblyAISTTService(
        api_key=os.getenv("ASSEMBLYAI_API_KEY"),
        vad_force_turn_endpoint=False,
        connection_params=AssemblyAIConnectionParams(
            end_of_turn_confidence_threshold=0.7,
            min_end_of_turn_silence_when_confident=160,
            max_turn_silence=2400,
        )
    )

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    messages = [
        {
            "role": "system",
            "content": "You are a helpful LLM in a WebRTC call. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way.",
        },
    ]

    context = OpenAILLMContext(messages)
    context_aggregator = llm.create_context_aggregator(context)

    pipeline = Pipeline(
        [
            transport.input(),  # Transport user input
            stt,
            context_aggregator.user(),  # User responses
            llm,  # LLM
            tts,  # TTS
            transport.output(),  # Transport bot output
            context_aggregator.assistant(),  # Assistant spoken responses
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            allow_interruptions=True,
            enable_metrics=True,
            enable_usage_metrics=True,
            report_only_initial_ttfb=True,
        ),
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info(f"Client connected")
        # Kick off the conversation.
        messages.append({"role": "system", "content": "Please introduce yourself to the user."})
        await task.queue_frames([context_aggregator.user().get_context_frame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info(f"Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=handle_sigint)

    await runner.run(task)


if __name__ == "__main__":
    from pipecat.examples.run import main

    main(run_example, transport_params=transport_params)
```

## Step 5: Run Your Voice Agent

Start your voice agent:

```bash
python voice_agent.py
```

You'll see a URL (typically `http://localhost:7860`) in the console output. Open this URL in your browser to test your voice agent!

## Step 6: Test Your Voice Agent

1. **Open the provided URL** in your browser (usually `http://localhost:7860`)
2. **Allow microphone access** when prompted by your browser
3. **Click "Connect"** to join the session
4. **Start talking** to your voice agent and have a conversation!

The web interface is provided automatically by Pipecat's example framework, making testing simple and straightforward.

<Note>
  For more testing examples and advanced configurations, check out the [Pipecat examples repository](https://github.com/pipecat-ai/pipecat/tree/main/examples) and the [quickstart guide](https://docs.pipecat.ai/getting-started/quickstart).
</Note>

## Configuration

### Turn Detection (Key Feature)

AssemblyAI's new turn detection model was built specifically for voice agents and you can tweak it to fit your use case. It processes both audio and linguistic information to determine an end of turn confidence score on every inference, and if that confidence score is past the set threshold, it triggers end of turn.

This custom model was designed to address 2 major issues with voice agents. With traditional VAD (voice activity detection) approaches based on silence alone, there are situations where the agent wouldn't wait for a user to finish their turn even if the audio data suggested it. Think of a situation like "My credit card number is____" - if someone is looking that up, traditional VAD may not wait for the user, where our turn detection model is far better in these situations.

Additionally, in situations where we are certain that the user is done speaking like "What is my credit score?", a high end of turn confidence is returned, greater than the threshold, and triggering end of turn, allowing for minimal turnaround latency in those scenarios.

```python
stt = AssemblyAISTTService(
    api_key=os.getenv("ASSEMBLYAI_API_KEY"),
    vad_force_turn_endpoint=False,  # Use AssemblyAI's STT-based turn detection
    connection_params=AssemblyAIConnectionParams(
        end_of_turn_confidence_threshold=0.7,
        min_end_of_turn_silence_when_confident=160,  # in ms
        max_turn_silence=2400,  # in ms
    )
)
```

**Parameter tuning:**
- **end_of_turn_confidence_threshold**: Raise or lower the threshold based on how confident you'd like us to be before triggering end of turn based on confidence score
- **min_end_of_turn_silence_when_confident**: Increase or decrease the amount of time we wait to trigger end of turn when confident
- **max_turn_silence**: Lower or raise the amount of time needed to trigger end of turn when end of turn isn't triggered by a high confidence score

<Tip>
  You can also set `vad_force_turn_endpoint=True` if you'd like turn detection to be based on VAD instead of our advanced turn detection model.
</Tip>

For more information, see our [Universal-Streaming end-of-turn detection guide](https://www.assemblyai.com/docs/speech-to-text/universal-streaming#end-of-turn-detection) and [message-by-message breakdown](https://www.assemblyai.com/docs/speech-to-text/universal-streaming/message-sequence).

**Customizing your agent:**

Modify the system message in the messages array:

```python
messages = [
    {
        "role": "system",
        "content": "You are a friendly customer service representative. Help users with technical questions and maintain a professional tone. Keep responses under 30 seconds.",
    },
]
```

This replaces the existing system message in the code. The current example uses:
```python
messages = [
    {
        "role": "system",
        "content": "You are a helpful LLM in a WebRTC call. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way.",
    },
]
```

For customizing Cartesia TTS voices, see the [Pipecat Cartesia TTS documentation](https://docs.pipecat.ai/server/services/tts/cartesia).

For configuring OpenAI models and parameters, see the [Pipecat OpenAI LLM documentation](https://docs.pipecat.ai/server/services/llm/openai).

For complete details on all AssemblyAI parameters, see the [AssemblyAI Universal-Streaming API Reference](https://www.assemblyai.com/docs/speech-to-text/universal-streaming#reference).

<Tip>
  **Want to build more advanced voice agents?** Pipecat has guides on building custom voice agents and workflows. Explore their [documentation](https://docs.pipecat.ai/) to see how you can add custom processors, integrate with databases, or build multi-modal experiences.
</Tip>

## Production Deployment

When your voice agent is working well in development, it's time to deploy it to production. Pipecat has comprehensive deployment guides to help you get started - check out their [deployment overview](https://docs.pipecat.ai/guides/deployment/overview) for detailed instructions.

**Pipecat Cloud**  

Pipecat offers a managed cloud service for deploying voice agents at scale. Check out [Pipecat Cloud](https://pipecat.ai/cloud) for managed infrastructure that handles scaling, monitoring, and deployment automatically.

**Self-Hosting**  

Since Pipecat is open source, you have complete control over your deployment and data. You can deploy on AWS, Google Cloud, Azure, or any other hosting platform. Consider using containerization with Docker for easier deployment and scaling.

## More Questions?

If you get stuck, or have any other questions, we'd love to help you out. Contact our support team at support@assemblyai.com or create a [support ticket](https://www.assemblyai.com/contact/support).


---
title: Vapi
description: Vapi voice agent integration
---

## Overview

Vapi is a developer platform for building voice AI agents, they handle the complex backend of voice agents for you so you can focus on creating great voice experiences. In this guide, we'll show you how to integrate AssemblyAI's streaming speech-to-text model into your Vapi voice agent.

<Card
  title="Vapi"
  icon={
    <img src="https://assemblyaiassets.com/images/Vapi.svg" alt="Vapi logo" />
  }
  href="https://docs.vapi.ai/providers/transcriber/assembly-ai"
>
  View Vapi's AssemblyAI STT provider documentation.
</Card>

## Quick start

<Steps>
    **Head to the "Assistants" tab in your Vapi dashboard.**

    <Frame>
        <img src="file:ca47a3c9-e4ac-401a-bda7-c4e76fee79ec" />
    </Frame>

    **Click on your assistant and then the "Transcriber" tab.**

    <Frame>
        <img src="file:7ca3f3dd-db10-463a-afb4-f603cc3dd92b" />
    </Frame>

    **Select "Assembly AI" on the Provider dropdown and make sure the "Universal Streaming API" option is toggled on.**

    <Frame>
        <img src="file:5903fffc-4278-4106-9f7c-0564c1fae228" />
    </Frame>

</Steps>

Your voice agent now uses **AssemblyAI** for speech-to-text (STT) processing.

<Info>
  New to Vapi? Visit the [Quickstart
  Guide](https://docs.vapi.ai/quickstart/introduction) to explore various
  example voice agent workflows. For the easiest way to test a voice agent,
  follow this [simple phone-based guide](https://docs.vapi.ai/quickstart/phone).
  Vapi offers a wide range of example workflows to get you up and running
  quickly.
</Info>

## Recommended settings for optimal latency

### Transcriber settings

For best latency, Format Turns should be turned off (adds about 50ms of delay), and Universal Streaming should be enabled.

<Frame>
  <img src="file:6adf3371-3418-4cbd-bdab-ab5a2b2877f5" />
</Frame>

### Advanced > Start speaking plan

For start speaking plan, raise the `Wait seconds` to prevent false starts. `Smart Endpointing` should be turned off.

<Frame>
  <img src="file:5a05687c-a06d-4c0c-ad12-dc21b9efba34" />
</Frame>

### Advanced > Stop speaking plan

For stop speaking plan, lower `Voice seconds` to allow for faster interruptions.

<Frame>
  <img src="file:8c05935c-87dc-4cf4-abbd-e87a623866b7" />
</Frame>


---
title: Introducing Slam-1
subtitle: Learn how to transcribe pre-recorded audio using Slam-1.
hide-nav-links: true
description: Learn how to transcribe prerecorded audio using Slam-1.
---

## Overview

Slam-1 is our new Speech Language Model that combines LLM architecture with ASR encoders for superior speech-to-text transcription. This model delivers unprecedented accuracy through its understanding of context and semantic meaning. Check out our [Slam-1 blog post](https://www.assemblyai.com/blog/slam-1-public-beta) to learn more about this new model!

<Info>
  Slam-1 is currently only supported for English on both the US and EU endpoint.
</Info>

## Quick Start

Slam-1 is available in beta through our standard API endpoint. To use it:

1. Make requests to https://api.assemblyai.com/v2/transcript with your API key
2. Add the `speech_model` parameter with value "slam-1"

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

data = {
    "audio_url": "https://assembly.ai/sports_injuries.mp3",
    "speech_model": "slam-1"
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

if response.status_code != 200:
    print(f"Error: {response.status_code}, Response: {response.text}")
    response.raise_for_status()

transcript_response = response.json()
transcript_id = transcript_response["id"]
polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()
    if transcript["status"] == "completed":
        print(transcript["text"])
        break
    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")
    else:
        time.sleep(3)
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from "axios";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const data = {
  audio_url: "https://assembly.ai/sports_injuries.mp3",
  speech_model: "slam-1",
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

</Tab>
</Tabs>

<Note title="Local audio files">

The above code example shows how to transcribe a file that is available via URL. If you would like to work with local files, see our [API Reference](https://www.assemblyai.com/docs/api-reference/files/upload) for more information on transcribing local files.

</Note>

## Fine-tuning Slam-1

Improve transcription accuracy by leveraging Slam-1's contextual understanding capabilities by prompting the model with certain words or phrases that are likely to appear frequently in your audio file.

Rather than simply increasing the likelihood of detecting specific words, Slam-1's multi-modal architecture actually understands the semantic meaning and context of the terminology you provide, enhancing transcription quality not just of the exact terms you specify, but also related terminology, variations, and contextually similar phrases.

Provide up to 1000 domain-specific words or phrases (maximum 6 words per phrase) that may appear in your audio using the optional `keyterms_prompt` parameter:

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

data = {
    "audio_url": "https://assembly.ai/sports_injuries.mp3",
    "speech_model": "slam-1",
    "keyterms_prompt": ['differential diagnosis', 'hypertension', 'Wellbutrin XL 150mg']
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

if response.status_code != 200:
    print(f"Error: {response.status_code}, Response: {response.text}")
    response.raise_for_status()

transcript_response = response.json()
transcript_id = transcript_response["id"]
polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()
    if transcript["status"] == "completed":
        print(transcript["text"])
        break
    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")
    else:
        time.sleep(3)
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from "axios";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const data = {
  audio_url: "https://assembly.ai/sports_injuries.mp3",
  speech_model: "slam-1",
  keyterms_prompt: [
    "differential diagnosis",
    "hypertension",
    "Wellbutrin XL 150mg",
  ],
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

</Tab>
</Tabs>

<Note title="Keyword count limits">

While we support up to 1000 key words and phrases, actual capacity may be lower due to internal tokenization and implementation constraints.

Key points to remember:

- Each word in a multi-word phrase counts towards the 1000 keyword limit
- Capitalization affects capacity (uppercase tokens consume more than lowercase)
- Longer words consume more capacity than shorter words

For optimal results, use shorter phrases when possible and be mindful of your total token count when approaching the keyword limit.

</Note>

Here is an example of what a `keyterms_prompt` list might look like for a transcription of a professional therapy session for a patient named Jane Doe, who is being treated for anxiety and depression:

```txt wordWrap
["Jane Doe", "cognitive behavioral therapy", "major depressive disorder", "generalized anxiety disorder", "ADHD", "trauma-informed care", "Lexapro 10mg", "psychosocial assessment", "therapeutic alliance", "emotional dysregulation", "GAD-7", "PHQ-9", "Citalopram 20mg", "Lorazepam 2mg"]
```

## Feedback

We welcome your feedback on Slam-1 during this beta period. Share your thoughts by emailing our Support team at support@assemblyai.com.

---
title: Cookbooks
description: Explore use cases with easy-to-follow guides.
layout: overview
---

<style>
  {`
  /* Styles for custom list items */
  .no-bullets {
    list-style-type: none;
    padding-left: 0;
    margin-left: 0;
  }

  .no-bullets li {
    margin: 0;
    padding-left: 0;

    &:before {
      display: none !important;
    }
  }

  .link-cta {
    text-decoration: none;
    padding: 8px 12px;
    margin-left: -12px;
    border-radius: 0.5rem;

    p {
      margin: 0 !important;s
    }
  }

  .link-cta:hover {
    background-color: var(--accent-3);
  }

  .link-cta svg {
    transition: transform 0.2s ease;
  }

  .link-cta:hover svg {
    transform: translateX(4px);
  }

  .card-stack {
    position: relative;
    width: 100%;
    margin-bottom: 20px;
  }

  body#fern-docs .card-stack > div {
    height: 100%;
  }

  .card-stack .fern-card {
    background: var(--grayscale-1);
    height: 100%;
    overflow: hidden;
  }

  .card-stack .fern-card > div {
    height: 100%;
  }

   .card-stack .fern-card > div > div {
    height: 100%;
  }

  .card-stack .fern-card > div > div > div:nth-child(2) {
    height: 100%;
  }

  /* Animation to change the color of the CTA text when hovering over the featured cookbook */
  .card-stack .fern-card:hover .cta {
    color: rgba(var(--accent-aaa),var(--tw-text-opacity,1));
    transition: color 0.2s ease;
  }

  .card-stack .fern-card .cta svg {
    transform: translateX(0);
    transition: color 0.2s ease, transform 0.2s ease;
  }

  .card-stack .fern-card:hover .cta svg {
    background-color: rgba(var(--accent-aaa),var(--tw-text-opacity,1));
    transform: translateX(4px);
  }

  /* Styling for the card stack effect behind each featured cookbook */
  .card-stack::before,
  .card-stack::after {
    content: '';
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    border-radius: 1rem;
    z-index: -1;
    border: 1px solid var(--border);
    background-color: var(--grayscale-1);
  }

  .card-stack::before {
    transform: translateY(8px) translateX(-8px);
    border-radius: 1.25rem;
  }

  .card-stack::after {
    transform: translateY(4px) translateX(-4px);
  }

  /* Animation to collapse card stack when hovering over the featured cookbook */
  .card-stack:hover::before,
  .card-stack:hover::after {
    transform: translateY(0) translateX(0);
    transition: transform 0.2s ease;
  }

  .bg-noise {
    background-position: 0 0;
    background-size: 5rem 10rem;
    background-image: url(https://fern-image-hosting.s3.us-east-1.amazonaws.com/6789751936e0c7bd371e8cf2_noise-full.webp);
    z-index: -1;
    opacity: 0.7;
    justify-content: center;
    align-items: center;
    width: calc(100% + 5rem);
    height: calc(100% + 5rem);
    display: flex;
    position: absolute;
    top: 0;
    left: 0;
  }
`}
</style>

Discover what you can build with the AssemblyAI API with real-world code
examples.

For examples using the API without SDKs see [API guides](#api-guides).

## **Popular**

<CardGroup cols={3}>
  <div className="card-stack">
    <Card href="guides/transcribe_youtube_videos">
      <div className="bg-noise"></div>
      <div className="flex flex-col h-full justify-between">
        <div className="flex flex-col gap-4 mb-auto">
          <div className="flex flex-row gap-2">
            <img
              src="file:311b1fba-02d1-479a-a452-0d35b2bc9b37"
              alt="Speech to Text icon"
              className="mx-0"
            />
            <p>Speech-to-Text</p>
          </div>
          <div className="t-default text-base font-medium">
            How to Transcribe YouTube Videos
          </div>
        </div>
        <div className="flex flex-row gap-2 items-center mt-4 cta">
          <p>Build this</p>
          <Icon icon="duotone arrow-right" />
        </div>
      </div>
    </Card>
  </div>

<div className="card-stack">
  <Card href="guides/subtitles" className="cookbook-card">
    <div className="bg-noise"></div>
    <div className="flex flex-col h-full justify-between">
      <div className="flex flex-col gap-4 mb-auto">
        <div className="flex flex-row gap-2">
          <img
            src="file:311b1fba-02d1-479a-a452-0d35b2bc9b37"
            alt="Speech to Text icon"
            className="mx-0"
          />
          <p>Speech-to-Text</p>
        </div>
        <div className="t-default text-base font-medium">
          How to Generate Subtitles for Videos
        </div>
      </div>
      <div className="flex flex-row gap-2 items-center mt-4 cta">
        <p>Build this</p>
        <Icon icon="duotone arrow-right" />
      </div>
    </div>
  </Card>
</div>

<div className="card-stack">
  <Card href="guides/speaker-identification" className="cookbook-card">
    <div className="bg-noise"></div>
    <div className="flex flex-col h-full justify-between">
      <div className="flex flex-col gap-4 mb-auto">
        <div className="flex flex-row gap-2">
          <img
            src="file:82c18742-25c0-44d8-87a3-c6118bc1ec7f"
            alt="Audio Intelligence icon"
            className="mx-0"
          />
          <p>Audio Intelligence</p>
        </div>
        <div className="t-default text-base font-medium">
          Identify Speaker Names From the Transcript Using LeMUR
        </div>
      </div>
      <div className="flex flex-row gap-2 items-center mt-4 cta">
        <p>Build this</p>
        <Icon icon="duotone arrow-right" />
      </div>
    </div>
  </Card>
</div>

<div className="card-stack">
  <Card href="guides/real_time_lemur" className="cookbook-card">
    <div className="bg-noise"></div>
    <div className="flex flex-col h-full justify-between">
      <div className="flex flex-col gap-4 mb-auto">
        <div className="flex flex-row gap-2">
          <img
            src="file:8a7b98ce-e936-4242-a322-bda28613d559"
            alt="LeMUR icon"
            className="mx-0"
          />
          <p>LeMUR</p>
        </div>
        <div className="t-default text-base font-medium">
          Use LeMUR with Streaming Speech-to-Text
        </div>
      </div>
      <div className="flex flex-row gap-2 items-center mt-4 cta">
        <p>Build this</p>
        <Icon icon="duotone arrow-right" />
      </div>
    </div>
  </Card>
</div>

<div className="card-stack">
  <Card href="guides/input-text-chapters" className="cookbook-card">
    <div className="bg-noise"></div>
    <div className="flex flex-col h-full justify-between">
      <div className="flex flex-col gap-4 mb-auto">
        <div className="flex flex-row gap-2">
          <img
            src="file:311b1fba-02d1-479a-a452-0d35b2bc9b37"
            alt="Speech to Text icon"
            className="mx-0"
          />
          <p>Speech-to-Text</p>
        </div>
        <div className="t-default text-base font-medium">
          Create Chapter Summaries with LeMURs Custom Text Input Parameter
        </div>
      </div>
      <div className="flex flex-row gap-2 items-center mt-4 cta">
        <p>Build this</p>
        <Icon icon="duotone arrow-right" />
      </div>
    </div>
  </Card>
</div>

  <div className="card-stack">
    <Card href="guides/custom-vocab-lemur" className="cookbook-card">
      <div className="bg-noise"></div>
      <div className="flex flex-col h-full justify-between">
        <div className="flex flex-col gap-4 mb-auto">
          <div className="flex flex-row gap-2">
            <img
              src="file:8a7b98ce-e936-4242-a322-bda28613d559"
              alt="LeMUR icon"
              className="mx-0"
            />
            <p>LeMUR</p>
          </div>
          <div className="t-default text-base font-medium">
            Use LeMUR to Boost Custom Vocabulary List
          </div>
        </div>
        <div className="flex flex-row gap-2 items-center mt-4 cta">
          <p>Build this</p>
          <Icon icon="duotone arrow-right" />
        </div>
      </div>
    </Card>
  </div>
</CardGroup>

## By product

<div className="guides-list">
  <ul className="no-bullets">
    <li>
      <a
        href="guides/audio-intelligence"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Audio Intelligence{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/lemur"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/streaming"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Streaming{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/pre-recorded-audio"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Pre-recorded{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
  </ul>
</div>

## **All**

<div className="guides-list">
  <ul className="no-bullets">
    <li>
      <a
        href="guides/entity_redaction"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Redact PII Entities in a Transcript with Entity Detection{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/automatic-language-detection-separate"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Use Automatic Language Detection as a Separate Step From Transcription{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/automatic-language-detection-route-default-language"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Route to Default Language if Language Confidence is Low{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/automatic-language-detection-route-nano-model"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Route to Nano Speech Model if Detected Language Confidence is Low{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/gradio-frontend"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Build a UI for Transcription with Gradio and Python{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/talk-listen-ratio"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Calculate the Talk / Listen Ratio of Speakers{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/translate_transcripts"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Translate AssemblyAI Transcripts Into Other Languages Using Commercial
        Models{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/audio-duration-fix"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Correct Audio Duration Discrepancies with Multi-Tool Validation and
        Transcoding{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/speaker_timeline"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Plot A Speaker Timeline with Matplotlib{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/subtitle_creation_by_word_count"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Create Custom Length Subtitles{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/speaker_labelled_subtitles"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Create Subtitles with Speaker Labels{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/detecting-low-confidence-words"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Detect Low Confidence Words in a Transcript{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/do-more-with-sdk"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Do More With Our SDKs{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/subtitles"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Generate Subtitles for Videos{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/common_errors_and_solutions"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Troubleshoot Common Errors{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/how_to_use_the_eu_endpoint"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Select The EU Region for EU Data Residency{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/transcribe_youtube_videos"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Get YouTube Video Transcripts with yt-dlp{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/retry-server-error"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Implement Retry Server Error Logic{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/retry-upload-error"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Implement Retry Upload Error Logic{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/identify_duplicate_channels"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Identify Duplicate Dual Channel Files{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/make-speaker-labels"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Iterate over Speaker Labels with Make.com{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/schedule_delete"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Schedule a DELETE request with AssemblyAI and EasyCron{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/speaker-diarization-with-async-chunking"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Use Speaker Diarization with Async Chunking{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/titanet-speaker-identification"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Setup A Speaker Identification System using Pinecone & Nvidia TitaNet{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/sdk-node-batch"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Transcribe Multiple Files Simultaneously Using the Node SDK{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/transcribe_from_s3"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Transcribe from an S3 Bucket{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/translate_subtitles"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Translate an AssemblyAI Subtitle Transcript{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/transcribing-github-files"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Transcribe GitHub Files{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/transcribing-google-drive-file"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Transcribe a Google Drive File{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/Use_AssemblyAI_with_Pyannote_to_generate_custom_Speaker_Labels"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Generate Custom Speaker Labels with Pyannote{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/batch_transcription"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Transcribe Multiple Files Simultaneously Using the Python SDK{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/aws_to_aai"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Migration guide: AWS Transcribe to AssemblyAI{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/dg_to_aai"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Migration guide: Deepgram to AssemblyAI{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/google_to_aai"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Migration guide: Google Speech-to-Text to AssemblyAI{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/oai_to_aai"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Migration guide: OpenAI to AssemblyAI{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/custom-vocab-lemur"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Boost Transcription Accuracy with LeMUR (LeMUR Custom Vocab){" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/input-text-chapters"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Create Chapter Summaries with LeMURs Custom Text Input Parameter{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/custom-topic-tags"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Custom Topic Tags{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/counting-tokens"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Estimate Input Token Costs for LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/dialogue-data"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Extract Dialogue Data with LeMUR and JSON{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/transcript-citations"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Extract Quotes with Timestamps Using LeMUR + Semantic Search{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/timestamped-transcripts"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Extract Transcript Quotes with LeMURs Custom Text Input Parameter{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/past-response-prompts"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Pass Context from Previous LeMUR Requests{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/sales-playbook"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Implement a Sales Playbook Using LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/task-endpoint-ai-coach"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Setup an AI Coach with LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/task-endpoint-action-items"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Generate Action Items with LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/task-endpoint-custom-summary"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Generate A Custom Summary Using LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/task-endpoint-structured-QA"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Prompt A Structured Q&A Response Using LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/call-sentiment-analysis"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Analyze The Sentiment Of A Customer Call using LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/phone-call-segmentation"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Segment A Phone Call using LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/input-text-speaker-labels"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Process Speaker Labels with LeMURs Custom Text Input Parameter{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/soap-note-generation"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Generate SOAP Notes using LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/lemur-transcript-citations"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Generate Transcript Citations using LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/speaker-identification"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Identify Speaker Names From the Transcript Using LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/noise_reduction_streaming"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Apply Noise Reduction to Audio for Streaming Speech-to-Text{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/real_time_translation"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Translate Streaming STT Transcripts with LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/terminate_realtime_programmatically"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Terminate Streaming Session After Inactivity{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/transcribe_system_audio"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Transcribe System Audio in Real-Time (macOS){" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/real_time_lemur"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Use LeMUR with Streaming Speech-to-Text (STT){" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/lemur-pii-redaction"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Redact PII from Text Using LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/traditional_simplified_chinese"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Transform Chinese transcripts into Simplified or Traditional Text{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/v2_to_v3_migration"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Migrate from Streaming v2 to Streaming v3{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="guides/streaming_transcribe_audio_file"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Transcribe Audio Files with Streaming Speech-to-Text{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
  </ul>
</div>

## API Guides

<div className="guides-list">
  <ul className="no-bullets">
    <li className="flex items-center gap-2">
      <a
        href="/docs/guides/transcribing-an-audio-file"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Transcribing an audio file{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>{" "}
      <span className="speech-to-text-tag">Speech-to-Text</span>
    </li>
    <li className="flex items-center gap-2">
      <a
        href="/docs/guides/generating-subtitles-for-videos"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Generating subtitles for videos{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>{" "}
      <span className="speech-to-text-tag">Speech-to-Text</span>
    </li>
    <li className="flex items-center gap-2">
      <a
        href="/docs/guides/real-time-streaming-transcription"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Using real-time streaming{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>{" "}
      <span className="streaming-tag">Streaming</span>
    </li>
    <li className="flex items-center gap-2">
      <a
        href="/docs/guides/summarizing-virtual-meetings"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Summarizing virtual meetings{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>{" "}
      <span className="audio-intelligence-tag">Audio Intelligence</span>
    </li>
    <li className="flex items-center gap-2">
      <a
        href="/docs/guides/identifying-hate-speech-in-audio-or-video-files"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Identifying hate speech in audio or video files{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>{" "}
      <span className="audio-intelligence-tag">Audio Intelligence</span>
    </li>
    <li className="flex items-center gap-2">
      <a
        href="/docs/guides/identifying-highlights-in-audio-or-video-files"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Identifying highlights in audio and video files{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>{" "}
      <span className="audio-intelligence-tag">Audio Intelligence</span>
    </li>
    <li className="flex items-center gap-2">
      <a
        href="/docs/guides/identifying-speakers-in-audio-recordings"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Identifying speakers in audio recordings{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>{" "}
      <span className="audio-intelligence-tag">Audio Intelligence</span>
    </li>
    <li className="flex items-center gap-2">
      <a
        href="/docs/guides/creating-summarized-chapters-from-podcasts"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Creating summarized chapters from podcasts{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>{" "}
      <span className="audio-intelligence-tag">Audio Intelligence</span>
    </li>
    <li className="flex items-center gap-2">
      <a
        href="/docs/guides/generate-meeting-action-items-with-lemur"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Generate meeting action items with LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>{" "}
      <span className="lemur-tag">LeMUR</span>
    </li>
    <li className="flex items-center gap-2">
      <a
        href="/docs/guides/automatic-language-detection-workflow"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Separating automatic language detection from transcription{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>{" "}
      <span className="speech-to-text-tag">Speech-to-Text</span>
    </li>
    <li className="flex items-center gap-2">
      <a
        href="/docs/guides/process-speaker-labels-with-lemur"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Process speaker labels with LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>{" "}
      <span className="lemur-tag">LeMUR</span>
    </li>
  </ul>
</div>


<style>
  {`
    /* Styles for custom list items */
    .no-bullets {
      list-style-type: none;
      padding-left: 0;
      margin-left: 0;
    }

    .no-bullets li {
      margin: 0;
      padding-left: 0;

      &:before {
        display: none !important;
      }
    }

    .link-cta {
      text-decoration: none;
      padding: 8px 12px;
      margin-left: -12px;
      border-radius: 0.5rem;

      p {
        margin: 0 !important;s
      }
    }

    .link-cta:hover {
      background-color: var(--accent-3);
    }

    .link-cta svg {
      transition: transform 0.2s ease;
    }

    .link-cta:hover svg {
      transform: translateX(4px);
    }
`}
</style>

The Speech Recognition model enables you to transcribe spoken words into written text and is the foundation of all AssemblyAI products.
On top of the core transcription, you can enable other features and models, such as [Speaker Diarization](https://www.assemblyai.com/docs/speech-to-text/speaker-diarization), by adding additional parameters to the same transcription request.

## Basic transcription workflows

<div className="guides-list">
  <ul className="no-bullets">
    <li>
      <a
        href="transcribe_youtube_videos"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Get YouTube Video Transcripts with yt-dlp{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="gradio-frontend"Speech-to-Text
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Build a UI for Transcription with Gradio and Python{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="detecting-low-confidence-words"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Detect Low Confidence Words in a Transcript{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="how_to_use_the_eu_endpoint"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Select The EU Region for EU Data Residency{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>

  </ul>
</div>

## Batch transcription

<div className="guides-list">
  <ul className="no-bullets">
    <li>
      <a
        href="batch_transcription"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Transcribe Multiple Files Simultaneously Using the Python SDK{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="sdk-node-batch"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Transcribe Multiple Files Simultaneously Using the Node SDK{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
  </ul>
</div>

## Hosting audio files

<div className="guides-list">
  <ul className="no-bullets">
    <li>
      <a
        href="transcribe_from_s3"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Transcribe from an S3 Bucket{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="transcribing-google-drive-file"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Transcribe a Google Drive File{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="transcribing-github-files"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Transcribe GitHub Files{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
  </ul>
</div>

## Speaker Labels

<div className="guides-list">
  <ul className="no-bullets">
    <li className="flex items-center gap-2">
      <a
        href="/docs/guides/identifying-speakers-in-audio-recordings"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Identifying speakers in audio recordings{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>{" "}
    </li>
    <li>
      <a
        href="make-speaker-labels"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Iterate over Speaker Labels with Make.com{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="talk-listen-ratio"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Calculate the Talk / Listen Ratio of Speakers{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="speaker_timeline"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Plot A Speaker Timeline with Matplotlib{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="Use_AssemblyAI_with_Pyannote_to_generate_custom_Speaker_Labels"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Generate Custom Speaker Labels with Pyannote{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="speaker-diarization-with-async-chunking"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Use Speaker Diarization with Async Chunking{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="titanet-speaker-identification"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Setup A Speaker Identification System using Pinecone & Nvidia TitaNet{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
  </ul>
</div>

## Automatic Language Detection

<div className="guides-list">
  <ul className="no-bullets">
    <li className="flex items-center gap-2">
      <a
        href="/docs/guides/automatic-language-detection-workflow"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Separating automatic language detection from transcription{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>{" "}
    </li>
    <li>
      <a
        href="automatic-language-detection-separate"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Use Automatic Language Detection as a Separate Step From Transcription{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="automatic-language-detection-route-default-language"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Route to Default Language if Language Confidence is Low{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="automatic-language-detection-route-nano-model"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Route to Nano Speech Model if Detected Language Confidence is Low{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
  </ul>
</div>

## Subtitles

<div className="guides-list">
  <ul className="no-bullets">
    <li>
      <a
        href="subtitle_creation_by_word_count"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Create Custom Length Subtitles{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="speaker_labelled_subtitles"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Create Subtitles with Speaker Labels{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="subtitles"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Generate Subtitles for Videos{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="translate_subtitles"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Translate an AssemblyAI Subtitle Transcript{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
  </ul>
</div>

## Delete Transcripts

<div className="guides-list">
  <ul className="no-bullets">
    <li>
      <a
        href="schedule_delete"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Schedule a DELETE request with AssemblyAI and EasyCron{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
  </ul>
</div>

## Error Handling and Audio File Fixes

<div className="guides-list">
  <ul className="no-bullets">
    <li>
      <a
        href="common_errors_and_solutions"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Troubleshoot Common Errors{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="retry-server-error"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Implement Retry Server Error Logic{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="retry-upload-error"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Implement Retry Upload Error Logic{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="identify_duplicate_channels"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Identify Duplicate Dual Channel Files{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="audio-duration-fix"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Correct Audio Duration Discrepancies with Multi-Tool Validation and
        Transcoding{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
  </ul>
</div>

## Translation

<div className="guides-list">
  <ul className="no-bullets">
    <li>
      <a
        href="translate_subtitles"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Translate an AssemblyAI Subtitle Transcript{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="translate_transcripts"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Translate AssemblyAI Transcripts Into Other Languages Using Commercial
        Models{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="traditional_simplified_chinese"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Transform Chinese transcripts into Simplified or Traditional Text{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
  </ul>
</div>

## Migration Guides

<div className="guides-list">
  <ul className="no-bullets">
    <li>
      <a
        href="aws_to_aai"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Migration guide: AWS Transcribe to AssemblyAI{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="dg_to_aai"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Migration guide: Deepgram to AssemblyAI{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="google_to_aai"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Migration guide: Google Speech-to-Text to AssemblyAI{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="oai_to_aai"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Migration guide: OpenAI to AssemblyAI{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
  </ul>
</div>

## Do More with our SDKS

<div className="guides-list">
  <ul className="no-bullets">
    <li>
      <a
        href="do-more-with-sdk"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Do More With Our SDKs{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
  </ul>
</div>


<style>
  {`
  /* Styles for custom list items */
  .no-bullets {
    list-style-type: none;
    padding-left: 0;
    margin-left: 0;
  }
  
  .no-bullets li {
    margin: 0;
    padding-left: 0;

    &:before {
      display: none !important;
    }
  }
  
  .link-cta {
    text-decoration: none;
    padding: 8px 12px;
    margin-left: -12px;
    border-radius: 0.5rem;

    p {
      margin: 0 !important;s
    }
  }
  
  .link-cta:hover {
    background-color: var(--accent-3);
  }
  
  .link-cta svg {
    transition: transform 0.2s ease;
  }
  
  .link-cta:hover svg {
    transform: translateX(4px);
  }
`}
</style>

Use our Audio Intelligence models to analyze audio and gain additional insights beyond speech to text.

<div className="guides-list">
  <ul className="no-bullets">
    <li className="flex items-center gap-2">
      <a
        href="/docs/guides/identifying-hate-speech-in-audio-or-video-files"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Identifying hate speech in audio or video files{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>{" "}
    </li>
    <li>
      <a
        href="entity_redaction"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Redact PII Entities in a Transcript with Entity Detection{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li className="flex items-center gap-2">
      <a
        href="/docs/guides/creating-summarized-chapters-from-podcasts"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Creating summarized chapters from podcasts{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>{" "}
    </li>
    <li className="flex items-center gap-2">
      <a
        href="/docs/guides/summarizing-virtual-meetings"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Summarizing virtual meetings{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>{" "}
    </li>
    <li className="flex items-center gap-2">
      <a
        href="/docs/guides/identifying-highlights-in-audio-or-video-files"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Identifying highlights in audio and video files{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>{" "}
    </li>
  </ul>
</div>


<style>
  {`
    /* Styles for custom list items */
    .no-bullets {
      list-style-type: none;
      padding-left: 0;
      margin-left: 0;
    }
    
    .no-bullets li {
      margin: 0;
      padding-left: 0;

      &:before {
        display: none !important;
      }
    }
    
    .link-cta {
      text-decoration: none;
      padding: 8px 12px;
      margin-left: -12px;
      border-radius: 0.5rem;

      p {
        margin: 0 !important;s
      }
    }
    
    .link-cta:hover {
      background-color: var(--accent-3);
    }
    
    .link-cta svg {
      transition: transform 0.2s ease;
    }
    
    .link-cta:hover svg {
      transform: translateX(4px);
    }
`}
</style>

AssemblyAI's Streaming Speech-to-Text (STT) allows you to transcribe live audio streams with high accuracy and low latency. By streaming your audio data to our secure WebSocket API, you can receive transcripts back within a few hundred milliseconds.

## Basic Streaming Workflows

<div className="guides-list">
  <ul className="no-bullets">
    <li>
      <a
        href="/docs/speech-to-text/universal-streaming"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Using real-time streaming{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>{" "}
    </li>
    <li>
      <a
        href="transcribe_system_audio"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Transcribe System Audio in Real-Time (macOS){" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="terminate_realtime_programmatically"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Terminate Streaming Session After Inactivity{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="v2_to_v3_migration"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Migrate from Streaming v2 to Streaming v3 (Python){" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="v2_to_v3_migration_js" 
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Migrate from Streaming v2 to Streaming v3 (JavaScript){" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
  </ul>
</div>

## Streaming for Front-End Applications

<div className="guides-list">
  <ul className="no-bullets">
    <li>
      <a
        href="https://github.com/AssemblyAI-Community/streaming-api-nextjs"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Next.js Example Using Streaming STT{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="https://github.com/AssemblyAI/realtime-transcription-browser-js-example"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Vanilla JavaScript Front-End Examples{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
  </ul>
</div>

## Streaming with LeMUR

<div className="guides-list">
  <ul className="no-bullets">
    <li>
      <a
        href="real_time_lemur"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Use LeMUR with Streaming Speech-to-Text (STT){" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="real_time_translation"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Translate Streaming STT Transcripts with LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
  </ul>
</div>

## Use Case Specific Streaming Workflows

<div className="guides-list">
  <ul className="no-bullets">
    <li>
      <a
        href="noise_reduction_streaming"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Apply Noise Reduction to Audio for Streaming Speech-to-Text{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="streaming_transcribe_audio_file"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Transcribe Audio Files with Streaming Speech-to-Text{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
  </ul>
</div>


<style>
  {`
    /* Styles for custom list items */
    .no-bullets {
      list-style-type: none;
      padding-left: 0;
      margin-left: 0;
    }
    
    .no-bullets li {
      margin: 0;
      padding-left: 0;

      &:before {
        display: none !important;
      }
    }
    
    .link-cta {
      text-decoration: none;
      padding: 8px 12px;
      margin-left: -12px;
      border-radius: 0.5rem;

      p {
        margin: 0 !important;s
      }
    }
    
    .link-cta:hover {
      background-color: var(--accent-3);
    }
    
    .link-cta svg {
      transition: transform 0.2s ease;
    }
    
    .link-cta:hover svg {
      transform: translateX(4px);
    }
`}
</style>

Apply Large Language Models to spoken data. A Large Language Model (LLM) is a machine learning model that uses natural language processing (NLP) to generate text. LeMUR is a framework that lets you apply LLMs to audio transcripts, for example to ask questions about a call, or to summarize a meeting.

## Basic LeMUR Workflows

<div className="guides-list">
  <ul className="no-bullets">
    <li>
      <a
        href="task-endpoint-ai-coach"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Setup an AI Coach with LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="task-endpoint-action-items"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Generate Action Items with LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="task-endpoint-structured-QA"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Prompt A Structured Q&A Response Using LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="counting-tokens"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Estimate Input Token Costs for LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
  </ul>
</div>

## Analyze Speakers with LeMUR

<div className="guides-list">
  <ul className="no-bullets">
    <li>
      <a
        href="input-text-speaker-labels"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Process Speaker Labels with LeMURs Custom Text Input Parameter{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="speaker-identification"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Identify Speaker Names From the Transcript Using LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
  </ul>
</div>

## Get Quotes and Citations with LeMUR

<div className="guides-list">
  <ul className="no-bullets">
    <li>
      <a
        href="dialogue-data"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Extract Dialogue Data with LeMUR and JSON{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="transcript-citations"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Extract Quotes with Timestamps Using LeMUR + Semantic Search{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="timestamped-transcripts"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Extract Transcript Quotes with LeMURs Custom Text Input Parameter{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="lemur-transcript-citations"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Generate Transcript Citations using LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
  </ul>
</div>

## Substitute Audio Intelligence with LeMUR

| Model/Feature          | Use with LeMUR                                                                         |
| ---------------------- | -------------------------------------------------------------------------------------- |
| **Sentiment Analysis** | [Customer Call Sentiment Analysis](call-sentiment-analysis)                            |
| **Custom Vocabulary**  | [Boost Transcription Accuracy](custom-vocab-lemur)                                     |
| **Auto Chapters**      | [Creating Chapter Summaries with the Custom Text Input Parameter](input-text-chapters) |
| **Summarization**      | [Create Custom Summaries using the Task Endpoint](task-endpoint-custom-summary)        |
| **Topic Detection**    | [Create Custom Topic Tags](custom-topic-tags)                                       |

## Use Case-Specific LeMUR Workflows

<div className="guides-list">
  <ul className="no-bullets">
    <li>
      <a
        href="sales-playbook"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Implement a Sales Playbook Using LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="past-response-prompts"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Pass Context from Previous LeMUR Requests{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li className="flex items-center gap-2">
      <a
        href="generate-meeting-action-items-with-lemur"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Generate meeting action items with LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="phone-call-segmentation"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Segment A Phone Call using LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
    <li>
      <a
        href="soap-note-generation"
        className="link-cta rounded-lg flex items-center gap-2"
      >
        Generate SOAP Notes using LeMUR{" "}
        <Icon
          icon="duotone arrow-right"
          color="rgba(var(--accent-aaa),var(--tw-text-opacity,1))"
        />
      </a>
    </li>
  </ul>
</div>


---
title: Livekit
description: Livekit voice agent integration
---

<Note>
  This guide assumes prior knowledge of LiveKit. If you haven't used LiveKit before and are unfamiliar with LiveKit, please check out our [Building a Voice Agent with LiveKit and AssemblyAI guide](https://www.assemblyai.com/docs/speech-to-text/livekit-intro-guide).
</Note>

## Overview

LiveKit is an open source platform for developers building realtime media applications. In this guide, we'll show you how to integrate AssemblyAI's streaming speech-to-text model into your Livekit voice agent using the Agents framework.

<Card 
    title="Livekit" 
    icon={<img src="https://assemblyaiassets.com/images/Livekit.svg" alt="Livekit logo"/>} 
    href="https://docs.livekit.io/agents/integrations/stt/assemblyai/"
>
    View Livekit's AssemblyAI STT plugin documentation.
</Card>

## Quick start

### Installation

Install the plugin from PyPI:

```bash
pip install "livekit-agents[assemblyai]"
```

### Authentication

The AssemblyAI plugin requires an [AssemblyAI API key](https://www.assemblyai.com/docs/api-reference/overview#authorization). Set `ASSEMBLYAI_API_KEY` in your `.env` file.

<Tip>
  You can obtain an AssemblyAI API key by signing up
  [here](https://www.assemblyai.com/dashboard/signup).
</Tip>

### Basic usage

Use AssemblyAI STT in an `AgentSession` or as a standalone transcription service:

```python
from livekit.plugins import assemblyai

session = AgentSession(
    stt = assemblyai.STT(
      end_of_turn_confidence_threshold=0.7,
      min_end_of_turn_silence_when_confident=160,
      max_turn_silence=2400,
    ),
    # ... llm, tts, etc.
    vad=silero.VAD.load(), # VAD Enabled for Interruptions
    turn_detection="stt", # Enable Turn Detection
)
```

## Configuration

### Turn Detection (Key Feature)

AssemblyAI's new turn detection model was built specifically for voice agents and you can tweak it to fit your use case. It processes both audio and linguistic information to determine an end of turn confidence score on every inference, and if that confidence score is past the set threshold, it triggers end of turn.

This custom model was designed to address 2 major issues with voice agents. With traditional VAD (voice activity detection) approaches based on silence alone, there are situations where the agent wouldn't wait for a user to finish their turn even if the audio data suggested it. Think of a situation like "My credit card number is____" - if someone is looking that up, traditional VAD may not wait for the user, where our turn detection model is far better in these situations.

Additionally, in situations where we are certain that the user is done speaking like "What is my credit score?", a high end of turn confidence is returned, greater than the threshold, and triggering end of turn, allowing for minimal turnaround latency in those scenarios.

```python
# STT-based turn detection (recommended)
turn_detection="stt"

stt=assemblyai.STT(
    end_of_turn_confidence_threshold=0.7,
    min_end_of_turn_silence_when_confident=160,  # in ms
    max_turn_silence=2400,  # in ms
)
```

**Parameter tuning:**
- **end_of_turn_confidence_threshold**: Raise or lower the threshold based on how confident you'd like us to be before triggering end of turn based on confidence score
- **min_end_of_turn_silence_when_confident**: Increase or decrease the amount of time we wait to trigger end of turn when confident
- **max_turn_silence**: Lower or raise the amount of time needed to trigger end of turn when end of turn isn't triggered by a high confidence score

<Tip>
  You can also set `turn_detection="vad"` if you'd like turn detection to be based on Silero VAD instead of our advanced turn detection model.
</Tip>

For more information, see our [Universal-Streaming end-of-turn detection guide](https://www.assemblyai.com/docs/speech-to-text/universal-streaming#end-of-turn-detection) and [message-by-message breakdown](https://www.assemblyai.com/docs/speech-to-text/universal-streaming/message-sequence).

### Parameters

<ParamField path="api_key" type="str">
  Your AssemblyAI API key.
</ParamField>

<ParamField path="sample_rate" type="int" default="16000">
  The sample rate of the audio stream
</ParamField>

<ParamField path="encoding" type="str" default="pcm_s16le">
  The encoding of the audio stream. Allowed values: `pcm_s16le`, `pcm_mulaw`
</ParamField>

<ParamField path="format_turns" type="bool" default="True">
  Whether to return formatted final transcripts. If enabled, formatted final
  transcripts will be emitted shortly following an end-of-turn detection.
</ParamField>

<ParamField path="end_of_turn_confidence_threshold" type="float" default="0.7">
  The confidence threshold to use when determining if the end of a turn has been
  reached.  
  In our API the default is 0.4, but the default in LiveKit is set to 0.65.
</ParamField>

<ParamField path="min_end_of_turn_silence_when_confident" type="int" default="160">
  The minimum amount of silence required to detect end of turn when confident.
</ParamField>

<ParamField path="max_turn_silence" type="int" default="2400">
  The maximum amount of silence allowed in a turn before end of turn is triggered.
</ParamField>


---
title: Pipecat
description: Pipecat voice agent integration
---

<Note>
  This guide assumes prior knowledge of Pipecat. If you haven't used Pipecat before and are unfamiliar with Pipecat, please check out our [Building a Voice Agent with Pipecat and AssemblyAI guide](https://www.assemblyai.com/docs/speech-to-text/pipecat-intro-guide).
</Note>

## Overview

Pipecat is an open source platform for developers building realtime media applications. In this guide, we'll show you how to integrate AssemblyAI's streaming speech-to-text model into your Pipecat voice agent using the Pipeline framework.

<Card 
    title="Pipecat" 
    icon={<img src="https://assemblyaiassets.com/images/Pipecat.svg" alt="Pipecat logo"/>} 
    href="https://docs.pipecat.ai/server/services/stt/assemblyai"
>
    View Pipecat's AssemblyAI STT plugin documentation.
</Card>

## Quick start

### Installation

Install the AssemblyAI service from PyPI:

```bash
pip install "pipecat-ai[assemblyai]"
```

### Authentication

The AssemblyAI service requires an [AssemblyAI API key](https://www.assemblyai.com/docs/api-reference/overview#authorization). Set `ASSEMBLYAI_API_KEY` in your `.env` file.

<Tip>
  You can obtain an AssemblyAI API key by signing up
  [here](https://www.assemblyai.com/dashboard/signup).
</Tip>

### Basic usage

Use AssemblyAI STT in a `Pipeline`:

```python
from pipecat.services.assemblyai.stt import AssemblyAISTTService, AssemblyAIConnectionParams

# Configure service
stt = AssemblyAISTTService(
    api_key=os.getenv("ASSEMBLYAI_API_KEY"),
    vad_force_turn_endpoint=False,  # Use AssemblyAI's STT-based turn detection
    connection_params=AssemblyAIConnectionParams(
        end_of_turn_confidence_threshold=0.7,
        min_end_of_turn_silence_when_confident=160,
        max_turn_silence=2400,
    )
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    llm,
    tts,
    transport.output(),
])
```

## Configuration

### Turn Detection (Key Feature)

AssemblyAI's new turn detection model was built specifically for voice agents and you can tweak it to fit your use case. It processes both audio and linguistic information to determine an end of turn confidence score on every inference, and if that confidence score is past the set threshold, it triggers end of turn.

This custom model was designed to address 2 major issues with voice agents. With traditional VAD (voice activity detection) approaches based on silence alone, there are situations where the agent wouldn't wait for a user to finish their turn even if the audio data suggested it. Think of a situation like "My credit card number is____" - if someone is looking that up, traditional VAD may not wait for the user, where our turn detection model is far better in these situations.

Additionally, in situations where we are certain that the user is done speaking like "What is my credit score?", a high end of turn confidence is returned, greater than the threshold, and triggering end of turn, allowing for minimal turnaround latency in those scenarios.

You can set the `vad_force_turn_endpoint` parameter within the `AssemblyAISTTService` constructor:

```python
stt = AssemblyAISTTService(
    api_key=os.getenv("ASSEMBLYAI_API_KEY"),
    vad_force_turn_endpoint=False,  # Use AssemblyAI's STT-based turn detection
    connection_params=AssemblyAIConnectionParams(
        end_of_turn_confidence_threshold=0.7,
        min_end_of_turn_silence_when_confident=160,  # in ms
        max_turn_silence=2400,  # in ms
    )
)
```

**Parameter tuning:**
- **end_of_turn_confidence_threshold**: Raise or lower the threshold based on how confident you'd like us to be before triggering end of turn based on confidence score
- **min_end_of_turn_silence_when_confident**: Increase or decrease the amount of time we wait to trigger end of turn when confident
- **max_turn_silence**: Lower or raise the amount of time needed to trigger end of turn when end of turn isn't triggered by a high confidence score

<Tip>
  You can also set `vad_force_turn_endpoint=True` if you'd like turn detection to be based on VAD instead of our advanced turn detection model.
</Tip>

For more information, see our [Universal-Streaming end-of-turn detection guide](https://www.assemblyai.com/docs/speech-to-text/universal-streaming#end-of-turn-detection) and [message-by-message breakdown](https://www.assemblyai.com/docs/speech-to-text/universal-streaming/message-sequence).

## Parameters

### Constructor Parameters

<ParamField path="api_key" type="str" required>
  Your AssemblyAI API key.
</ParamField>

<ParamField path="connection_params" type="AssemblyAIConnectionParams">
  Connection parameters for the AssemblyAI WebSocket connection. See below for
  details.
</ParamField>

<ParamField path="vad_force_turn_endpoint" type="bool" default="True">
  When true, sends a `ForceEndpoint` event to AssemblyAI when a
  `UserStoppedSpeakingFrame` is received. Requires a VAD (Voice Activity
  Detection) processor in the pipeline to generate these frames.
</ParamField>

<ParamField path="language" type="Language" default="Language.EN">
  Language for transcription. AssemblyAI currently only supports English
  Streaming transcription.
</ParamField>

### Connection Parameters

<ParamField path="sample_rate" type="int" default="16000">
  The sample rate of the audio stream
</ParamField>

<ParamField path="encoding" type="str" default="pcm_s16le">
  The encoding of the audio stream. Allowed values: `pcm_s16le`, `pcm_mulaw`
</ParamField>

<ParamField path="format_turns" type="bool" default="True">
  Whether to return formatted final transcripts. If enabled, formatted final
  transcripts will be emitted shortly following an end-of-turn detection.
</ParamField>

<ParamField path="end_of_turn_confidence_threshold" type="float" default="0.7">
  The confidence threshold to use when determining if the end of a turn has been
  reached.
</ParamField>

<ParamField path="min_end_of_turn_silence_when_confident" type="int"  default="160">
  The minimum amount of silence required to detect end of turn when confident.
</ParamField>

<ParamField path="max_turn_silence" type="int" default="2400">
  The maximum amount of silence allowed in a turn before end of turn is
  triggered.
</ParamField>


---
title: Vapi
description: Vapi voice agent integration
---

## Overview

Vapi is a developer platform for building voice AI agents, they handle the complex backend of voice agents for you so you can focus on creating great voice experiences. In this guide, we'll show you how to integrate AssemblyAI's streaming speech-to-text model into your Vapi voice agent.

<Card
  title="Vapi"
  icon={
    <img src="https://assemblyaiassets.com/images/Vapi.svg" alt="Vapi logo" />
  }
  href="https://docs.vapi.ai/providers/transcriber/assembly-ai"
>
  View Vapi's AssemblyAI STT provider documentation.
</Card>

## Quick start

<Steps>
    **Head to the "Assistants" tab in your Vapi dashboard.**

    <Frame>
        <img src="file:ca47a3c9-e4ac-401a-bda7-c4e76fee79ec" />
    </Frame>

    **Click on your assistant and then the "Transcriber" tab.**

    <Frame>
        <img src="file:7ca3f3dd-db10-463a-afb4-f603cc3dd92b" />
    </Frame>

    **Select "Assembly AI" on the Provider dropdown and make sure the "Universal Streaming API" option is toggled on.**

    <Frame>
        <img src="file:5903fffc-4278-4106-9f7c-0564c1fae228" />
    </Frame>

</Steps>

Your voice agent now uses **AssemblyAI** for speech-to-text (STT) processing.

<Info>
  New to Vapi? Visit the [Quickstart
  Guide](https://docs.vapi.ai/quickstart/introduction) to explore various
  example voice agent workflows. For the easiest way to test a voice agent,
  follow this [simple phone-based guide](https://docs.vapi.ai/quickstart/phone).
  Vapi offers a wide range of example workflows to get you up and running
  quickly.
</Info>

## Recommended settings for optimal latency

### Transcriber settings

For best latency, Format Turns should be turned off (adds about 50ms of delay), and Universal Streaming should be enabled.

<Frame>
  <img src="file:6adf3371-3418-4cbd-bdab-ab5a2b2877f5" />
</Frame>

### Advanced > Start speaking plan

For start speaking plan, raise the `Wait seconds` to prevent false starts. `Smart Endpointing` should be turned off.

<Frame>
  <img src="file:5a05687c-a06d-4c0c-ad12-dc21b9efba34" />
</Frame>

### Advanced > Stop speaking plan

For stop speaking plan, lower `Voice seconds` to allow for faster interruptions.

<Frame>
  <img src="file:8c05935c-87dc-4cf4-abbd-e87a623866b7" />
</Frame>


---
title: "\U0001F99C️\U0001F517 LangChain Integration with AssemblyAI"
description: Transcribe audio in LangChain using the built-in integration with AssemblyAI.
hide-nav-links: true
---

## AssemblyAI LangChain Integration

AssemblyAI has a LangChain integration for both Python and JavaScript.
Learn more about our language-specific integrations:

<CardGroup cols={2}>
  <Card icon="brands python" title="Python" href="langchain/python" />
  <Card icon="brands js" title="JavaScript" href="langchain/js" />
</CardGroup>

## New to LangChain?

[LangChain](https://www.langchain.com) is an open-source framework for developing applications with [Large Language Models (LLMs)](https://www.assemblyai.com/blog/introduction-large-language-models-generative-ai/) and other AI technologies.
LangChain has a set of pre-built components that you can use to load data and apply LLMs to your data.
However, LLMs only operate on textual data and don't understand speech in audio and video files.
To apply LLMs to speech, you first need to transcribe the audio to text, which is what the AssemblyAI integration helps you with.

Learn more about LangChain in this video.

<iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/RoR4XJw8wIc?si=-GPeOBm2d48ZTVQf"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  allowfullscreen
></iframe>


---
title: "\U0001F99C️\U0001F517 LangChain Python Integration with AssemblyAI"
description: >-
  Transcribe audio in LangChain Python using the built-in integration with
  AssemblyAI.
hide-nav-links: true
---

To apply LLMs to speech, you first need to transcribe the audio to text, which is what the AssemblyAI integration for LangChain helps you with.

<Note>

Looking for the LangChain JavaScript integration?<br />
[Go to the LangChain.JS integration](js).

</Note>

## Quickstart

Install [the AssemblyAI package](https://github.com/langchain-ai/langchain) and [the AssemblyAI Python SDK](https://github.com/AssemblyAI/assemblyai-python-sdk):

```bash
pip install langchain
pip install assemblyai
```

Set your AssemblyAI API key as an environment variable named `ASSEMBLYAI_API_KEY`. You can [get a free AssemblyAI API key from the AssemblyAI dashboard](https://www.assemblyai.com/app/api-keys).

```bash
# Mac/Linux:
export ASSEMBLYAI_API_KEY=<YOUR_API_KEY>

# Windows:
set ASSEMBLYAI_API_KEY=<YOUR_API_KEY>
```

Import the `AssemblyAIAudioTranscriptLoader` from `langchain.document_loaders`.

```python
from langchain.document_loaders
```

1. Pass the local file path or URL as the `file_path` argument of the `AssemblyAIAudioTranscriptLoader`.
2. Call the `load` method to get the transcript as LangChain documents.

```python
audio_file = "https://assembly.ai/sports_injuries.mp3"
# or a local file path: audio_file = "./sports_injuries.mp3"

loader = AssemblyAIAudioTranscriptLoader(file_path=audio_file)

docs = loader.load()
```

The `load` method returns an array of documents, but by default, there's only one document in the array with the full transcript.

The transcribed text is available in the `page_content` attribute:

```python
docs[0].page_content
# Load time, a new president and new congressional makeup. Same old ...
```

The `metadata` contains the full JSON response with more meta information:

```python
{
  'language_code': <LanguageCode.en_us: 'en_us'>,
  'audio_url': 'https://assembly.ai/nbc.mp3',
  'punctuate': True,
  'format_text': True,
  ...
}
```

## Transcript formats

You can specify the `transcript_format` argument to load the transcript in different formats.

Depending on the format, `load_data()` returns either one or more documents. These are the different `TranscriptFormat` options:

- `TEXT`: One document with the transcription text
- `SENTENCES`: Multiple documents, splits the transcription by each sentence
- `PARAGRAPHS`: Multiple documents, splits the transcription by each paragraph
- `SUBTITLES_SRT`: One document with the transcript exported in SRT subtitles format
- `SUBTITLES_VTT`: One document with the transcript exported in VTT subtitles format

```python
from langchain.document_loaders.assemblyai

loader = AssemblyAIAudioTranscriptLoader(
    file_path="./your_file.mp3",
    transcript_format=TranscriptFormat.SENTENCES,
)

docs = loader.load()
```

## Transcription config

You can also specify the `config` argument to use different transcript features and audio intelligence models.
Here's an example of using the `config` argument to enable speaker labels, auto chapters, and entity detection:

```python


config = aai.TranscriptionConfig(
    speaker_labels=True, auto_chapters=True, entity_detection=True
)

loader = AssemblyAIAudioTranscriptLoader(file_path="./your_file.mp3", config=config)
```

<Info>
  For the full list of options, see [Transcript API
  reference](https://assemblyai.com/docs/api-reference/transcripts/submit#request).
</Info>

## Pass the AssemblyAI API key as an argument

Instead of configuring the AssemblyAI API key as the `ASSEMBLYAI_API_KEY` environment variable,
you can also pass it as the `api_key` argument.

```python
loader = AssemblyAIAudioTranscriptLoader(
    file_path="./your_file.mp3", api_key="<YOUR_API_KEY>"
)
```

## Additional resources

You can learn more about using LangChain with AssemblyAI in these resources.

- [LangChain docs for the AssemblyAI document loader](https://python.langchain.com/docs/integrations/document_loaders/assemblyai)
- [How to use audio data in LangChain with Python](https://www.assemblyai.com/blog/load-audio-langchain-python/)
- [Retrieval Augmented Generation on audio data with LangChain and Chroma](https://www.assemblyai.com/blog/retrieval-augmented-generation-audio-langchain/)
- [Build LangChain Audio Apps with Python in 5 Minutes](https://www.youtube.com/watch?v=7w7ysaDz2W4)
- [How to use LangChain for RAG over audio files](https://www.youtube.com/watch?v=l9YJrLg61ac)
- [AssemblyAI Python SDK](https://github.com/AssemblyAI/assemblyai-python-sdk)


---
title: "\U0001F99C️\U0001F517 LangChain JavaScript Integration with AssemblyAI"
description: >-
  Transcribe audio in LangChain.JS using the built-in integration with
  AssemblyAI.
hide-nav-links: true
---

To apply LLMs to speech, you first need to transcribe the audio to text, which is what the AssemblyAI integration for LangChain helps you with.

<Note>

Looking for the Python integration?<br />
[Go to the LangChain Python integration](python).

</Note>

## Quickstart

Add the [AssemblyAI SDK](https://github.com/AssemblyAI/assemblyai-node-sdk) to your project:

<Tabs>
  <Tab title="npm">

```bash
npm install langchain @langchain/community
```

  </Tab>
  <Tab title="yarn">

```bash
yarn add langchain @langchain/community
```

  </Tab>
  <Tab title="pnpm">

```bash
pnpm add langchain @langchain/community
```

  </Tab>
  <Tab title="bun">

```bash
bun add langchain @langchain/community
```

  </Tab>
</Tabs>

To use the loaders, you need an [AssemblyAI account](https://www.assemblyai.com/dashboard/signup) and get your AssemblyAI API key from the [dashboard](https://www.assemblyai.com/app/api-keys).
Configure the API key as the `ASSEMBLYAI_API_KEY` environment variable or the `apiKey` options parameter.

```javascript

  AudioTranscriptLoader,
  // AudioTranscriptParagraphsLoader,
  // AudioTranscriptSentencesLoader
} from "@langchain/community/document_loaders/web/assemblyai";

// You can also use a local file path and the loader will upload it to AssemblyAI for you.
const audioUrl = "https://assembly.ai/espn.m4a";

// Use `AudioTranscriptParagraphsLoader` or `AudioTranscriptSentencesLoader` for splitting the transcript into paragraphs or sentences
const loader = new AudioTranscriptLoader(
  {
    audio: audioUrl,
    // any other parameters as documented here: https://www.assemblyai.com/docs/api-reference/transcript#create-a-transcript
  },
  {
    apiKey: "<ASSEMBLYAI_API_KEY>", // or set the `ASSEMBLYAI_API_KEY` env variable
  }
);
const docs = await loader.load();
console.dir(docs, { depth: Infinity });
```

<Info>
  - You can use the `AudioTranscriptParagraphsLoader` or
  `AudioTranscriptSentencesLoader` to split the transcript into paragraphs or
  sentences. - If the `audio_file` is a local file path, the loader will upload
  it to AssemblyAI for you. - The `audio_file` can also be a video file. See the
  [list of supported file types in the FAQ
  doc](https://support.assemblyai.com/articles/2616970375-what-audio-and-video-file-types-are-supported-by-your-api).
  - If you don't pass in the `apiKey` option, the loader will use the
  `ASSEMBLYAI_API_KEY` environment variable. - You can add more properties in
  addition to `audio`. Find the full list of request parameters in the
  [AssemblyAI API docs](https://www.assemblyai.com/docs/api-reference/overview).
</Info>

<br />

You can also use the `AudioSubtitleLoader` to get `srt` or `vtt` subtitles as a document.

```javascript
// You can also use a local file path and the loader will upload it to AssemblyAI for you.
const audioUrl = "https://assembly.ai/espn.m4a";

const loader = new AudioSubtitleLoader(
  {
    audio: audioUrl,
    // any other parameters as documented here: https://www.assemblyai.com/docs/api-reference/transcript#create-a-transcript
  },
  "srt", // srt or vtt
  {
    apiKey: "<ASSEMBLYAI_API_KEY>", // or set the `ASSEMBLYAI_API_KEY` env variable
  }
);

const docs = await loader.load();
console.dir(docs, { depth: Infinity });
```

## Additional resources

You can learn more about using LangChain with AssemblyAI in these resources:

- [The LangChain docs for the AssemblyAI document loader](https://js.langchain.com/docs/integrations/document_loaders/web_loaders/assemblyai_audio_transcription)
- [How to integrate spoken audio into LangChain.js using AssemblyAI](https://www.assemblyai.com/blog/integrate-audio-langchainjs/)
- [Integrate Audio into LangChain.js apps in 5 Minutes](https://www.youtube.com/watch?v=hNpUSaYZIzs)
- [AssemblyAI JavaScript SDK](https://github.com/AssemblyAI/assemblyai-node-sdk)


---
title: Transcribe Your Amazon Connect Recordings
---

This guide walks through the process of setting up a transcription pipeline for Amazon Connect recordings using AssemblyAI.

### Get Started

Before we begin, make sure you have:

- An AssemblyAI account and an API key. You can [sign up](https://assemblyai.com/dashboard/signup) for a free account and get your API key from your dashboard.
- An AWS account.
- An [Amazon Connect instance](https://docs.aws.amazon.com/connect/latest/adminguide/amazon-connect-instances.html).

## Step-by-Step Instructions

<Steps>
<Step>

In the AWS console, navigate to the **Amazon Connect** services page. Select your instance and then click into the **Data Storage** section. On this page, find the subsection named **Call Recordings** and note the S3 bucket path where your call recordings are stored, you'll need this for later.

<img src="file:186783d2-051a-4256-b30d-3cd0aa371e25" />

</Step>
<Step>

Navigate to the **Lambda** services page, and create a new function. Set the runtime to **Python 3.13**. In the **Change default execution role** section, choose the option to create a new role with basic Lambda permissions. Assign a function name and then click **Create function**.

<img src="file:40f2bc8c-1352-435c-ac75-7b1a9ea9102c" />

</Step>
<Step>

In this new function, scroll down to the **Code Source** section and paste the following code into `lambda_function.py`.

```python
import json
import os
import boto3
import http.client
import time
from urllib.parse import unquote_plus
import logging

# Configure logging

logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Configuration settings

# See config parameters here: https://www.assemblyai.com/docs/api-reference/transcripts/submit

ASSEMBLYAI_CONFIG = {
# 'language_code': 'en_us',
# 'multichannel': True,
# 'redact_pii': True,
}

# Initialize AWS services

s3_client = boto3.client('s3')

def get_presigned_url(bucket, key, expiration=3600):
    """Generate a presigned URL for the S3 object"""

    logger.info({
        "message": "Generating presigned URL",
        "bucket": bucket,
        "key": key,
        "expiration": expiration
    })

    s3_client_with_config = boto3.client(
        's3',
        config=boto3.session.Config(signature_version='s3v4')
    )

    return s3_client_with_config.generate_presigned_url(
        'get_object',
        Params={'Bucket': bucket, 'Key': key},
        ExpiresIn=expiration
    )

def delete_transcript_from_assemblyai(transcript_id, api_key):
    """
    Delete transcript data from AssemblyAI's database using their DELETE endpoint.

    Args:
    transcript_id (str): The AssemblyAI transcript ID to delete
    api_key (str): The AssemblyAI API key

    Returns:
    bool: True if deletion was successful, False otherwise
    """

    headers = {
        "authorization": api_key,
        "content-type": "application/json"
    }

    conn = http.client.HTTPSConnection("api.assemblyai.com")

    try: # Send DELETE request to AssemblyAI API
        conn.request("DELETE", f"/v2/transcript/{transcript_id}", headers=headers)
        response = conn.getresponse()

        # Check if deletion was successful (HTTP 200)
        if response.status == 200:
            response_data = json.loads(response.read().decode())
            logger.info(f"Successfully deleted transcript {transcript_id} from AssemblyAI")
            return True
        else:
            error_message = response.read().decode()
            logger.error(f"Failed to delete transcript {transcript_id}: HTTP {response.status} - {error_message}")
            return False

    except Exception as e:
        logger.info(f"Error deleting transcript {transcript_id}: {str(e)}")
        return False
    finally:
        conn.close()

def transcribe_audio(audio_url, api_key):
    """Transcribe audio using AssemblyAI API with http.client"""
    logger.info({"message": "Starting audio transcription"})

    headers = {
    "authorization": api_key,
    "content-type": "application/json"
    }

    conn = http.client.HTTPSConnection("api.assemblyai.com")

    # Submit the audio file for transcription with config parameters

    request_data = {"audio_url": audio_url}

    # Add all configuration settings

    request_data.update(ASSEMBLYAI_CONFIG)

    json_data = json.dumps(request_data)
    conn.request("POST", "/v2/transcript", json_data, headers)
    response = conn.getresponse()

    if response.status != 200:
        raise Exception(f"Failed to submit audio for transcription: {response.read().decode()}")

    response_data = json.loads(response.read().decode())
    transcript_id = response_data['id']
    logger.info({"message": "Audio submitted for transcription", "transcript_id": transcript_id})

    # Poll for transcription completion

    while True:
        conn = http.client.HTTPSConnection("api.assemblyai.com")
        conn.request("GET", f"/v2/transcript/{transcript_id}", headers=headers)
        polling_response = conn.getresponse()
        polling_data = json.loads(polling_response.read().decode())

       if polling_data['status'] == 'completed':
           conn.close()
           logger.info({"message": "Transcription completed successfully"})
           return polling_data  # Return full JSON response instead of just text
       elif polling_data['status'] == 'error':
           conn.close()
           raise Exception(f"Transcription failed: {polling_data['error']}")

       conn.close()
       time.sleep(3)

def lambda_handler(event, context):
    """Lambda function to handle S3 events and process audio files"""
    try: # Get the AssemblyAI API key from environment variables
        api_key = os.environ.get('ASSEMBLYAI_API_KEY')
        if not api_key:
            raise ValueError("ASSEMBLYAI_API_KEY environment variable is not set")

       # Process each record in the S3 event
       for record in event.get('Records', []):
           # Get the S3 bucket and key
           bucket = record['s3']['bucket']['name']
           key = unquote_plus(record['s3']['object']['key'])

           # Generate a presigned URL for the audio file
           audio_url = get_presigned_url(bucket, key)

           # Get the full transcript JSON from AssemblyAI
           transcript_data = transcribe_audio(audio_url, api_key)

           # Prepare the transcript key - maintaining path structure but changing directory and extension
           transcript_key = key.replace('/CallRecordings/', '/AssemblyAITranscripts/', 1).replace('.wav', '.json')

           # Convert the JSON data to a string
           transcript_json_str = json.dumps(transcript_data, indent=2)

           # Upload the transcript JSON to the same bucket but in transcripts directory
           s3_client.put_object(
               Bucket=bucket,  # Use the same bucket
               Key=transcript_key,
               Body=transcript_json_str,
               ContentType='application/json'
           )
           logger.info({"message": "Transcript uploaded to transcript bucket successfully.", "key": transcript_key})

           # Uncomment the following line to delete transcript data from AssemblyAI after saving to S3
           # https://www.assemblyai.com/docs/api-reference/transcripts/delete
           # delete_transcript_from_assemblyai(transcript_data['id'], api_key)

       return {
           "statusCode": 200,
           "body": json.dumps({
               "message": "Audio file(s) processed successfully",
               "detail": "Transcripts have been stored in the AssemblyAITranscripts directory"
           })
       }

    except Exception as e:
       print(f"Error: {str(e)}")

       return {
            "statusCode": 500,
            "body": json.dumps({
            "message": "Error processing audio file(s)",
            "error": str(e)
            })
       }
```

</Step>
<Step>

At the top of the lambda function, you can edit the config to enable features for your transcripts. To see all available parameters, check out our [API reference](https://www.assemblyai.com/docs/api-reference/transcripts/submit).

```python
ASSEMBLYAI_CONFIG = {
   # 'language_code': 'en_us',
   # 'multichannel': True,
   # 'redact_pii': True,
}
```

<Tip>

If you would like to delete transcripts from AssemblyAI after completion, you can uncomment line **166** to enable the `delete_transcript_from_assemblyai` function. This ensures the transcript data is only saved on your S3 database and not stored on AssemblyAI's database.

</Tip>

Once you have finished editing the lambda function, click **Deploy** to save your changes.

</Step>
<Step>

On the same page, navigate to the **Configuration** section, under **General configuration** adjust the timeout to 15min 0sec and click **Save**. The processing times for transcription will be a lot shorter, but this ensures plenty of time for the function to complete.

<img src="file:4b703c3e-35cb-473a-bc29-a11169790fb3" />

</Step>
<Step>

Now from this page, on the left side panel click **Environment variables**. Click edit and then add an environment variable, `ASSEMBLYAI_API_KEY`, and set the value to your AssemblyAI API key. Then click **Save**.

<img src="file:eddd4561-5904-49f5-9da6-18e6f5751f9c" />

</Step>
<Step>

Now, navigate to the **IAM** services page. On the left side panel under Access Management click **Roles** and search for your Lambda function role (it's structure should look like `function_name-role-id`). Click into the role and then in the **Permissions policies** section click the dropdown for **Add permissions** and then select **Attach policies**.

From this page, find the policy named `AmazonS3FullAccess` and click **Add permissions**.

<img src="file:af78fedd-ea17-4baf-a11a-fcddd6ab8ebb" />

</Step>
<Step>

Now, navigate to the **S3** services page and click into the general purpose bucket where your Amazon Connect recordings are stored. Browse to the **Properties** tab and then scroll down to **Event notifications**. Click **Create event notification**. Give the event a name and then in the prefix section, insert the folder path we noted from Step 1 to ensure the event is triggered for the correct folder.

<img src="file:669b8ba0-71ec-499f-9cdb-50647e35ef14" />

Then in the **Event types** section, select **All object create events**.

<img src="file:0dcaeb15-d3b5-414f-905d-91282ed2c7bd" />

Then scroll down to the **Destination** section, set the destination as **Lambda function** and then select the Lambda function we created in Step 2. Then click **Save changes**.

<img src="file:c8544062-dfd6-4fb6-a281-966ca937518b" />

</Step>
<Step>

To finalise the integration, we'll need to set the recording behaviour from within your AWS Contact Flows. Navigate to your Amazon Connect instance access URL and sign in to your Admin account. In the left side panel, navigate to the **Routing** section and then select **Flows**.

Choose a flow to test with, in this case we'll utilize the `Sample inbound flow (first contact experience)`. You should see the **Block Library** on the left hand side of the page. In this section, search for `Set recording and analytics behaviour` and then drag the block into your flow diagram and connect the arrows.

You can see in our example, we place the block right at the entry of the call flow:

<img src="file:496281bf-9ba4-4393-a35c-a54301b12629" />

After connecting this block, click the 3 vertical dots in the top right of the block and select **Edit settings**. Scroll down to the **Enable recording and analytics** subsection and expand the **Voice** section. Then select `On` and select `Agent and customer` (or whoever you'd like to record). Then click **Save**, click **Save** again in the top right and then click **Publish** to publish the flow.

<img src="file:14456739-aff2-474c-b217-097defa7e30a" />

With this new flow published, you should now receive recordings for your Amazon Connect calls that utilize that flow, and you should now receive AssemblyAI transcripts for those recordings!

<Info>

The Amazon Connect Call Recordings are saved in the S3 bucket with this naming convention:
**/connect/\{instance-name\}/CallRecordings/\{YYYY\}/\{MM\}/\{DD\}/\{contact-id\}\_\{YYYYMMDDThh:mm\}\_UTC.wav**

The AssemblyAI Transcripts will be saved in the S3 bucket with this naming convention:
**/connect/\{instance-name\}/AssemblyAITranscripts/\{YYYY\}/\{MM\}/\{DD\}/\{contact-id\}\_\{YYYYMMDDThh:mm\}\_UTC.json**

</Info>
</Step>
<Step>

To view the logs for this integration, navigate to the **CloudWatch** services page and under the **Logs** section, select **Log groups**. Select the log group that matches your Lambda to view the most recent log stream.

</Step>
</Steps>


---
title: Transcribe Genesys Cloud Recordings with AssemblyAI
---

This guide walks through the process of setting up a transcription pipeline to send audio data from Genesys Cloud to AssemblyAI.

To accomplish this, we'll stream audio through Genesys's [AudioHook Monitor](https://appfoundry.genesys.com/filter/genesyscloud/listing/a3ff6a99-d866-4734-ab7a-16cff2e4308c) integration to a WebSocket server.
Upon call completion, the server will process this audio into a wav file and send it to AssemblyAI's Speech-to-text API for [pre-recorded audio](/docs/speech-to-text/pre-recorded-audio) transcription.

You can find all the necessary code for this guide [here](https://github.com/gsharp-aai/genesys-async-guide).

## Architecture Overview
Here's the general flow our app will follow:
```
+---------------------+     +--------------------+     +----------------------+
| 1. Genesys Cloud    |  →  | 2. WebSocket       |  →  | 3. Convert Raw       |
| (AudioHook Monitor) |     | Server             |     | Audio to WAV         |
+---------------------+     +--------------------+     +----------------------+
                                                                 ↓                                                             
+--------------------+     +---------------------+     +----------------------+
| 6. S3 Bucket       |  ←  | 5. AssemblyAI API   |  ←  | 4. Audio Upload (S3) |
| (Transcript Store) |     | (Transcription)     |     | (Trigger Lambda)     |
+--------------------+     +---------------------+     +----------------------+
```

## Getting started
Before we begin, make sure you have:
- An AssemblyAI account and an API key. You can [sign up](https://assemblyai.com/dashboard/signup) for a free account and get your API key from your [dashboard](https://www.assemblyai.com/dashboard/api-keys).
- An AWS account, an [Access key](https://us-east-1.console.aws.amazon.com/iam/home#/security_credentials), and permissions to S3, Lambda, and CloudWatch.
- A [Genesys Cloud](https://www.genesys.com/genesys-cloud) account with the necessary permissions to create call flows, phone numbers, and routes.
- [ngrok](https://ngrok.com/downloads/mac-os) installed.

## Genesys AudioHook Monitor

In order to stream your voice calls to third party services outside of the Genesys Cloud platform, Genesys offers an official integration called [AudioHook Monitor](https://appfoundry.genesys.com/filter/genesyscloud/listing/a3ff6a99-d866-4734-ab7a-16cff2e4308c).

This integration allows you to specify the URL of a [WebSocket server](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API) that implements the [AudioHook Protocol](https://developer.genesys.cloud/devapps/audiohook/), and once a connection has been established, Genesys will send both text (metadata messages/events encoded as JSON) and binary data (WebSocket frames containing the raw audio data in μ-law (Mu-law, PCMU) format).

An understanding of this integration and protocol is recommended before proceeding with this tutorial. Here are some helpful resources to get started:

- [Genesys App Foundry](https://appfoundry.genesys.com/filter/genesyscloud/listing/a3ff6a99-d866-4734-ab7a-16cff2e4308c)
- [AudioHook Monitor](https://help.mypurecloud.com/articles/about-audiohook-monitor/)
- [AudioHook Protocol](https://developer.genesys.cloud/devapps/audiohook/)
- [AudioHook Sample Service Repo](https://github.com/purecloudlabs/audiohook-reference-implementation#genesys-audiohook-sample-service)

## Step 1: Create a call flow in Genesys (optional)
You may already have an inbound call flow set up in Genesys (if so, skip to [Step 3](/docs/integrations/genesys-cloud#step-3-create-a-s3-bucket)), but we'll create a simple one from scratch for the sake of this tutorial.

<Note title="Adapting existing call flows">
All you need to add is the **Audio Monitoring** step from the toolbox and make sure **Suppress recording for the entire flow** is unchecked in the flow's **Recording and Speech Recognition** settings.
</Note>

<Steps>
<Step>

Within the [Architect tool](https://apps.usw2.pure.cloud/architect/#/inboundcall/flows), click **Add** to create a new call flow. Enter a **Name** for your flow and click **Create Flow**. Select the newly created flow to open the drag and drop editor.

<img src="file:d804c7b9-8ab5-4fa0-97d9-baa95bd89ea1" />

</Step>
<Step>

Create a **Reusable Task** from the bottom left of the left-side menu. From the Toolbox, search for **Audio Monitoring** and drag it just after the **Start** step of our flow. In the right-side menu for this option, make sure **Enable Monitoring** is enabled.

<img src="file:4de35e41-445d-4291-ad0b-b33c31abc571" />

</Step>
<Step>

Back in the Toolbox, search for **Transfer to User** and set that as the next step. In the right-hand menu, under **User** select a caller. Under **Pre-Transfer Audio** and **Failed Transfer Audio**, type your preferred messages.
<img src="file:413c2c30-d924-49bc-aa23-862622937a01" />

</Step>
<Step>

Search for **Disconnect** in the toolbox and drag that as the step following **Failure**.
<img src="file:4bc20f82-3a1e-4a21-909e-f156b10c29cf" />

</Step>
<Step>

Search the Toolbox for **Jump to Reusable Task** and drag this tool to the Main Menu at the top of the left-side menu. Select a **DTMF** and **Speech Recognition** value (this will be used to transfer your call to the agent). Under **Task**, select the task you just created.
<img src="file:f554b942-e67a-456b-88de-0a4e87669222" />

</Step>
<Step>

Under Settings in the left-side menu, navigate to the **Recording and Speech Recognition** section. Make sure **Suppress recording for the entire flow** is unchecked.
<img src="file:c9f38f8a-cbac-434e-bff7-3049217892dd" />

</Step>
<Step>

In the top navbar, make sure to click **Save** and then click **Publish** to have your changes take effect.

</Step>
</Steps>

## Step 2: Setup a phone and routing for your flow (optional)
<Steps>
<Step>

In the Genesys Cloud Admin section, navigate to the [Phones page](https://apps.usw2.pure.cloud/directory/#/admin/telephony/phone-management/phones) under the **Telephony section** and click **Add** to create a new phone. For **Person**, assign the User from your organization that you selected for the **Transfer to User** step in the [previous section](/docs/integrations/genesys-cloud#step-3).
<img src="file:85064df7-4276-43c2-ac6a-39d7dc06a156" />
<img src="file:c1b31a83-ce71-4c14-8eeb-01754ca2551b" />


</Step>
<Step>

Navigate to the [Number Management](https://apps.usw2.pure.cloud/directory/#/admin/telecom/numbers/numbers) page under the **Genesys Cloud Voice** section and select **Purchase Numbers**.
Enter an area code and click **Search**. Select a phone number from the list and click **Complete Purchase**.

<img src="file:29449970-89a0-4432-b07b-17a83a7a12f3" />
<img src="file:8479d851-a982-43e3-bd78-6ccc5d0b88a6" />

</Step>
<Step>

Navigate to the [Call Routing](https://apps.usw2.pure.cloud/directory/#/admin/routing/ivrs) page under the **Routing** section and select **Add**. Under **What call flow should be used?** select your flow. For **Inbound Numbers**, type the number you purchased in the above step. Then click **Create**.

<img src="file:14d909c0-1ac0-4b05-bc2d-c2f13ed036ca" />
<img src="file:665905e6-9ee8-4d6e-8b96-b67559b75c59" />

</Step>
<Step>

Under the **Telephony** section, navigate to the [External Trunks](https://apps.usw2.pure.cloud/directory/#/admin/telephony/trunks/external) page. Click **Create New**. Under **Caller ID**, the **Caller Address** will be an E.164 number and the phone number you created. 

<img src="file:59a1eb92-5221-4178-86a7-de30a21807f7" />
<img src="file:c036ae35-0305-40f9-a1de-c3a180bec372" />

</Step>
<Step>

Under **SIP Access Control**, select **Allow All** (*note: this is only for development and testing purposes, please specify actual IPs in production*). Under the **Media** section, make sure you select **Record calls on this trunk**. Then click **Save External Trunk** (it may take a few moments for your trunk to be ready).

<img src="file:af859dde-a73f-4e85-9a47-0afa517efa04" />
<img src="file:e8db478f-5063-4942-96a3-6313f0e39725" />

</Step>
</Steps>

## Step 3: Create a S3 bucket

After our Genesys call ends, store the audio file in S3.
<Steps>
<Step>

Click **Create bucket**. Give your bucket a name like `your-audiohook-bucket`. Scroll down and click **Create bucket**.

<img src="file:97a3055b-c787-48e8-8d6e-38dbaf387e97" />

</Step>
</Steps>

## Step 4: Create a WebSocket server
In this step, we'll set up a WebSocket server to receive messages and audio data from Genesys as they are sent.
Our server must respond to certain events (i.e. `open`, `close`, `ping`, `pause`, etc.) according to the AudioHook protocol.
Outside of these event messages, audio data is also transferred. We'll capture and temporarily store this audio locally until the connection is closed, at which point the server processes the audio to a `wav` file and uploads both the `wav` and `raw` audio files to a S3 bucket.

The AudioHook Monitor will send requests to a WebSocket URL we specify when setting up the integration in [Step 5](/docs/integrations/genesys-cloud#step-5-setting-up-audiohook-monitor).
When first enabled, AudioHook Monitor will do a quick verification step to ensure that the WebSocket server has implemented the AudioHook protocol correctly.

For this example, the server is written in JavaScript ([Express](https://expressjs.com/)) and hosted locally. We'll use [ngrok](https://ngrok.com/) to create a secure tunnel that exposes it to the internet with a public URL so that Genesys can make a connection.
However, the server can be implemented using your preferred programming language and deployed in whatever environment you choose, provided both support WebSocket TLS connections for secure bidirectional text and binary message exchange.

<Note title="Server implementation">
This server is a method to get up and running quickly for development and testing purposes without the complexity of a production deployment. How you implement this in practice will vary widely depending on your traffic volume, scaling needs, reliability requirements, security concerns, etc.
</Note>

<Steps>
<Step>

Clone this [example repo](https://github.com/gsharp-aai/genesys-async-guide) of a WebSocket server that implements the AudioHook protocol.
Follow the `README` instructions to download the necessary dependencies and start the server.
Make sure to look over `server.js` to get an understanding of how the requests from Genesys are received, processed, and responded to, as well as how the audio is stored, converted, and uploaded to our S3 bucket.

</Step>
<Step>

Make sure to create a `.env` file and set the variables:

```bash
PORT=3000
AWS_REGION=us-east-1 # Region of your S3 bucket
AWS_ACCESS_KEY_ID=<ACCESS_ID> # Found under IAM > Security Credentials
AWS_SECRET_ACCESS_KEY=<SECRET_KEY> # Found under IAM > Security Credentials
S3_BUCKET=your-audiohook-bucket # Name of your S3 bucket
S3_KEY_PREFIX=calls/ # The file structure you want your bucket to follow
API_KEY=<YOUR_API_KEY> # Used for authenticating messages from Genesys
RECORDINGS_DIR=./recordings # Temp file storage location
```

<Note>
`AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` can be found on your account's [IAM > Security Credentials page](https://us-east-1.console.aws.amazon.com/iam/home?region=us-east-1#/security_credentials). `API_KEY` is explained further in the [next step](/docs/integrations/genesys-cloud#step-3-3), but it can be anything you want to verify that the requests are actually originating from Genesys.
</Note>

</Step>
<Step>

Download [ngrok](https://ngrok.com/). Assuming your server is running on `port 3000`, run `ngrok http 3000 --inspect=false` in your terminal.
From the resulting terminal output, note the forwarding url that should look something like this: `https://<id>.ngrok-free.app`. This is our WebSocket server URL that we'll provide to the AudioHook Monitor in the next step.

<img src="file:795a26ee-e83e-4674-8661-ab983744754e" />

</Step>
</Steps>

## Step 5: Setting up AudioHook Monitor

<Steps>
<Step>

In the Genesys Cloud Admin section, navigate to the [Integrations](https://apps.usw2.pure.cloud/directory/#/admin/integrations/apps) page. Add a new integration via the plus sign in the top right corner.
<img src="file:84c3f416-509d-4933-a6fd-d4db7819212d" />

</Step>

<Step>

Search for **AudioHook Monitor** and install.
<img src="file:e3014119-32e5-41f0-a24c-d9e18dc057de" />

</Step>
<Step>

Navigate to the AudioHook Monitor's **Configuration** tab. Under the **Properties section**, make sure both channels are selected and the **Connection URI** is set to the ngrok url from the [previous step](/docs/integrations/genesys-cloud#step-3-2). For the ngrok url, replace `https` with `wss`.

<img src="file:45b31d2c-403f-4387-b6f8-bec61efe8308" />

</Step>
<Step>

In the **Configuration** tab, navigate to the **Credentials** section, and click **Configure**. Here you can set an API key to a value our server will use to verify that requests originated from Genesys. This is done via the `X-API-KEY` request header. Our server will compare this key to the value we set in our `.env` for `API_KEY`, so make sure they match. Click **Save**.
<img src="file:ac04644a-e191-4382-90e5-d94b9b4d0c14" />

</Step>
<Step>

Back on the **Integrations** page, click the toggle button under the **Status** column to activate your AudioHook. Genesys will attempt to verify our server is correctly configured according to the AudioHook protocol. If it is unable to do so, a red error will show with the reason for the failed connection. If it succeeds, the connection will toggle to Active.
<img src="file:10331745-8b4a-4250-a0b4-3cdb0a82a270" />

</Step>
</Steps>

## Step 6: Set up your AssemblyAI API call

<Steps>
<Step>

Navigate to the [Lambda](https://us-east-1.console.aws.amazon.com/lambda/home) services page, and create a new function. Set the runtime to `Node.js 22.x`. In the **Change default execution role** section, choose the option to create a **new role with basic Lambda permissions**. Assign a function name and then click **Create function**.

<img src="file:290124d4-e8b1-41bf-95f8-02f0902faa74" />

</Step>
<Step>

In this new function, scroll down to the **Code Source** section and paste the following code into `index.js`:

```javascript
// Import required AWS SDK modules
import { S3 } from '@aws-sdk/client-s3';
import { getSignedUrl } from '@aws-sdk/s3-request-presigner';
import { GetObjectCommand } from '@aws-sdk/client-s3';

// Configure logging
const logger = {
  info: (data) => console.log(JSON.stringify(data)),
  error: (data) => console.error(JSON.stringify(data))
};

// Configuration settings for AssemblyAI
// See config parameters here: https://www.assemblyai.com/docs/api-reference/transcripts/submit
const ASSEMBLYAI_CONFIG = {
  multichannel: true // Using multichannel here as we told Genesys to send us multichannel audio.
};

// Initialize AWS S3 client
const s3Client = new S3();

/**
 * Generate a presigned URL for the S3 object
 * @param {string} bucket - S3 bucket name
 * @param {string} key - S3 object key
 * @param {number} expiration - URL expiration time in seconds
 * @returns {Promise<string>} Presigned URL
 */
const getPresignedUrl = async (bucket, key, expiration = 3600) => {
  logger.info({
    message: "Generating presigned URL",
    bucket: bucket,
    key: key,
    expiration: expiration
  });

  const command = new GetObjectCommand({
    Bucket: bucket,
    Key: key
  });

  return getSignedUrl(s3Client, command, { expiresIn: expiration });
}

/**
 * Delete transcript data from AssemblyAI's database
 * @param {string} transcriptId - The AssemblyAI transcript ID to delete
 * @param {string} apiKey - The AssemblyAI API key
 * @returns {Promise<boolean>} True if deletion was successful, False otherwise
 */
const deleteTranscriptFromAssemblyAI = async (transcriptId, apiKey) => {
  try {
    const response = await fetch(`https://api.assemblyai.com/v2/transcript/${transcriptId}`, {
      method: 'DELETE',
      headers: {
        'authorization': apiKey,
        'content-type': 'application/json'
      }
    });
    
    if (response.ok) {
      logger.info(`Successfully deleted transcript ${transcriptId} from AssemblyAI`);
      return true;
    } else {
      const errorData = await response.text();
      logger.error(`Failed to delete transcript ${transcriptId}: HTTP ${response.status} - ${errorData}`);
      return false;
    }
  } catch (error) {
    logger.error(`Error deleting transcript ${transcriptId}: ${error.message}`);
    return false;
  }
}

/**
 * Submit audio for transcription
 * @param {object} requestData - Request data including audio URL and config
 * @param {string} apiKey - AssemblyAI API key
 * @returns {Promise<string>} Transcript ID
 */
const submitTranscriptionRequest = async (requestData, apiKey) => {
  const response = await fetch('https://api.assemblyai.com/v2/transcript', {
    method: 'POST',
    headers: {
      'authorization': apiKey,
      'content-type': 'application/json'
    },
    body: JSON.stringify(requestData)
  });

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`Failed to submit audio for transcription: ${errorText}`);
  }

  const responseData = await response.json();
  const transcriptId = responseData.id;
  
  logger.info({
    message: "Audio submitted for transcription",
    transcript_id: transcriptId
  });
  
  return transcriptId;
}

/**
 * Poll for transcription completion
 * @param {string} transcriptId - Transcript ID
 * @param {string} apiKey - AssemblyAI API key
 * @returns {Promise<object>} Transcription data
 */
const pollTranscriptionStatus = async (transcriptId, apiKey) => {
  const sleep = (ms) => new Promise(resolve => setTimeout(resolve, ms));
  
  // Keep polling until we get a completion or error
  while (true) {
    const response = await fetch(`https://api.assemblyai.com/v2/transcript/${transcriptId}`, {
      method: 'GET',
      headers: {
        'authorization': apiKey,
        'content-type': 'application/json'
      }
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Failed to poll transcription status: ${errorText}`);
    }

    const pollingData = await response.json();

    if (pollingData.status === 'completed') {
      logger.info({ message: "Transcription completed successfully" });
      return pollingData;
    } else if (pollingData.status === 'error') {
      throw new Error(`Transcription failed: ${pollingData.error}`);
    }
    
    // Wait before polling again
    await sleep(3000);
  }
}

/**
 * Transcribe audio using AssemblyAI API
 * @param {string} audioUrl - URL of the audio file
 * @param {string} apiKey - AssemblyAI API key
 * @returns {Promise<object>} Transcription data
 */
const transcribeAudio = async (audioUrl, apiKey) => {
  logger.info({ message: "Starting audio transcription" });

  // Prepare request data with config parameters
  const requestData = { audio_url: audioUrl, ...ASSEMBLYAI_CONFIG };
  
  // Submit the audio file for transcription
  const transcriptId = await submitTranscriptionRequest(requestData, apiKey);
  
  // Poll for transcription completion
  return await pollTranscriptionStatus(transcriptId, apiKey);
}

/**
 * Lambda function handler
 * @param {object} event - S3 event
 * @param {object} context - Lambda context
 * @returns {Promise<object>} Response
 */
export const handler = async (event, context) => {
  try {
    // Get the AssemblyAI API key from environment variables
    const apiKey = process.env.ASSEMBLYAI_API_KEY;
    if (!apiKey) {
      throw new Error("ASSEMBLYAI_API_KEY environment variable is not set");
    }

    // Process each record in the S3 event
    const records = event.Records || [];
    
    for (const record of records) {
      // Get the S3 bucket and key
      const bucket = record.s3.bucket.name;
      const key = decodeURIComponent(record.s3.object.key.replace(/\+/g, ' '));
      
      // Generate a presigned URL for the audio file
      const audioUrl = await getPresignedUrl(bucket, key);
      
      // Get the full transcript JSON from AssemblyAI
      const transcriptData = await transcribeAudio(audioUrl, apiKey);
      
      // Prepare the transcript key - maintaining path structure but changing directory and extension
      const transcriptKey = key
        .replace('audio', 'transcripts', 1)
        .replace('.wav', '.json');
      
      // Convert the JSON data to a string
      const transcriptJsonStr = JSON.stringify(transcriptData, null, 2);
      
      // Upload the transcript JSON to the same bucket but in transcripts directory
      await s3Client.putObject({
        Bucket: bucket,  // Use the same bucket
        Key: transcriptKey, // Store under the /transcripts directory
        Body: transcriptJsonStr,
        ContentType: 'application/json'
      });
      
      logger.info({
        message: "Transcript uploaded to transcript bucket successfully.",
        key: transcriptKey
      });
      
      // Uncomment the following line to delete transcript data from AssemblyAI after saving to S3
      // https://www.assemblyai.com/docs/api-reference/transcripts/delete
      // await deleteTranscriptFromAssemblyAI(transcriptData.id, apiKey);
    }

    return {
      statusCode: 200,
      body: JSON.stringify({
        message: "Audio file(s) processed successfully",
        detail: "Transcripts have been stored in the AssemblyAITranscripts directory"
      })
    };
  } catch (error) {
    console.error(`Error: ${error.message}`);
    return {
      statusCode: 500,
      body: JSON.stringify({
        message: "Error processing audio file(s)",
        error: error.message
      })
    };
  }
};
```

</Step>
<Step>

At the top of the Lambda function, you can edit the config to enable features for your transcripts. Since our call is two channels, we'll want to set `multichannel` to `true`. To see all available parameters, check out our [API reference](/docs/api-reference/transcripts/submit).
```javascript
ASSEMBLYAI_CONFIG = {
  'multichannel': true,
  // 'language_code': 'en_us',
  // 'redact_pii': true
  // etc.
}
```

<Tip>
If you would like to delete transcripts from AssemblyAI after completion, you can uncomment `line 212` to enable the `deleteTranscriptFromAssemblyAI` function. This ensures the transcript data is only saved to your S3 bucket and not stored on AssemblyAI's database.
</Tip>

<Note>
For more on our data retention policies, see [this page](https://support.assemblyai.com/articles/2240096256-does-assemblyai-offer-zero-data-retention#default-data-retention-audio-transcripts-4) from our FAQ.
</Note>

</Step>
<Step>

Once you have finished editing the Lambda function, click **Deploy** to save your changes.

<img src="file:068dd076-7df7-4e0e-8a66-7ebc60512313" />

</Step>
<Step>

On the same page, navigate to the **Configuration** section. Under **General configuration**, click **Edit**, and then adjust **Timeout** to `15min 0sec` and click **Save**. The processing times for transcription will be much shorter, but this ensures the function will have plenty of time to run.

<img src="file:a5a42127-ac46-402f-92a6-a091e52ea5fc" />
<img src="file:9a69c4a8-3973-48d7-b372-2c2b99352c45" />

</Step>
<Step>

On the left side panel, click **Environment variables**. Click **Edit**. Add an environment variable, `ASSEMBLYAI_API_KEY`, and set the value to your AssemblyAI [API key](https://www.assemblyai.com/dashboard/api-keys). Then click **Save**.

<img src="file:3f4eea27-5695-49dc-8e53-a8e942964815" />

</Step>
<Step>

Now, navigate to the [IAM](https://us-east-1.console.aws.amazon.com/iam/) services page. On the left side panel under **Access Management**, click **Roles** and search for your Lambda function's role (its structure should look like `<function_name>-<role_id>`). Click the role and then in the **Permissions policies** section click the dropdown for **Add permissions** and then select **Attach policies.**

<img src="file:a4a66818-a3eb-484b-8ff2-10d28c5febb4" />

</Step>
<Step>

From this page, find the policies named `AmazonS3FullAccess` and `CloudWatchEventsFullAccess`. Click **Add permissions** for both.

<img src="file:995417ab-d608-4b60-a45d-c55afdcf1e98" />

<Note>
`CloudWatchEventsFullAccess` is optional, but helpful for debugging purposes. Once your Lambda runs, it should output all logs to [CloudWatch](https://us-east-1.console.aws.amazon.com/cloudwatch) under a Log group `/aws/lambda/<your-lambda-fn>`
</Note>

</Step>
<Step>

Now, navigate to the [S3](https://us-east-1.console.aws.amazon.com/s3) services page and click into the general purpose bucket where your Genesys recordings are stored. Browse to the **Properties** tab and then scroll down to **Event notifications**. Click **Create event notification**.

<img src="file:80fb13a9-9011-4e11-8599-9cac7ec879da" />

</Step>
<Step>

Give the event a name and then in the **Prefix** section enter `calls/` (or whatever `S3_KEY_PREFIX` is set to), and in the **Suffix** section enter `.wav`. This will ensure the event is triggered once our `wav` file has been uploaded. In the **Event types** section, select **All object create events**.

<img src="file:8972bc50-867d-4373-a382-de490d7a8187" />

</Step>
<Step>

Scroll down to the **Destination** section, set the destination as **Lambda function** and then select the Lambda function we created in [Step 6](/docs/integrations/genesys-cloud#step-2-4). Then click **Save changes**.

<img src="file:6517623b-6474-447d-84dc-5f873804c8fb" />

</Step>
</Steps>

## Step 7: Transcribe your first call

<Steps>
<Step>


To test everything is working, call the phone number you linked to this flow in [Step 2](/docs/integrations/genesys-cloud#step-2-setup-a-phone-and-routing-for-your-flow-optional). Referring to the example flow above, press the DTMF value on the key pad or say the Speech Recognition value. Once transferred, your WebSocket server should start to receive data and output to console:

```bash
{
  version: '2',
  id: '<id>',
  type: 'ping',
  seq: 4,
  position: 'PT8.2S',
  parameters: { rtt: 'PT0.035392266S' },
  serverseq: 3
}
Received binary audio data: 3200 bytes 
Received binary audio data: 3200 bytes
...
Processed 146KB of audio data so far
```

</Step>
<Step>

Once the call has ended, you should see the following server logs:

```bash
{
  version: '2',
  id: '<id>',
  type: 'close',
  seq: 5,
  position: 'PT10.2S',
  parameters: { reason: 'end' },
  serverseq: 4
}
Handling close message
Closing file stream
Converting raw audio to WAV: '<wav file name>'

# Skipping ffmpeg output for brevity...

Uploading recording '<raw file>' to S3
Successfully uploaded raw recording to S3: '<raw file>'
Successfully uploaded WAV recording to S3: '<wav file>'
Sent closed response, seq=5
WebSocket closed for session '<session_id>': code=1000, reason=Session Ended
Cleaning up session '<session_id>'
Deleted local raw recording file: '<raw_file>'
Deleted local WAV recording file: '<wav_file>'
```

</Step>
<Step>
  To view the logs for this Lambda function, navigate to the [CloudWatch](https://us-east-1.console.aws.amazon.com/cloudwatch) services page and under the Logs section, select **Log groups**.
  Select the log group that matches your Lambda to view the most recent log stream. This can be very useful for debugging purposes if you run into any issues.

  <img src="file:0614c53f-1f7d-415b-9790-f88651578e9d" />
</Step>
<Step>

Head to your S3 bucket. Within the `/calls` directory, files will be stored under a unique identifier with the following structure:

`your-audiohook-bucket/calls/<timestamp>_<call_id>_<speaker_id>/<file_type>`

with audio files (both `raw` and `wav`) under `/audio` and transcript responses under `/transcripts`.

<img src="file:6ddec148-3f3f-4bde-a2ab-c57767d642c0" />

<Note>
The `raw` file can be nice to have for conversions to other formats in the future, but this step can be omitted to save on storage costs.
</Note>

</Step>

<Step>
**Success!** You have successfully integrated AssemblyAI with Genesys Cloud via AudioHook Monitor. If you run into any issues or have further questions, please reach out to our [Support team](https://www.assemblyai.com/contact/support).
</Step>
</Steps>

## Other considerations

### Supported audio formats
- Audio is sent as binary WebSocket frames containing the raw audio data in the negotiated format. Currently, only μ-law (Mu-law, PCMU) is [supported](https://developer.genesys.cloud/devapps/audiohook/session-walkthrough#audio-streaming).
- Before being uploaded to S3, the audio is converted to `wav` format using [ffmpeg](https://ffmpeg.org/). As a lossless format, `wav` generally results in high transcription accuracy, but is not required. A full list of file formats supported by AssemblyAI's API can be found [here](https://support.assemblyai.com/articles/2616970375-what-audio-and-video-file-types-are-supported-by-your-api).

### Multichannel
- As mentioned in [Step 6](/docs/integrations/genesys-cloud#step-2-4), the `multichannel` parameter should be enabled as the files are stereo utilizing one channel for each participant. When possible, multiple channels are recommended by AssemblyAI for the most accurate transcription results.
- If single channel is preferred, you can simplify the approach to only send a single channel with both speakers (via AudioHook) and adjust your server code to be single channel.


---
title: Transcribe Your Zoom Meetings
---

This guide creates a Node.js service that captures audio from Zoom Real-Time Media Streams (RTMS) and provides both real-time and asynchronous transcription using AssemblyAI.

<Note title="Zoom RTMS Documentation">
  For complete Zoom RTMS documentation, visit
  https://developers.zoom.us/docs/rtms/
</Note>

## Features

- **Real-time Transcription**: Live transcription during meetings using AssemblyAI's streaming API
- **Asynchronous Transcription**: Complete post-meeting transcription with advanced features
- **Flexible Audio Modes**:
  - Mixed stream (all participants combined)
  - Individual participant streams transcribed
- **Multichannel Audio Support**: Separate channels for different participants
- **Configurable Processing**: Enable/disable real-time or async transcription independently

## Setup

### Prerequisites

- Node.js 16+
- FFmpeg installed on your system
- Zoom RTMS Developer Preview access
- AssemblyAI API key
- ngrok (for local development and testing)

### Installation

1. **Clone the example repository and install dependencies**:

```bash
git clone https://github.com/zkleb-aai/assemblyai-zoom-rtms.git
cd assemblyai-zoom-rtms
npm install
```

2. **Configure environment variables**:

```bash
cp .env.example .env
```

Fill in your `.env` file:

```env
# Zoom Configuration
ZM_CLIENT_ID=your_zoom_client_id
ZM_CLIENT_SECRET=your_zoom_client_secret
ZOOM_SECRET_TOKEN=your_webhook_secret_token

# AssemblyAI Configuration
ASSEMBLYAI_API_KEY=your_assemblyai_api_key

# Service Configuration
PORT=8080
REALTIME_ENABLED=true
REALTIME_MODE=mixed
ASYNC_ENABLED=true
AUDIO_CHANNELS=mono
AUDIO_SAMPLE_RATE=16000
TARGET_CHUNK_DURATION_MS=100
```

### Local development with ngrok

For testing and development, you can use ngrok to expose your local server to the internet:

1. **Install ngrok**: Download from [ngrok.com](https://ngrok.com/) or install via package manager:

   ```bash
   # macOS
   brew install ngrok

   # Windows (chocolatey)
   choco install ngrok

   # Or download directly from ngrok.com
   ```

2. **Start your local server**:

   ```bash
   npm start
   ```

3. **In a separate terminal, start ngrok**:

   ```bash
   ngrok http 8080
   ```

4. **Copy the ngrok URL**: ngrok will display a forwarding URL like:

   ```
   Forwarding    https://example-abc123.ngrok-free.app -> http://localhost:8080
   ```

5. **Use the ngrok URL in your Zoom app webhook configuration**:
   ```
   https://example-abc123.ngrok-free.app/webhook
   ```

### Configuration options

#### Real-time transcription

- `REALTIME_ENABLED`: Enable/disable live transcription (default: `true`)
- `REALTIME_MODE`:
  - `mixed`: Single stream with all participants combined
  - `individual`: Separate streams per participant

#### Audio settings

- `AUDIO_CHANNELS`: `mono` or `multichannel`
- `AUDIO_SAMPLE_RATE`: Audio sample rate in Hz (default: `16000`)
- `TARGET_CHUNK_DURATION_MS`: Audio chunk duration for streaming (default: `100`)

#### Async transcription

- `ASYNC_ENABLED`: Enable/disable post-meeting transcription (default: `true`)

## Usage

### Start the service

```bash
npm start
```

The service will start on the configured port (default: 8080) and display:

```
🎧 Zoom RTMS to AssemblyAI Transcription Service
📋 Configuration:
   Real-time: ✅ (mixed)
   Audio: mono @ 16000Hz
   Async: ✅
🚀 Server running on port 8080
📡 Webhook endpoint: http://localhost:8080/webhook
```

### Configure Zoom webhook

1. In your Zoom App configuration, set the webhook endpoint to:

   ```
   # For production
   https://your-domain.com/webhook

   # For local development with ngrok
   https://example-abc123.ngrok-free.app/webhook
   ```

2. Subscribe to these events:
   - `meeting.rtms_started`
   - `meeting.rtms_stopped`

### Testing with ngrok

When using ngrok for testing:

1. **Keep ngrok running**: The ngrok tunnel must remain active during testing
2. **Update webhook URL**: If you restart ngrok, you'll get a new URL that needs to be updated in your Zoom app configuration
3. **Monitor ngrok logs**: ngrok shows incoming webhook requests in its terminal output
4. **Free tier limitations**: The free ngrok tier has some limitations; consider upgrading for heavy testing

### Real-time output

During meetings, you'll see live transcription:

```
🚀 AssemblyAI session started: [abc12345]
🎙️ [abc12345] Hello everyone, welcome to the meeting
📝 [abc12345] FINAL: Hello everyone, welcome to the meeting.
```

### Post-meeting files

After each meeting, the service generates:

- `transcript_[meeting_uuid].json` - Full AssemblyAI response with metadata
- `transcript_[meeting_uuid].txt` - Plain text transcript

## Advanced configuration

### AssemblyAI features

Modify the `ASYNC_CONFIG` object in the code to enable additional features:

```javascript
const ASYNC_CONFIG = {
  speaker_labels: true, // Speaker identification
  auto_chapters: true, // Automatic chapter detection
  sentiment_analysis: true, // Sentiment analysis
  entity_detection: true, // Named entity recognition
  redact_pii: true, // PII redaction
  summarization: true, // Auto-summarization
  auto_highlights: true, // Key highlights
};
```

See [AssemblyAI's API documentation](https://www.assemblyai.com/docs/api-reference/transcripts/submit) for all available options.

### Audio processing modes

#### Mixed mode (default)

- Single audio stream combining all participants
- Most efficient for general transcription
- Best for meetings with clear speakers

#### Individual mode

- Separate transcription stream per participant
- Better speaker attribution
- Higher resource usage

#### Multichannel audio

- Separate audio channels for different participants
- Enables advanced speaker separation
- Requires `AUDIO_CHANNELS=multichannel`

## API endpoints

### `POST` /webhook

Handles Zoom RTMS webhook events:

- URL validation
- Meeting start/stop events
- Automatic RTMS connection setup

## Error handling

The service includes comprehensive error handling:

- Automatic reconnection for dropped connections
- Graceful cleanup on meeting end
- Audio buffer flushing to prevent data loss
- Temporary file cleanup

## Monitoring

### Real-time logs

- Connection status updates
- Audio processing statistics
- Transcription progress
- Error notifications

### Example log output

```
📡 Connecting to Zoom signaling for meeting abc123
✅ Zoom signaling connected for meeting abc123
🎵 Connecting to Zoom media for meeting abc123
✅ Zoom media connected for meeting abc123
🚀 Started audio streaming for meeting abc123
🎵 [abc12345] 100 chunks, 32768 bytes, 10.2s
📝 [abc12345] FINAL: This is the final transcription.
```

### Development workflow

1. Start your local server: `npm start`
2. Start ngrok in another terminal: `ngrok http 8080`
3. Update your Zoom app webhook URL with the ngrok URL
4. Test with Zoom meetings
5. Monitor logs in both your app and ngrok terminals


---
title: Semantic Kernel Integration for AssemblyAI
description: >-
  Transcribe audio in Semantic Kernel for C# .NET using the built-in integration
  with AssemblyAI.
hide-nav-links: true
---

Semantic Kernel is an SDK for multiple programming languages to develop applications with [Large Language Models (LLMs)](https://www.assemblyai.com/blog/introduction-large-language-models-generative-ai/#what-are-language-models).
However, LLMs only operate on textual data and don't understand what is said in audio files.
With the [AssemblyAI integration for Semantic Kernel](https://github.com/AssemblyAI/assemblyai-semantic-kernel), you can use AssemblyAI's transcription models using the `TranscribePlugin` to transcribe your audio and video files.

## Quickstart

Add the [AssemblyAI.SemanticKernel NuGet package](https://www.nuget.org/packages/AssemblyAI.SemanticKernel) to your project.

<Tabs>
  <Tab  title="dotnet CLI">

```bash
dotnet add package AssemblyAI.SemanticKernel
```

  </Tab>
  <Tab title="Package Manager Console">

```powershell
Install-Package AssemblyAI.SemanticKernel
```

  </Tab>
</Tabs>

Next, register the `TranscriptPlugin` into your kernel:

```csharp
using AssemblyAI.SemanticKernel;
using Microsoft.SemanticKernel;

// Build your kernel
var kernel = Kernel.CreateBuilder();

// Get AssemblyAI API key from env variables, or much better, from .NET configuration
string apiKey = Environment.GetEnvironmentVariable("ASSEMBLYAI_API_KEY")
  ?? throw new Exception("ASSEMBLYAI_API_KEY env variable not configured.");

kernel.ImportPluginFromObject(
    new TranscriptPlugin(apiKey: apiKey)
);
```

## Usage

Get the `Transcribe` function from the transcript plugin and invoke it with the context variables.

```csharp
var result = await kernel.InvokeAsync(
    nameof(TranscriptPlugin),
    TranscriptPlugin.TranscribeFunctionName,
    new KernelArguments
    {
        ["INPUT"] = "https://assembly.ai/espn.m4a"
    }
);
Console.WriteLine(result.GetValue<string>());
```

You can get the transcript using `result.GetValue<string>()`.

You can also upload local audio and video file. To do this:

- Set the `TranscriptPlugin.AllowFileSystemAccess` property to `true`.
- Configure the `INPUT` variable with a local file path.

```csharp
kernel.ImportPluginFromObject(
    new TranscriptPlugin(apiKey: apiKey)
    {
        AllowFileSystemAccess = true
    }
);
var result = await kernel.InvokeAsync(
    nameof(TranscriptPlugin),
    TranscriptPlugin.TranscribeFunctionName,
    new KernelArguments
    {
        ["INPUT"] = "https://assembly.ai/espn.m4a"
    }
);
Console.WriteLine(result.GetValue<string>());
```

You can also invoke the function from within a semantic function like this.

```csharp
string prompt = """
                Here is a transcript:
                {{TranscriptPlugin.Transcribe "https://assembly.ai/espn.m4a"}}
                ---
                Summarize the transcript.
                """;
var result = await kernel.InvokePromptAsync(prompt);
Console.WriteLine(result.GetValue<string>());
```

## Additional resources

You can learn more about using Semantic Kernel with AssemblyAI in these resources:

- [Ask .NET Rocks! questions with Semantic Kernel, GPT, and Chroma DB](https://www.assemblyai.com/blog/ask-dotnetrocks-questions-semantic-kernel/)
- [AssemblyAI integration for Semantic Kernel GitHub repository](https://github.com/AssemblyAI/assemblyai-semantic-kernel)


---
title: Integrate Activepieces with AssemblyAI
description: Add Speech AI to your Activepieces flows with the AssemblyAI piece.
hide-nav-links: true
---

[Activepieces](https://www.activepieces.com/) is an open-source, no-code automation platform that enables users to streamline workflows by connecting various applications and automating tasks.

With the AssemblyAI piece for Activepieces, you can use AssemblyAI to transcribe audio data with speech recognition models, analyze the data with audio intelligence models, and build generative features on top of it with LLMs.
You can supply audio to the AssemblyAI piece and connect the output of any of AssemblyAI's models to other services in your Activepieces flow.

## Quickstart

<Steps>
<Step>
Create or edit a flow in Activepiece. Add a trigger of your choosing, and then click the plus-icon to add a new action.
Search for AssemblyAI, click on the AssemblyAI piece, and select the action that you want to use.

![Add an AssemblyAI piece action](file:154a2490-0568-4b1c-95f8-794fb115503d)

</Step>
<Step>

Create a new connection or select an existing one.

![Select a connection to AssemblyAI in Activepieces](file:c2abc406-9229-4c96-9b98-7df0c8c240e1)

In the **API Key** field, enter the API key from your [AssemblyAI dashboard](https://www.assemblyai.com/app/api-keys), and click **Save**.

![Configure your connection to AssemblyAI in Activepieces](file:03d29939-f689-41b4-bbd2-329485b5bddd)

</Step>
<Step>

Finally, configure your AssemblyAI action. Continue reading to learn more about all the available action.

</Step>
</Steps>

## AssemblyAI actions

The AssemblyAI piece for Activepieces provides the following actions:

### Files

#### Upload File

Upload an audio file to AssemblyAI so you can transcribe it.
You can pass the `Upload URL` output field to the `Audio URL` input field of Transcribe an Audio File module.

### Transcripts

#### Transcribe

Transcribe an audio file and wait until the transcript has completed or failed.
Configure the `Audio URL` field with the URL of the audio file you want to transcribe.
The `Audio URL` must be accessible by AssemblyAI's servers.
If you don't have a publicly accessible URL, you can use the Upload a File module to upload the audio file to AssemblyAI.

If you don't want to wait until the transcript is ready, uncheck the `Wait until transcript is ready` parameter.

<Info>
  Configure your desired [Audio Intelligence models](/audio-intelligence) when
  you create the transcript. The results of the models will be included in the
  transcript output.
</Info>

#### Get Transcript

Retrieve a transcript by ID.

#### Get Transcript Paragraphs

Retrieve the paragraphs of a transcript.

<Note>You can only invoke this module after the transcript is completed.</Note>

#### Get Transcript Sentences

Retrieve the sentences of a transcript.

<Note>You can only invoke this module after the transcript is completed.</Note>

#### Get Transcript Subtitles

Create SRT or VTT subtitles for a transcript.

<Note>You can only invoke this module after the transcript is completed.</Note>

#### Get Transcript Redacted Audio

First, you need to configure PII audio redaction using these fields when you create the transcript:

- `Redact PII`: `Checked`
- `Redact PII Audio`: `Checked`
- `Redact PII Policies`: Configure at least one PII policy

Then, you can use this module to retrieve the redacted audio of the transcript.

<Note>You can only invoke this module after the transcript is completed.</Note>

#### Search words in transcript

Search for words in a transcript.

<Note>You can only invoke this module after the transcript is completed.</Note>

#### List transcripts

Paginate over all transcripts.

#### Delete transcript

Delete a transcript by ID.
Deleting a transcript doesn't delete the transcript resource itself, but removes the data from the resource and marks it as deleted.

<Note>
  You can only invoke this module after the transcript status is "completed" or
  "error".
</Note>

### LeMUR

#### Run a Task using LeMUR

Prompt different LLMs over your audio data using LeMUR.
You have to configure either the `Transcript IDs` or `Input Text` input field.

#### Retrieve LeMUR response

Retrieve a LeMUR response that was previously generated.

#### Purge LeMUR request data

Delete the data for a previously submitted LeMUR request.
Response data from the LLM, as well as any context provided in the original request will be removed.

### Other actions

#### Custom API Call

Make your own REST API HTTP requests to the AssemblyAI API using your existing connection.

## Additional resources

You can learn more about using Activepieces with AssemblyAI in these resources:

- [AssemblyAI Integrations on Activepieces](https://www.activepieces.com/pieces/assemblyai)
- [npmjs page for @activepieces/piece-assemblyai](https://www.npmjs.com/package/@activepieces/piece-assemblyai)


---
title: Integrate Make with AssemblyAI
description: >-
  Use our Make (formerly Integromat) app to use AssemblyAI's speech AI in your
  Make scenarios.
hide-nav-links: true
---

[Make](https://make.com/) (formerly Integromat) is a workflow automation tool that lets you integrate various services together without requiring coding knowledge.

With the AssemblyAI app for Make, you can use our AI models to process audio data by transcribing it with speech recognition models, analyzing it with audio intelligence models, and building generative features on top of it with LLMs.
You can supply audio to the AssemblyAI app and connect the output of our models to other services in your Make scenarios.

## Quickstart

<Steps>
<Step>

Create or edit a scenario in Make.
Add a new module, search for AssemblyAI, and select the module that you want to use.

![Search for AssemblyAI modules in Make](file:a528223d-d045-47eb-8270-9ceefbcd3d79)

</Step>

<Step>

Select the module that you want to use.

![Add an AssemblyAI module in Make](file:90d98d83-d46f-4264-be95-791ec4267786)

</Step>

<Step>

Create a new connection or select an existing one.
In **AssemblyAI API Key**, enter the API key from your [AssemblyAI dashboard](https://www.assemblyai.com/app/api-keys), and click **Save**.

![Create a connection to AssemblyAI in Make](file:f1781b19-e664-44f4-957a-c1a9f8f8cbce)

</Step>

<Step>

Finally, configure your AssemblyAI module. Continue reading to learn more about all the available modules.

![Configure an AssemblyAI module in Make](file:fa4b1b4d-d950-4ed8-a8f5-f0c976e91d24)

</Step>
</Steps>

## AssemblyAI app modules

The AssemblyAI app for Make provides the following modules:

### Files

#### Upload a File

Upload an audio file to AssemblyAI so you can transcribe it.
You can pass the `Upload URL` output field to the `Audio URL` input field of [Transcribe an Audio File](#transcribe-an-audio-file) module.

### Transcripts

#### Transcribe an Audio File

Transcribe an audio file and wait until the transcript has completed or failed.
Configure the `Audio URL` field with the URL of the audio file you want to transcribe.
The `Audio URL` must be accessible by AssemblyAI's servers.
If you don't have a publicly accessible URL, you can use the [Upload a File](#upload-a-file) module to upload the audio file to AssemblyAI.

If you don't want to wait until the transcript is ready, change the `Wait until Transcript is Ready` parameter to `No` under **Show advanced settings**.

<Info>
  Configure your desired [Audio Intelligence models](/audio-intelligence) when
  you create the transcript. The results of the models will be included in the
  transcript output.
</Info>

#### Wait until Transcript is Ready

Wait for an existing transcript to be ready.
This module will complete when the status of the transcript changes to "completed" or "error".

#### Watch for Transcript Ready Notification

Create a webhook URL to receive a notification when a transcript is ready.
When the transcript is ready, the webhook will be invoked with the transcript status and ID.
The status will be "completed" or "error".

#### Get a Transcript

Retrieve a transcript by ID.

#### Get Paragraphs of a Transcript

Retrieve the paragraphs of a transcript.

<Note>You can only invoke this module after the transcript is completed.</Note>

#### Get Sentences of a Transcript

Retrieve the sentences of a transcript.

<Note>You can only invoke this module after the transcript is completed.</Note>

#### Get Subtitles for a Transcript

Create SRT or VTT subtitles for a transcript.

<Note>You can only invoke this module after the transcript is completed.</Note>

#### Get Redacted Audio of a Transcript

First, you need to configure PII audio redaction using these fields when you create the transcript:

- `Redact PII`: `Yes`
- `Redact PII Audio`: `Yes`
- `Redact PII Policies`: Configure at least one PII policy

Then, you can use this module to retrieve the redacted audio of the transcript.

<Note>You can only invoke this module after the transcript is completed.</Note>

#### Search for Words in a Transcript

Search for words in a transcript.

<Note>You can only invoke this module after the transcript is completed.</Note>

#### List Transcripts

Paginate over all transcripts.

#### Delete a Transcript

Delete a transcript by ID.
Deleting a transcript does not delete the transcript resource itself, but removes the data from the resource and marks it as deleted.

<Note>
  You can only invoke this module after the transcript status is "completed" or
  "error".
</Note>

### LeMUR

#### Run a Task using LeMUR

Prompt different LLMs over your audio data using LeMUR.
You have to configure either the `Transcript IDs` or `Input Text` input field.

#### Purge a LeMUR Request

Delete the data for a previously submitted LeMUR request.
Response data from the LLM, as well as any context provided in the original request will be removed.

### Other modules

#### Make an API Call

Make your own REST API HTTP requests to the AssemblyAI API using your existing connection.

## Additional resources

You can learn more about using Make with AssemblyAI in these resources:

- [Redact PII in Audio with Make and AssemblyAI](https://www.assemblyai.com/blog/redact-pii-audio-with-make/)
- [Make Integration page for AssemblyAI](https://www.make.com/en/integrations/assembly-ai)
- [AssemblyAI Make App Invitation Link](https://us1.make.com/app/invite/f21437a4d43b63efc8ee9aec385d9f10)


---
title: The Postman collection for the AssemblyAI API
description: Use the AssemblyAI API Postman collection to experiment with our API.
hide-nav-links: true
---

Postman is a user-friendly tool for testing API endpoints. The AssemblyAI API Postman collection contains all the HTTP requests you can make to the AssemblyAI API, so you don't need to write them yourself.

<Card
  icon="signs-post"
  title="AssemblyAI API Postman collection"
  href="https://assembly.ai/postman"
/>

## Quickstart

<Steps>
<Step>

Open the [AssemblyAI API collection](https://assembly.ai/postman) in Postman and click the **Fork** button. This will create a copy of the collection that you can edit.

![Click Fork on the AssemblyAI API collection](file:c8a57401-b2cd-43c4-ad99-ba867b771e37)

Fill out the form and click **Fork Collection**.

</Step>
<Step>

Next, click on the **Variables** tab and configure the `apiKey` variable with your AssemblyAI API key.
You can find your AssemblyAI API key in the [AssemblyAI dashboard](https://www.assemblyai.com/app/api-keys).

![Configure AssemblyAI API key in Postman collection](file:32ebe4c3-d9fe-42d4-af2e-67c0373421cc)

</Step>
<Step>

Let's upload an audio file:

1. Open the **Files > Upload a media file** request
2. Switch to the **Body** tab
3. Change the dropdown from **none** to **binary**
4. Select an audio file of your choosing, or [download this sample audio file](https://assembly.ai/nbc.mp3)
5. Click the **Send** button

![Send upload audio file to AssemblYAI API HTTP request](file:202f9f5f-b6b3-4bda-8327-2c0901c9e842)

Inspect the **Body** of the response and copy the `upload_url` value.

![The response for uploading an audio file to AssemblYAI API](file:215651da-232b-4c66-8a34-03e1e9227703)

</Step>
<Step>

Now that the audio file is uploaded, you can transcribe the audio file.

1. Open the **Transcripts > Transcribe audio** request
2. Switch to the **Body** tab
3. Find the `audio_url` property and update it to the `upload_url` value from the previous request.
4. Remove all the other properties. Optionally, you can leave any property that you do want to use.
5. Click the **Send** button

![Create a transcript HTTP request](file:b245a38b-ef92-4fe7-a985-a4c45b85e66a)

Inspect the **Body** of the response and copy the `id` value.

![Create a transcript HTTP response](file:1d656a79-6f3e-43f7-86b7-f69b8c7e4da2)

</Step>
<Step>

The transcription job will take longer depending on the duration of the file.
You need to check the `status` property to check if a transcript is ready.
The `status` goes from `queued` to `processing` to `completed`.
The `status` can also become `error` at any point. If an error occurs, you can find the error message under the `error` property.

1. Open the **Transcripts > Get transcript** request
2. Find the `transcript_id` under **Path variables** and update the value with the `id` value from the previous request.
3. Remove all the other properties. Optionally, you can leave any property that you do want to use.
4. Click the **Send** button

![Get a transcript HTTP request and response](file:69f68c8f-9fd5-499f-9530-9b4c94a4d0f4)

Inspect the **Body** of the response to check if the `status` is `completed` or `error`.
If not, resend the request until it is `completed` or `error`.

</Step>
<Step>

Now that you have a completed transcript, you can send these other HTTP requests with your current transcript ID:

- Transcripts
  - Get subtitles for transcript
  - Get sentences in transcript
  - Get paragraphs in transcript
  - Search words in transcript
  - Get redacted audio (if PII audio redaction is enabled)
- LeMUR
  - Run a task using LeMUR
  - Summarize a transcript using LeMUR
  - Ask questions using LeMUR
  - Extract action items

Let's prompt an LLM to summarize the transcript using LeMUR:

1. Open the **LeMUR > Run a task using LeMUR** request
2. Switch to the **Body** tab
3. Find the `transcript_ids` property and replace the sample ID with your transcript ID
4. Set the `final_model` property to ` "anthropic/claude-3-5-sonnet"`
5. Set the `prompt` property to `"Summarize the transcript"`
6. Remove the other properties
7. Click the **Send** button

![Summarize transcript using LeMUR](file:b53639d5-51dd-49f3-97e2-feb543de6bab)

</Step>
</Steps>


---
title: Integrate Power Automate with AssemblyAI
description: >-
  Use our Power Automate & Azure Logic Apps connector to use AssemblyAI's speech
  AI in your flows.
hide-nav-links: true
---

[Microsoft Power Automate](https://www.microsoft.com/en-us/power-platform/products/power-automate) is a low-code workflow automation platform with a rich collection of connectors to Microsoft's first-party services and third-party services. [Azure Logic Apps](https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-overview) is the equivalent service built for developers and IT pros.

The AssemblyAI connector makes our API available to both Microsoft Power Automate and Azure Logic Apps.
With the connector, you can use AssemblyAI to transcribe audio data with speech recognition models, analyze the data with audio intelligence models, and build generative features on top of it with LLMs.
You can supply audio to the AssemblyAI connector and connect the output of our models to other services in your flows.

## Quickstart

<Steps>
<Step>

Create or edit a flow in Power Automate.
Add a new action, search for AssemblyAI, and select the action that you want to use.

![Search for AssemblyAI actions in Power Automate](file:23d1ace1-d091-4be0-8a16-82b45308d4f1)

</Step>
<Step>

You will be prompted to create a connection to AssemblyAI. Give your connection a name and enter the API key from your [AssemblyAI dashboard](https://www.assemblyai.com/app/api-keys), and click **Create new**.

![Create a connection to AssemblyAI in Power Automate](file:247e49b1-f205-4d76-91f4-aac14fdd5a6d)

</Step>
<Step>

Finally, configure your AssemblyAI action. Continue reading to learn more about all the available actions.

![Configure an AssemblyAI action in Power Automate](file:05b501a0-229f-49d4-b352-8f74431e9ef9)

</Step>
</Steps>

## Upload a File

To transcribe an audio file using AssemblyAI, the file needs to be accessible to AssemblyAI.
If your audio file is already accessible via a URL, you can use your existing URL.

Otherwise, you can use the `Upload a Media File` action to upload a file to AssemblyAI.
You will get back a URL for your file which can only be used to transcribe using your API key.
Once you transcribe the file, the file will be removed from AssemblyAI's servers.

## Transcribe Audio

To transcribe your audio, configure the `Audio URL` parameter using your audio file URL.
Then, configure the additional parameters to enable more [Speech Recognition](https://www.assemblyai.com/docs/speech-to-text/pre-recorded-audio) features and [Audio Intelligence](https://www.assemblyai.com/docs/audio-intelligence) models.

The result of the Transcribe Audio action is a queued transcript which will start being processed immediately.
To get the completed transcript, you have two options:

1. [Handle the Transcript Ready Webhook](#handle-the-transcript-ready-webhook)
2. [Poll the Transcript Status](#poll-the-transcript-status)

### Handle the Transcript Ready Webhook

If you don't want to handle the webhook using Logic Apps or Power Automate, configure the `Webhook URL` parameter in your `Transcribe Audio` action, and implement your webhook following [AssemblyAI's webhook documentation](https://www.assemblyai.com/docs/getting-started/webhooks#handle-webhook-deliveries).

To handle the webhook using Logic Apps or Power Automate, follow these steps:

<Steps>
<Step>

Create a separate Logic App or Power Automate Flow.

</Step>
<Step>

Configure `When an HTTP request is received` as the trigger:

- Set `Who Can Trigger The Flow?` to `Anyone`
- Set `Request Body JSON Schema` to:
  ```json
  {
    "type": "object",
    "properties": {
      "transcript_id": {
        "type": "string"
      },
      "status": {
        "type": "string"
      }
    }
  }
  ```
- Set `Method` to `POST`

</Step>
<Step>

Add an AssemblyAI `Get Transcript` action, passing in the `transcript_id` from the trigger to the `Transcript ID` parameter.

</Step>
<Step>

Before doing anything else, you should check whether the `Status` is `completed` or `error`.
Add a `Condition` action that checks if the `Status` from the `Get Transcript` output is `error`:

- In the `True` branch, add a `Terminate` action
  - Set the `Status` to `Failed`
  - Set the `Code` to `Transcript Error`
  - Pass the `Error` from the `Get Transcript` output to the `Message` parameter.
- You can leave the `False` branch empty.

Now you can add any action after the `Condition` knowing the transcript status is `completed`,
and you can retrieve any of the output properties of the `Get Transcript` action.

</Step>
<Step>

Save your Logic App or Flow. The `HTTP URL` will be generated for the `When an HTTP request is received` trigger.
Copy the `HTTP URL` and head back to your original Logic App or Flow.

</Step>
<Step>

In your original Logic App or Flow, update the `Transcribe Audio` action.
Paste the `HTTP URL` you copied previously into the `Webhook URL` parameter, and save.

</Step>
</Steps>

When the transcript status becomes `completed` or `error`, AssemblyAI will send an HTTP POST request to the webhook URL,
which will be handled by your other Logic App or Flow.

As an alternative to using the webhook, you can poll the transcript status as explained in the next section.

### Poll the Transcript Status

You can poll the transcript status using the following steps:

<Steps>
<Step>

Add an `Initialize variable` action

- Set `Name` to `transcript_status`
- Set `Type` to `String`
- Store the `Status` from the `Transcribe Audio` output into the `Value` parameter

</Step>
<Step>

Add a `Do until` action

- Configure the `Loop Until` parameter with the following Fx code:
  ```plaintext
  or(equals(variables('transcript_status'), 'completed'), equals(variables('transcript_status'), 'error'))
  ```
  This code checks whether the `transcript_status` variable is `completed` or `error`.
- Configure the `Count` parameter to `86400`
- Configure the `Timeout` parameter to `PT24H`

Inside the `Do until` action, add the following actions:

- Add a `Delay` action that waits for one second
- Add a `Get Transcript` action and pass the `ID` from the `Transcribe Audio` output to the `Transcript ID` parameter
- Add a `Set variable` action
  - Set `Name` to `transcript_status`
  - Pass the `Status` of the `Get Transcript` output to the `Value` parameter

The `Do until` loop will continue until the transcript is completed, or an error occurred.

</Step>
<Step>

Add another `Get Transcript` action, like before, but add it after the `Do until` loop so its output becomes available outside the scope of the `Do until` action.

</Step>
<Step>

Before doing anything else, you should check whether the transcript `Status` is `completed` or `error`.
Add a `Condition` action that checks if the `transcript_status` is `error`:

- In the `True` branch, add a `Terminate` action
  - Set `Status` to `Failed`
  - Set `Code` to `Transcript Error`
  - Pass the `Error` from the `Get Transcript` output to the `Message` parameter.
- You can leave the `False` branch empty.

</Step>
</Steps>

Now you can add any action after the `Condition` knowing the transcript status is `completed`,
and you can retrieve any of the output properties of the `Get Transcript` action.

## Connector actions

The AssemblyAI app for Power Automate provides the following actions:

### Files

#### Upload a Media File

Upload a media file to AssemblyAI's servers.
You can pass the `Upload URL` output field to the `Audio URL` input field of [Transcribe an Audio File](#transcribe-audio) action.

### Transcripts

#### Transcribe Audio

Create a transcript from a media file that is accessible via a URL.
Configure the `Audio URL` field with the URL of the audio file you want to transcribe.
The `Audio URL` must be accessible by AssemblyAI's servers.
If you don't have a publicly accessible URL, you can use the [Upload a File](#upload-a-file) action to upload the audio file to AssemblyAI.

<Warning title="Wait until transcript is ready">

The output of this action is a `queued` transcript. Learn [how to wait until the transcript is ready here](#transcribe-audio).

</Warning>

<Info>
  Configure your desired [Audio Intelligence models](/audio-intelligence) when
  you create the transcript. The results of the models will be included in the
  transcript output when the transcript is completed.
</Info>

#### Get Transcript

Get the transcript resource. The transcript is ready when the `status` is `completed`.

#### Get Paragraphs in Transcript

Get the transcript split by paragraphs. The API semantically segments your transcript into paragraphs to create more reader-friendly transcripts.

<Note>You can only invoke this action after the transcript is completed.</Note>

#### Get Sentences in Transcript

Get the transcript split by sentences. The API semantically segments the transcript into sentences to create more reader-friendly transcripts.

<Note>You can only invoke this action after the transcript is completed.</Note>

#### Get Subtitles for Transcript

Get the transcript resource. The transcript is ready when the `status` is `completed`.

<Note>You can only invoke this action after the transcript is completed.</Note>

#### Get Redacted Audio

First, you need to configure PII audio redaction using these fields when you create the transcript:

- `Redact PII`: `Yes`
- `Redact PII Audio`: `Yes`
- `Redact PII Policies`: Configure at least one PII policy

Then, you can use this action to retrieve the redacted audio of the transcript.

<Note>You can only invoke this action after the transcript is completed.</Note>

#### Search Words in Transcript

Search through the transcript for keywords. You can search for individual words, numbers, or phrases containing up to five words or numbers.

<Note>You can only invoke this action after the transcript is completed.</Note>

#### List Transcripts

Get the transcript resource. The transcript is ready when the `status` is `completed`.

#### Delete Transcript

Delete the transcript. Deleting does not delete the resource itself, but removes the data from the resource and marks it as deleted.

<Note>
  You can only invoke this action after the transcript status is `completed` or
  `error`.
</Note>

### LeMUR

#### Run a Task Using LeMUR

Use the LeMUR task endpoint to input your own LLM prompt.
You have to configure either the `Transcript IDs` or `Input Text` input field.

#### Retrieve LeMUR Response

Retrieve a LeMUR response that was previously generated.

#### Purge LeMUR Request Data

Delete the data for a previously submitted LeMUR request. The LLM response data, as well as any context provided in the original request will be removed.

## Additional resources

You can learn more about using Power Automate with AssemblyAI in these resources:

- [Power Automate & Logic Apps docs by Microsoft](https://learn.microsoft.com/en-us/connectors/assemblyai/)

---
title: Zapier Integration with AssemblyAI
description: Transcribe audio in Zapier using the AssemblyAI app.
hide-nav-links: true
---

Zapier is a workflow automation tool that lets you integrate various services together without requiring coding knowledge.
You can use our AI models to process audio data by transcribing it with speech recognition models and analyzing it with audio intelligence models.
You can supply audio to the AssemblyAI app and connect the output of our models to other services in your Zaps.

## Quickstart

<Steps>
<Step>
In your Zap editor, add an action, search for `AssemblyAI` and select the AssemblyAI app.

![Change action Zapier screen, with AssemblyAI in search box.](file:7513bacd-0aae-46f3-9a55-9439de7d366f)

</Step>

<Step>
Next, configure the action.
In the **App & event** tab, select **Transcribe** for the **Event** dropdown, then click **Continue**.

![App & event Zapier tab with Event field set to "Transcribe".](file:5d548740-48f2-4a20-978f-f6ed278369e6)

</Step>

<Step>
Then, in the **Account** tab, click **Sign in** which will open a separate window.
In the window, enter your AssemblyAI API key in **API Key** field, and click **Yes, Continue to AssemblyAI**.
Back in the Zap editor, click **Continue**.

![Account Zapier tab where you are prompted to Connect AssemblyAI with a Sign in button.](file:1ccbe32a-2c30-4326-8a54-38f305238ed1)

</Step>

<Step>
In the **Action** tab, enter the URL of the audio or video file you want to transcribe in the **Audio URL** field.
The URL has to be publicly accessible. Click **Continue**.

![Action Zapier tab where you are prompted to enter the Audio URL to transcribe using AssemblyAI.](file:773cd962-f612-4015-b718-85c89fe1e50f)

</Step>

<Step>
Finally, you can test the action. You can use all the fields returned by the action in subsequent steps.

<Note>
  All AssemblyAI actions return sample data during testing instead of running
  the action. This makes it easier to build your Zaps, however, you have to test
  using normal Zap runs to verify everything is working correctly. Learn more
  about why we [return sample data during testing
  below](#testing-with-sample-data).
</Note>

![Test Zapier tab where you can see the output of the AssemblyAI Transcribe action.](file:a19b1020-c307-492b-9275-ad434129e31f)

</Step>
</Steps>

## Zapier Actions

### Transcribe

Transcribe an audio file and wait until the transcript has completed or failed.
Configure the `Audio URL` field with the URL of the audio file you want to transcribe.
The `Audio URL` must be accessible by AssemblyAI's servers.

If you don't want to wait until the transcript is ready, change the `Wait until Transcript is Ready` parameter to `False`.

### Get Transcript

Retrieves a transcript by its ID.

### Get Transcript Subtitles

Export the transcript as SRT or VTT subtitles.

<Note>You can only invoke this action after the transcript is completed.</Note>

### Get Transcript Sentences

Retrieve the sentences of the transcript by its ID.

<Note>You can only invoke this action after the transcript is completed.</Note>

### Get Transcript Paragraphs

Retrieve the paragraphs of the transcript by its ID.

<Note>You can only invoke this action after the transcript is completed.</Note>

### Get Transcript Redacted Audio Result

Get the result of the redacted audio model.

<Note>You can only invoke this action after the transcript is completed.</Note>

### What about LeMUR?

Unfortunately, the Zapier platform doesn't have the necessary features for us to reliably offer LeMUR, our LLM framework for speech understanding.
We're looking at different avenues to add support in the future, but we have no timeline for when LeMUR support will be available.

## Testing with sample data

A transcript goes through multiple phases to transcribe audio, reflected by different statuses.
The initial status is `queued`, immediately followed by `processing`, and the final status is either `completed` or `error`.
**During a normal Zap run**, the Transcribe event will wait until the transcript status is `completed`, and throw an error if the status is `error`.
Unfortunately, this is not the case during testing.

Because of a Zapier platform limitation, **during testing**, the Transcribe event will return a transcript with the `queued` status.
A transcript that does not have the `completed` status cannot be used in subsequent tests.

This way you can easily test using sample data, but you still have to use normal Zap runs to verify the end-to-end functionality.

## Additional resources

You can learn more about using Zapier with AssemblyAI in these resources:

- [How to generate subtitles for your videos using the AssemblyAI app for Zapier](https://www.assemblyai.com/blog/generate-subtitles-with-zapier)
- [How to get started with AssemblyAI on Zapier](https://help.zapier.com/hc/en-us/articles/16411509681933-How-to-get-started-with-AssemblyAI-on-Zapier)
- [AssemblyAI app on Zapier](https://zapier.com/apps/assemblyai/integrations)


---
title: AssemblyAI plugin for Rivet
description: Transcribe audio in Rivet using the AssemblyAI app.
hide-nav-links: true
---

[Rivet](https://rivet.ironcladapp.com/) is an open-source visual AI programming environment.
Through a collaboration between AssemblyAI and Rivet, you can use AssemblyAI speech-to-text and LeMUR capabilities in Rivet.

## Quickstart

<Steps>
<Step>
In your Rivet project, switch to the **Plugins** tab, find the AssemblyAI plugin and click **Add**.

![AssemblyAI plugin in the Rivet Plugins tab.](file:0102681e-e396-4729-be1f-986ed49f11cb)

</Step>
<Step>

Next, swap back to the **Canvas** tab, click the three-dotted button, and then click **Settings** in the context menu.
In the **Settings** dialog, switch to the **Plugins** tab, and enter your AssemblyAI API key in the **AssemblyAI API Key** field.

![AssemblyAI API key field in Rivet Settings.](file:fcffef39-d488-4e9c-9364-44518e40957d)

</Step>
<Step>

Now you can add AssemblyAI nodes to your canvas by right-clicking on the canvas, and select any of the nodes under the **AssemblyAI** category.

</Step>
</Steps>

## Nodes

### Transcribe Audio node

The Transcribe Audio node transcribes audio using the [AssemblyAI API](https://www.assemblyai.com/docs/api-reference/overview) . It will return a transcript of the given audio source.

![Transcribe Audio node](file:2143d3ce-d700-431a-a18b-dbe6123d008e)

### LeMUR nodes

LeMUR is a framework by AssemblyAI to process audio files with an LLM.
The AssemblyAI plugin has a dedicated node for each LeMUR endpoint.
Each node accepts Transcript IDs or Input Text as input which you can get from the Transcribe Audio node. Additional parameters are available as inputs and as node configuration. For more information what these parameters do, see [LeMUR API reference](https://www.assemblyai.com/docs/api-reference/lemur).

#### LeMUR Summary node

The LeMUR Summary node uses LeMUR to summarize a given transcript.

![LeMUR Summary node](file:e2f2b892-aa80-4311-bcb6-3cd8d89078b7)

#### LeMUR Q&A node

LeMUR can generate answers from a transcript and questions.

![LeMUR Question & Answer node](file:3e3cc151-fa15-47b6-b190-2c35eb906527)

#### LeMUR Custom Task node

LeMUR can generate a response from a prompt and transcript.

![LeMUR Custom Task node](file:2cf207e1-6643-4eee-8641-817dd3fef1b6)

#### LeMUR Action Items node

The LeMUR Action Items node returns a list of action items from a meeting transcript.

![LeMUR Action Items node](file:c2017d84-4b79-4f34-8b6e-06b7e8a771f0)

## Additional resources

You can learn more about using Rivet with AssemblyAI in these resources:

- [Build a podcast question & answer application using Rivet and AssemblyAI](https://www.assemblyai.com/blog/podcast-qa-application-rivet/)
- [Rivet](https://rivet.ironcladapp.com/)
- [Rivet Docs](https://rivet.ironcladapp.com/docs)
- [Rivet AssemblyAI Docs](https://rivet.ironcladapp.com/docs/user-guide/plugins/built-in/assemblyai)
- [Rivet GitHub repository](https://github.com/ironclad/rivet)


---
title: Haystack Integration for AssemblyAI
description: >-
  Transcribe, summarize and diarize audio in a Haystack pipeline with Python
  using the integration with AssemblyAI.
hide-nav-links: true
---

[Haystack (2.x)](https://github.com/deepset-ai/haystack) is an open-source Python framework for building custom LLM applications. The Haystack Integration for AssemblyAI seamlessly integrates with Haystack to use audio files in LLM pipelines.

On top of audio transcription the AssemblyAITranscriber offers summarization and speaker diarization. This makes it possible to not only convert audio to text but also obtain concise summaries and identify speakers in a conversation.

## Quickstart

Install the [assemblyai-haystack package](https://pypi.org/project/assemblyai-haystack/) using pip. This package installs and uses the AssemblyAI Python SDK and Haystack 2.0. You can find more information about the SDK at the [AssemblyAI Python SDK GitHub repository](https://github.com/AssemblyAI/assemblyai-python-sdk).

```bash
pip install assemblyai-haystack
```

## Usage

Add an `AssemblyAITranscriber` component and initialize it by passing your AssemblyAI API key. Once the pipeline is ready to run, make sure to pass at least the `file_path` argument to the `run` function. The `file_path` can be a `URL` or a local file path. In the `run` function, you can also specify whether you want summarization and speaker diarization results.

```python


from assemblyai_haystack.transcriber
from haystack.document_stores.in_memory
from haystack
from haystack.components.writers

ASSEMBLYAI_API_KEY = os.environ.get("ASSEMBLYAI_API_KEY")

document_store = InMemoryDocumentStore()
file_url = "https://assembly.ai/wildfires.mp3"

indexing = Pipeline()
indexing.add_component("transcriber", AssemblyAITranscriber(api_key=ASSEMBLYAI_API_KEY))
indexing.add_component("writer", DocumentWriter(document_store))
indexing.connect("transcriber.transcription", "writer.documents")
indexing.run(
    {
        "transcriber": {
            "file_path": file_url,
            "summarization": None,
            "speaker_labels": None,
        }
    }
)

print("Indexed Document Count:", document_store.count_documents())
```

<Note>
  Calling `indexing.run()` blocks until the transcription is finished.
</Note>

The results of the transcription, summarization and speaker diarization are returned in separate document lists:

- `transcription`
- `summarization`
- `speaker_labels`

When `AssemblyAITranscriber` is used in a Haystack pipeline, transcription happens by default. In the metadata of the transcription, you will also get the `ID` of the transcription and the `URL` of your audio file.

A bullet point summary of what is being discussed will be returned if `summarization` is set to `TRUE`. The transcription divided into utterances of speakers will be returned if `speaker_labels` is set to `TRUE`.

The output of the `AssemblyAITranscriber` is a Haystack document. When all features are turned on, the created document looks like this:

```python
{
"transcription":
  [Document(
    id=bdf3eb20f6440cf4b15fa4fa3176eeb72bf0139a3ad4c76741724132907a5daa,
    content: "Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skyli...",
    meta: {
      'transcript_id': '2335cc07-1fbf-48ba-9855-7db3eeeb80f4',
      'audio_url': "https://assembly.ai/wildfires.mp3"
      }
    )
  ],

"summarization":
  [Document(
    id=f88864d9229b30013d5248156e74d5bfd4435e73aadb0c0ce79040be10a4f308,
    content: "- Smoke from hundreds of wildfires in Canada is triggering air quality alerts...")],

"speaker_labels":
  [Document(
    id=a7e222bc6a965ab1032401a6fa22da2e774294ce049b9d228acbb8b100ea2ecf,
    content: "Smoke from hundreds of wildfires in Canada is triggering air quality...",
    meta: {
      'speaker': 'A'
      }
    ),

  Document(
    id=711a1888af58601e6392490a5e4ca4c10958f93a52d8f0734869c54573ea76f5,
    content: "Well, there's a couple of things. The season has been pretty dry already...",
    meta: {
      'speaker': 'B'
      }
    ),

  Document(
    id=8fc78631d420e2e6127b8bdff2830f693febb91ed1566b9a84527cf023023d9e,
    content: "So what is it in this haze that makes it harmful?",
    meta: {
      'speaker': 'A'
      }
    ),

    ...
]}
```

## Additional resources

You can learn more about using Haystack with AssemblyAI in these resources:

- [Announcing the AssemblyAI Integration for Haystack](https://www.assemblyai.com/blog/announcing-the-assemblyai-integration-for-haystack/)
- [AssemblyAI integration for Haystack GitHub repository](https://github.com/AssemblyAI/assemblyai-haystack)



---
title: Integrate Twilio with AssemblyAI
description: Transcribe Twilio Voice data using AssemblyAI.
hide-nav-links: true
---

Twilio is a programmable communication platform for voice, messaging, and email.
By combining Twilio with AssemblyAI, you can transcribe voice calls in [real-time](/docs/speech-to-text/streaming), and voice recordings and voice messages [asynchronously](/docs/speech-to-text/pre-recorded-audio).
Combine transcription with our [audio intelligence models](/docs/audio-intelligence/summarization) and [LeMUR LLM framework](/docs/lemur/summarize-audio) to analyze the calls and messages.

Blog posts:

- [Transcribe a phone call in real-time using Python with AssemblyAI and Twilio](https://www.assemblyai.com/blog/transcribe-phone-call-real-time-python/)
- [Transcribe Phone Calls in Real-Time using Node.js with AssemblyAI, and Twilio](https://www.twilio.com/en-us/blog/phone-call-transcription-assemblyai-twilio-node)
- [Transcribe phone calls in real-time using C# .NET with AssemblyAI and Twilio](https://www.twilio.com/en-us/blog/transcribe-phone-calls-real-time-csharp-assemblyai-twilio)
- [Transcribe phone calls in real-time in Go with Twilio and AssemblyAI](https://www.assemblyai.com/blog/transcribe-phone-calls-in-realtime-in-go-with-twilio-and-assemblyai/)
- [Answer Questions about Twilio Voice Recordings with AssemblyAI and LangChain.js](https://www.twilio.com/en-us/blog/qa-voice-recordings-assemblyai-langchain-js)
- [Using Django & AssemblyAI for More Accurate Twilio Call Transcriptions](https://www.fullstackpython.com/blog/django-accurate-twilio-voice-transcriptions.html)

Videos:

- [Transcribe Twilio Phone Calls in Real-Time with AssemblyAI | JavaScript WebSockets Tutorial](https://youtu.be/3XmtJgWcOT0)


Thoughts and insights from 
AssemblyAI
Use our API
Build & Learn
October 30, 2023
Transcribe audio to text on Cloudflare Workers with AssemblyAI and TypeScript
In this tutorial, you'll learn how to create an application that transcribes the audio files (and video files) to text. You'll create a TypeScript backend on top of Cloudflare Workers and use the AssemblyAI APIs to transcribe the audio.

TutorialTypeScriptCloudflare
Niels Swimberghe


Table of contents
Prerequisites
Create the Cloudflare Worker
Create a file upload form
Create a transcript
Get the transcript
Deploy Worker to Cloudflare
Conclusion
Get $50 in credits
Prerequisites
Node.js version 16.13.0 or later
A Cloudflare account to deploy to the Cloudflare Workers runtime (optional).
An AssemblyAI account and API key to transcribe the audio. Get started with a free AssemblyAI account.
You can find the source code for this project on GitHub and use the "Deploy with Workers" button inside the README file to quickly deploy the code to the Cloudflare Workers runtime.

Create the Cloudflare Worker
Create your worker project using the C3 (create-cloudflare-cli) tool:

npm create cloudflare@latest

Answer the prompts as follows:

In which directory do you want to create your application? ./audio-transcriber.
You can use lowercase letters and dashes, but no spaces or uppercase letters.
What type of application do you want to create? "Hello World" Worker
Do you want to use TypeScript? yes
Do you want to use git for version control? no or yes, up to you.
Do you want to deploy your application? no or yes, up to you.
using create-cloudflare version 2.0.14 ╭ Create an application with Cloudflare Step 1 of 3 │ ├ In which directory do you want to create your application? │ dir ./audio-transcriber │ ├ What type of application do you want to create? │ type "Hello World" Worker │ ├ Do you want to use TypeScript? │ yes typescript │ ├ Copying files from "hello-world" template │ ├ Do you want to use TypeScript? │ yes typescript │ ├ Retrieving current workerd compatibility date │ compatibility date 2023-07-17 │ ├ Do you want to use git for version control? │ no git │ ╰ Application created ╭ Installing dependencies Step 2 of 3 │ ├ Installing dependencies │ installed via `npm install` │ ╰ Dependencies Installed ╭ Deploy with Cloudflare Step 3 of 3 │ ├ Do you want to deploy your application? │ yes deploy via `npm run deploy` │ ├ Logging into Cloudflare checking authentication status │ logged in │ ├ Selecting Cloudflare account retrieving accounts │ account ***@***.com's Account │ ┘ Deploying your application . ├ Deploying your application │ deployed via `npm run deploy` │ ├ SUCCESS View your deployed application at https://audio-transcriber.***.workers.dev │ │ Navigate to the new directory cd audio-transcriber │ Run the development server npm run start │ Deploy your application npm run deploy │ Read the documentation https://developers.cloudflare.com/workers │ Stuck? Join us at https://discord.gg/cloudflaredev │ ├ Waiting for DNS to propagate │ DNS propagation complete. │ ├ Waiting for deployment to become available │ deployment is ready at: https://audio-transcriber.***.workers.dev │ ├ Opening browser │ ╰ See you again soon!

Change directories into your new project:

cd audio-transcriber

You just scaffolded a Cloudflare Workers project which has a lot of files, mostly configuration for TypeScript and Cloudflare. You can find the code for your Worker under src/worker.ts.

/** * Welcome to Cloudflare Workers! This is your first worker. * * - Run `npm run dev` in your terminal to start a development server * - Open a browser tab at http://localhost:8787/ to see your worker in action * - Run `npm run deploy` to publish your worker * * Learn more at https://developers.cloudflare.com/workers/ */ export interface Env { // Example binding to KV. Learn more at https://developers.cloudflare.com/workers/runtime-apis/kv/ // MY_KV_NAMESPACE: KVNamespace; // // Example binding to Durable Object. Learn more at https://developers.cloudflare.com/workers/runtime-apis/durable-objects/ // MY_DURABLE_OBJECT: DurableObjectNamespace; // // Example binding to R2. Learn more at https://developers.cloudflare.com/workers/runtime-apis/r2/ // MY_BUCKET: R2Bucket; // // Example binding to a Service. Learn more at https://developers.cloudflare.com/workers/runtime-apis/service-bindings/ // MY_SERVICE: Fetcher; // // Example binding to a Queue. Learn more at https://developers.cloudflare.com/queues/javascript-apis/ // MY_QUEUE: Queue; } export default { async fetch(request: Request, env: Env, ctx: ExecutionContext): Promise { return new Response('Hello World!'); }, };

The code is very short, the fetch function receives an HTTP request and returns an HTTP response saying “Hello World!”.

Run the following command to start the worker:

npm run start

Look for the worker URL in the terminal output and use a browser to open it. You should see "Hello World!".

 Info  

If you look into the package.json file, you can find the different npm scripts that are available under the scripts property. There is the start script to run your worker locally, and the deploy script to deploy the worker to the Cloudflare Worker runtime. Both of these commands use the wrangler CLI which is a CLI for developers using Cloudflare.


The fetch function will handle all requests. In this project, you'll use the itty-router library to make it easier to execute different code depending on the path of the HTTP request. This library was originally made for Cloudflare Workers, but has expanded to different platforms too.

In a separate terminal, install the itty-router library using NPM.

npm install --save itty-router

Replace the code in worker.ts with the following:

import { IRequest, Router, error, html, json, text } from 'itty-router'; const router = Router(); export interface Env { } router.get('/hello/:name', ({ params: { name } }: IRequest) => html(` Hello ${name}! `)); export default { fetch: (req: IRequest, ...args: any) => router .handle(req, ...args) .then(json) .catch(error) };

This code sets up the router to handle all requests. If the return value of the selected route is not an HTTP response, it will convert the value to a JSON HTTP response using the json function. Errors will be caught by the error function.

Currently, there's only one route, /hello/:name, in which :name is a route parameter. This route will respond with HTML. The name parameter is passed into the HTML so it'll greet whichever name is set in the path.

Back in the browser, change the path to /hello/[YOURNAME] and replace [YOUR_NAME] with your name. You should be greeted with your name.

Create a file upload form
AssemblyAI can transcribe any audio file that's publicly available via URL, but if your audio file is on your local disk, you'll need to upload it somewhere to make it accessible to AssemblyAI.

Change the existing route and function so it returns HTML for a file upload form, and add another route that will accept the form submission using HTTP POST at path /upload-file:

router.get('/', () => html(`
Upload an audio or video file:
No file chosen Submit
`)) .post('/upload-file', async (request, env: Env) => { const formData = await request.formData(); const file = formData.get('file') as unknown as File; return text(`You uploaded ${file.name} (${file.size} bytes)`); });

The /upload-file route will respond with the file name and the size, just so you can verify that it's working.

Now that you can receive a file, you could upload the audio file to any file hosting service, including Cloudflare R2. To keep things as simple as possible, you will upload your audio file to AssemblyAI's CDN.

However, to interact with the AssemblyAI API, you'll first need to configure your API key.
Create a new file in the project root called .dev.vars and add your Assembly API key like this:

ASSEMBLYAI_API_KEY=[your_assemblyai_api_key]

Replace [your_assemblyai_api_key] with your AssemblyAI API key.

 Warning  

This API key will only be used during development. Make sure to keep this file and the API key a secret, and never check this file into source control.


Then, update the Env interface in the worker.ts file to include the API key:

export interface Env { ASSEMBLYAI_API_KEY: string; }

Next, install the AssemblyAI JavaScript SDK using npm.

npm install assemblyai --save

For all the previous changes to take effect, you need to restart your worker. Press ctrl + c to stop the worker, and use npm run start to start it again.

Update src/worker.ts to import the AssemblyAI client from the assemblyai package.

import { IRequest, Router, error, html, json, text } from 'itty-router'; import { AssemblyAI } from 'assemblyai';

Then inside the /upload-file route, create an AssemblyAI instance passing in the API key to the constructor, then use the client.files.upload function to upload the file to AssemblyAI's CDN:

.post('/upload-file', async (request, env: Env) => { const formData = await request.formData(); const file = formData.get('file') as unknown as File; const client = new AssemblyAI({ apiKey: env.ASSEMBLYAI_API_KEY }); const uploadUrl = await client.files.upload(file.stream()); return text(`Uploaded file to ${uploadUrl}`); });

 Note  

Files uploaded to AssemblyAI are exclusively accessible to AssemblyAI's services. These files are encrypted in transit and at rest, and will be automatically deleted after a period of time.


Go back to the browser at the root path, and upload a new file. You should receive a response with the URL of the file uploaded to AssemblyAI's CDN.

Create a transcript
Now that you have a file that's accessible (only) to AssemblyAI’s services, you can create a transcript using the AssemblyAI API.

Update the /upload-file route function so it creates a transcript from the uploaded file, then create a redirect response to another route, which we will define next.

.post('/upload-file', async (request, env: Env) => { const formData = await request.formData(); const file = formData.get('file') as unknown as File; const client = new AssemblyAI({ apiKey: env.ASSEMBLYAI_API_KEY }); const uploadUrl = await client.files.upload(file.stream()); let transcript = await client.transcripts.submit({ audio: uploadUrl }); const newUrl = new URL(`/transcript/${transcript.id}`, request.url); return Response.redirect(newUrl.toString(), 303); })

When you create a transcript, you will receive a transcript object back, with many empty properties such as the transcript text. That's because AssemblyAI queues the transcript and will then process the audio file. However, for now, you only need the id to redirect the user.

 Shortcut  

You can upload and create a transcript using a single function like this: await client.transcripts.submit({ audio: file.stream() }).


Get the transcript
While you can query the transcript object at any time, the transcript text, and many other properties will be empty until AssemblyAI is finished processing the audio file.

Create an HTTP GET route with the /transcripts/:id route pattern. This route will retrieve the transcript by ID and return the text if the status is completed. Otherwise, the status itself will be returned which can either be queued, processing, or error, along with a Refresh: 1 header which will automatically refresh the page after 1 seconds.

.get('/transcript/:id', async (request: IRequest, env: Env) => { const id = request.params.id; const client = new AssemblyAI({ apiKey: env.ASSEMBLYAI_API_KEY }); const transcript = await client.transcripts.get(id); if (transcript.status === 'completed') { return text(transcript.text); } else { return text(transcript.status, { headers: { 'Refresh': '1' // refreshes the browser every 1 seconds } }); } });

Now, head back to the browser, upload a file, and you should be redirected to the transcript route, which then returns the processing status, and eventually the text of the transcript.

 Info  

Instead of polling the transcript endpoint, you can use the webhook_url property to be notified when the transcript is completed, as documented in the AssemblyAI webhook guide.


Deploy Worker to Cloudflare
You can now deploy the worker to the Cloudflare Worker runtime. Use the npm deploy script to deploy the worker:

npm run deploy

The output will give you the URL where the worker is deployed. Earlier, you configured the AssemblyAI API key as an environment variable, but only locally. You also need to configure the key for your deployed worker. You can use the following wrangler command for this:

wrangler secret put ASSEMBLYAI_API_KEY

After running this command, you will be prompted to enter the value of the secret.

With the worker deployed and the secret configured, you and everyone else can now use the application just like you did locally.

Conclusion
You just learned how to create a Cloudflare Worker project for local development and how to deploy it to the runtime environment. You also learned how to use a router to execute different code depending on the URL of the incoming HTTP requests, how to handle a file upload. Finally, you used the AssemblyAI API to upload audio files and to transcribe those files.

Cloudflare has a lot more developer products that you can integrate into your application like queues, storage, databases, and more.

AssemblyAI can also do more than transcribe audio. AssemblyAI can summarize your transcript, create chapters with summaries, detect hate speech, identify speakers, ask LeMUR (AssemblyAI's LLM framework) any questions about long audio content, and more.

Related posts

July 18, 2025
Convert Speech to Text in Python in 5 Minutes
By Ryan O'Connor, Senior Developer Educator
TutorialAutomatic Speech RecognitionPython

July 15, 2025
How to Get YouTube Video Transcripts
By Patrick Loeber, Senior Developer Advocate
TutorialTranscriptsPython

July 15, 2025
Transcribe Twilio Phone Calls in Real-Time with AssemblyAI
By Ryan O'Connor, Senior Developer Educator
TutorialStreaming Speech-to-Text

July 11, 2025
Build an AI Voice Agent with DeepSeek R1, AssemblyAI, and ElevenLabs
By Smitha Kolan, Developer Educator
Streaming Speech-to-TextTutorialPython

AssemblyAI is a complete Speech AI system. Access automatic speech-to-text for your voice data (such as calls, virtual meetings, and podcasts) to transcribe speech, and add speaker labels, chapter notes, action items, and more—with AssemblyAI’s superhuman AI models.





Products
Overview
Speech-to-text
Streaming Speech-to-Text
Speech Understanding
Enterprise
Pricing
Resources
Blog
Support
Documentation
Benchmarks
Changelog
API status
Cookie settings
Company
Research
About
Careers
Contact sales
Customers
Security
Startup Program
© 2025 AssemblyAI, Inc.
Data processing addendum
Subprocessors
Terms of  service
Privacy policy

---
title: Account Management
---

On the [AssemblyAI dashboard](https://www.assemblyai.com/app), you can manage your projects and API keys, and see a breakdown of your usage and spend.

## Projects

Projects can be used to isolate data for different environments or applications, e.g., production, staging, or development. Each project has its own API keys, allowing for better organization and data access control.

Transcripts and other project-specific data is accessible only within the project they were created in — An API key from one project will not be able to access historical transcripts or data from another project. This separation maintains data security and prevents unintended cross-project access.

You can create, rename, and delete projects based on your plan:

| Usage limits       | Free | PAYG | Contracted | Enterprise |
| ------------------ | ---- | ---- | ---------- | ---------- |
| Number of projects | 2    | 2    | 5          | Custom     |

## API Keys

API keys are unique credentials that authenticate requests to the API. Each API key is associated with a specific project, ensuring secure and controlled access.

You can create and delete API keys based on your plan:

| Usage limits       | Free | PAYG | Contracted | Enterprise |
| ------------------ | ---- | ---- | ---------- | ---------- |
| Number of API keys | 2    | 4    | 25         | Custom     |

### Create a new API Key

1. Log in to your [AssemblyAI Dashboard](https://www.assemblyai.com/dashboard)
2. Navigate to the "API Keys" section
3. Click the "Create New API Key" button
4. Enter a descriptive name for your API key (e.g., "Production API", "Development API")
5. Click "Create"

### Delete an API Key

1. Log in to your [AssemblyAI Dashboard](https://www.assemblyai.com/dashboard)
2. Navigate to the "API Keys" section
3. Locate the API key you want to delete
4. Click the "Delete" button next to the key
5. Confirm the deletion in the popup dialog

<Error>
  This action cannot be undone. Make sure no active applications are using the
  key before deletion.
</Error>

## Reporting

Get insights into your usage and spend to track and manage costs effectively with the reporting tool in the AssemblyAI dashboard. You can analyze your usage and spend data at different levels of granularity:

- Account
- Product (e.g., Speech-to-text, Streaming, LeMUR)
- Models (e.g., Best, Claude 3.5 Sonnet, etc.)
- Project
- API key

## Usage Limits

<Note>

With the current version of multi-project support, rate limiting is applied at the account level, not at the project level. This means that the rate limits for each API key mirror the rate limits for the account.

Example: If an account has an Async concurrency of 200, each API key for that account will be able to process up to 200 requests concurrently.

</Note>

To ensure a smooth experience for all users, certain operations have per-account usage limits.

- **Concurrency limits** for asynchronous operations
- **Rate limits** for synchronous operations

<Note title="Need a higher concurrency?">

Our services are infinitely scalable and we offer custom concurrency limits that scale to support any workload at no additional cost. If you need a higher concurrency limit please either contact our{" "} <a href="https://www.assemblyai.com/contact" target="_blank"> Sales team </a>{" "} or send an email to our [Support team](https://www.assemblyai.com/contact/support).

</Note>

### Speech-to-Text usage limits

AssemblyAI limits the number of transcriptions being processed at any given time.

| Usage limit               | Free account | Paid account |
| ------------------------- | ------------ | ------------ |
| Concurrent transcriptions | 5            | 200          |

If you submit a transcription that would exceed your usage limit, it'll be added to a queue. Queued transcriptions will be processed automatically as previously submitted transcriptions complete.

If your account balance goes below zero, your concurrency limit will be reduced to 1.

If you exceed your concurrency limit, you'll receive an email stating that your transcripts have been throttled. Note that you'll only receive this email once per day.

![Email notifying user that their transcripts have been throttled due to exceeding the Concurrency limit](file:66fd3d09-0deb-45c5-891f-b6bcacf8367b)

### Streaming Speech-to-Text usage limits

AssemblyAI limits the number of concurrent streaming sessions.

| Usage limit                   | Free account | Paid account |
| ----------------------------- | ------------ | ------------ |
| Concurrent Streaming sessions | 5            | 100          |

<Tip>
Our Streaming STT feature includes automatically scaling concurrency limits for paid accounts.

Anytime you are using 70% or more of your current limit, your concurrency limit will automatically increase and scale up by 10% every 60 seconds.

As your traffic starts to scale back down and you are using less than 50% of your current limit, your concurrency will start to scale back down until it eventually returns to your default value.
</Tip>

If you exceed your current limit, you'll receive an error with an error message: `Unauthorized connection: Too many concurrent sessions`.

<Note title="Properly terminating sessions">

If you're consistently exceeding the limit of concurrent sessions, first make sure that you're terminating sessions properly.

- If you're using the [WebSocket API](https://www.assemblyai.com/docs/api-reference/streaming-api/streaming-api) directly, you need to send a `terminate_session` message.

</Note>

### LeMUR usage limits

LeMUR requests are rate limited within a 60-second time window. For more information, see [Rate limits](https://www.assemblyai.com/docs/api-reference/overview#rate-limits).

| Usage limit         | Free account | Paid account |
| ------------------- | ------------ | ------------ |
| Requests per minute | N/A*         | 30           |

\* LeMUR is only available for paid users.

## Best Practices

- Use different API keys for development and production environments
- Monitor usage and spend patterns for each API key in your dashboard
- Keep your API keys secure and never expose them in client-side code
- Use meaningful names and tags to easily identify the purpose of each key
- Store API keys as environment variables in your application


---
title: Webhooks
hide-nav-links: true
description: Get notified when a transcription is ready.
---

Webhooks are custom HTTP callbacks that you can define to get notified when your transcripts are ready.

To use webhooks, you need to set up your own webhook receiver to handle webhook deliveries.

## Create a webhook for a transcription

<Tip title="Don't have a webhook endpoint yet?">
  Create a test webhook endpoint with [webhook.site](https://webhook.site) to
  test your webhook integration.
</Tip>

<Tabs>
<Tab language="python-sdk" title="Python SDK">

To create a webhook, use `set_webhook()` on the transcription config. The URL must be accessible from AssemblyAI's servers.

Use `submit()` instead of `transcribe()` to create a transcription without waiting for it to complete.

```python
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig().set_webhook("https://example.com/webhook")

aai.Transcriber().submit(audio_file, config)
```

</Tab>
<Tab language="python" title="Python">

To create a webhook, set the `webhook_url` parameter when you create a new transcription. The URL must be accessible from AssemblyAI's servers.

```python
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url,
    "webhook_url": "https://example.com/webhook"
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)
```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">

To create a webhook, include the `webhook_url` parameter when you create a new transcription. The URL must be accessible from AssemblyAI's servers.

Use `submit()` instead of `transcribe()` to create a transcription without waiting for it to complete.

```javascript
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  webhook_url: "https://example.com/webhook",
};

const run = async () => {
  const transcript = await client.transcripts.submit(params);
};

run();
```

</Tab>
<Tab language="javascript" title="JavaScript">

To create a webhook, set the `webhook_url` parameter when you create a new transcription. The URL must be accessible from AssemblyAI's servers.

```javascript
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl,
  webhook_url: "https://example.com/webhook",
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });
```

</Tab>
<Tab language="csharp" title="C#">

To create a webhook, set the `webhook_url` parameter when you create a new transcription. The URL must be accessible from AssemblyAI's servers.

```csharp
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
public string Id { get; set; }
public string Status { get; set; }
public string Text { get; set; }
public string Error { get; set; }
}

async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
{
using (var fileStream = File.OpenRead(filePath))
using (var fileContent = new StreamContent(fileStream))
{
fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent))
        {
            response.EnsureSuccessStatusCode();
            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

}

async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
{
var data = new { audio_url = audioUrl, webhook_url = "https://example.com/webhook" };
var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

    using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
    {
        response.EnsureSuccessStatusCode();
        return await response.Content.ReadFromJsonAsync<Transcript>();
    }

}

// Main execution
using (var httpClient = new HttpClient())
{
httpClient.DefaultRequestHeaders.Authorization =
new AuthenticationHeaderValue("<YOUR-API-KEY>");

    var uploadUrl = await UploadFileAsync("my-audio.mp3", httpClient);
    var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);

}

```

</Tab>
<Tab language="ruby" title="Ruby">

To create a webhook, set the `webhook_url` parameter when you create a new transcription. The URL must be accessible from AssemblyAI's servers.

```ruby
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url,
    "webhook_url" => "https://example.com/webhook"
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end
```

</Tab>
<Tab language="php" title="PHP">

```php
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
"authorization: <YOUR_API_KEY>",
"content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
"audio_url" => $upload_url,
"webhook_url" => "https://example.com/webhook"
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

```

</Tab>
</Tabs>

## Handle webhook deliveries

When the transcript is ready, AssemblyAI will send a `POST` HTTP request to the URL that you specified.

<Note title="Webhooks and PII Audio Redaction">

If using webhooks with PII audio redaction enabled, you'll receive two webhook calls: the first when the redacted audio file is ready and the second when the request for transcription is completed.

</Note>

<Note title="Static Webhook IP addresses">

AssemblyAI sends all webhook deliveries from fixed IP addresses:

| Region | IP Address     |
| ------ | -------------- |
| US     | `44.238.19.20` |
| EU     | `54.220.25.36` |

</Note>

### Delivery payload

The webhook delivery payload contains a JSON object with the following properties:

```json
{
  "transcript_id": "5552493-16d8-42d8-8feb-c2a16b56f6e8",
  "status": "completed"
}
```

| Key             | Type   | Description                                                  |
| --------------- | ------ | ------------------------------------------------------------ |
| `transcript_id` | string | The ID of the transcript.                                    |
| `status`        | string | The status of the transcript. Either `completed` or `error`. |

### Retrieve a transcript with the transcript ID

<Tabs>
<Tab language="python-sdk" title="Python SDK" default>

```python
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

transcript = aai.Transcript.get_by_id("<TRANSCRIPT_ID>")

if transcript.status == "error":
  raise RuntimeError(f"Transcription failed: {transcript.error}")

print(transcript.text)

```

</Tab>
<Tab language="python" title="Python">

```python
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

transcript_id = "<TRANSCRIPT_ID>"

polling_endpoint = f"https://api.assemblyai.com/v2/transcript/{transcript_id}"

transcription_result = requests.get(polling_endpoint, headers=headers).json()

if transcription_result['status'] == 'completed':
  print(f"Transcript ID:", transcript_id)
elif transcription_result['status'] == 'error':
  raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK" default>

```javascript
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

const transcript = await client.transcripts.get("<TRANSCRIPT_ID>");

if (transcript.status === "error") {
  throw new Error(`Transcription failed: ${transcript.error}`);
}

console.log(transcript.text);
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const transcriptId = "<TRANSCRIPT_ID>";
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

const pollingResponse = await axios.get(pollingEndpoint, {
  headers: headers,
});
const transcriptionResult = pollingResponse.data;

if (transcriptionResult.status === "completed") {
  console.log(transcriptionResult.text);
} else if (transcriptionResult.status === "error") {
  throw new Error(`Transcription failed: ${transcriptionResult.error}`);
}
```

</Tab>
<Tab language="csharp" title="C#">

```csharp
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
public string Id { get; set; }
public string Status { get; set; }
public string Text { get; set; }
public string Error { get; set; }
}

async Task<Transcript> RetrieveTranscript(string transcriptId, HttpClient httpClient)
{
var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcriptId}";
    var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
    var transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();
    switch (transcript.Status)
    {
        case "completed":
            return transcript;
        case "error":
            throw new Exception($"Transcription failed: {transcript.Error}");
default:
throw new Exception("This code should not be reachable.");
}
}

// Main execution
using (var httpClient = new HttpClient())
{
httpClient.DefaultRequestHeaders.Authorization =
new AuthenticationHeaderValue("<YOUR-API-KEY>");
var transcript = await RetrieveTranscript("<TRANSCRIPT_ID>", httpClient);
}

```

</Tab>
<Tab language="ruby" title="Ruby">

```ruby
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
}

transcript_id = "<TRANSCRIPT_ID>"
polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
polling_http.use_ssl = true
polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
polling_response = polling_http.request(polling_request)

transcription_result = JSON.parse(polling_response.body)

if transcription_result['status'] == 'completed'
  puts "Transcription text: #{transcription_result['text']}"
elsif transcription_result['status'] == 'error'
  raise "Transcription failed: #{transcription_result['error']}"
else
  puts 'Waiting for transcription to complete...'
  sleep(3)
end
```

</Tab>
<Tab language="php" title="PHP">

```php
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
"authorization: <YOUR_API_KEY>",
"content-type: application/json"
);

$transcript_id = "<TRANSCRIPT_ID>";
$polling_endpoint = "https://api.assemblyai.com/v2/transcript/" . $transcript_id;

$polling_response = curl_init($polling_endpoint);
curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

$transcription_result = json_decode(curl_exec($polling_response), true);

if ($transcription_result['status'] === "completed") {
    echo $transcription_result['text'];
} else if ($transcription_result['status'] === "error") {
throw new Exception("Transcription failed: " . $transcription_result['error']);
}

```

  </Tab>
</Tabs>

## Authenticate webhook deliveries

You can authenticate webhook deliveries from AssemblyAI by including a custom HTTP header in the request.

<Tabs groupId="language">
<Tab language="python" title="Python">

To add an authentication header, include the auth header name and value in `set_webhook()`.

```python {2}
config = aai.TranscriptionConfig().set_webhook(
    "https://example.com/webhook", "X-My-Webhook-Secret", "secret-value"
)

aai.Transcriber().submit(audio_url, config)
```

</Tab>
<Tab language="javascript" title="JavaScript">

To add an authentication header, include the `webhook_auth_header_name` and `webhook_auth_header_value` parameters.

```javascript {5-6}
client.transcripts.submit({
  audio:
    'https://assembly.ai/wildfires.mp3',
  webhook_url: 'https://example.com/webhook'
  webhook_auth_header_name: "X-My-Webhook-Secret",
  webhook_auth_header_value: "secret-value"
})
```

</Tab>
<Tab language="golang" title="Go">

To add an authentication header, include the `WebhookAuthHeaderName` and `WebhookAuthHeaderValue` parameters.

```go {3-4}
client.Transcripts.SubmitFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    WebhookURL:             aai.String("https://example.com/webhook"),
    WebhookAuthHeaderName:  aai.String("X-My-Webhook-Secret"),
    WebhookAuthHeaderValue: aai.String("secret-value"),
})
```

</Tab>
<Tab language="java" title="Java">

To add an authentication header, include the `webhookAuthHeaderName` and `webhookAuthHeaderValue` parameters.

```java {3-4}
var params = TranscriptOptionalParams.builder()
        .webhookUrl("https://example.com/webhook")
        .webhookAuthHeaderName("X-My-Webhook-Secret")
        .webhookAuthHeaderValue("secret-value")
        .build();
```

</Tab>
<Tab language="csharp" title="C#">

To add an authentication header, include the `WebhookAuthHeaderName` and `WebhookAuthHeaderValue` parameters.

```csharp {5-6}
var transcriptParams = new TranscriptParams
{
    AudioUrl = audioUrl,
    WebhookUrl = "https://example.com/webhook",
    WebhookAuthHeaderName = "X-My-Webhook-Secret",
    WebhookAuthHeaderValue = "secret-value"
};
```

</Tab>
<Tab language="ruby" title="Ruby">

To add an authentication header, include the `webhook_auth_header_name` and `webhook_auth_header_value` parameters.

```ruby {4-5}
transcript = client.transcripts.submit(
  audio_url: audio_url,
  webhook_url: 'https://example.com/webhook',
  webhook_auth_header_value: 'X-My-Webhook-Secret',
  webhook_auth_header_name: 'secret-value'
)
```

</Tab>
</Tabs>
<Tabs>
<Tab language="python-sdk" title="Python SDK">

To add an authentication header, include the auth header name and value in `set_webhook()`.

```python
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig().set_webhook("https://example.com/webhook", "X-My-Webhook-Secret", "secret-value")

aai.Transcriber().submit(audio_file, config)
```

</Tab>
<Tab language="python" title="Python">

To add an authentication header, include the auth header name and value in `set_webhook()`.

```python
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url,
    "webhook_url": "https://example.com/webhook",
    "webhook_auth_header_name": "X-My-Webhook-Secret",
    "webhook_auth_header_value": "secret-value"
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)
```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">

To add an authentication header, include the `webhook_auth_header_name` and `webhook_auth_header_value` parameters.

```javascript
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  webhook_url: "https://example.com/webhook",
  webhook_auth_header_name: "X-My-Webhook-Secret",
  webhook_auth_header_value: "secret-value",
};

const run = async () => {
  const transcript = await client.transcripts.submit(params);
};

run();
```

</Tab>
<Tab language="javascript" title="JavaScript">

To add an authentication header, include the `webhook_auth_header_name` and `webhook_auth_header_value` parameters.

```javascript
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl,
  webhook_url: "https://example.com/webhook",
  webhook_auth_header_name: "X-My-Webhook-Secret",
  webhook_auth_header_value: "secret-value",
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });
```

</Tab>
<Tab language="csharp" title="C#">

To add an authentication header, include the `webhook_auth_header_name` and `webhook_auth_header_value` parameters.

```csharp
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
}

async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
{
    using (var fileStream = File.OpenRead(filePath))
    using (var fileContent = new StreamContent(fileStream))
    {
        fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent))
        {
            response.EnsureSuccessStatusCode();
            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }
}

async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
{
    var data = new { audio_url = audioUrl, webhook_url = "https://example.com/webhook", webhook_auth_header_name = "X-My-Webhook-Secret", webhook_auth_header_value = "secret-value" };
    var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

    using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
    {
        response.EnsureSuccessStatusCode();
        return await response.Content.ReadFromJsonAsync<Transcript>();
    }
}

// Main execution
using (var httpClient = new HttpClient())
{
    httpClient.DefaultRequestHeaders.Authorization =
        new AuthenticationHeaderValue("<YOUR-API-KEY>");

    var uploadUrl = await UploadFileAsync("my-audio.mp3", httpClient);
    var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);
}
```

</Tab>
<Tab language="ruby" title="Ruby">

To add an authentication header, include the `webhook_auth_header_name` and `webhook_auth_header_value` parameters.

```ruby
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url,
    "webhook_url" => "https://example.com/webhook",
    "webhook_auth_header_name" => "X-My-Webhook-Secret",
    "webhook_auth_header_value" => "secret-value"
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end
```

</Tab>
<Tab language="php" title="PHP">

To add an authentication header, include the auth header name and value in `set_webhook()`.

```php
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
"authorization: <YOUR_API_KEY>",
"content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
"audio_url" => $upload_url,
"webhook_url" => "https://example.com/webhook",
"webhook_auth_header_name" => "X-My-Webhook-Secret",
"webhook_auth_header_value" => "secret-value"
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

```

</Tab>
</Tabs>

## Add metadata to webhook deliveries

To associate metadata for a specific transcription request, you can add your own query parameters to the webhook URL.

```plain
https://example.com/webhook?customer_id=1234&order_id=5678
```

Now, when you receive the webhook delivery, you'll know the customer who requested it.

## Failed webhook deliveries

Webhook deliveries can fail for multiple reasons. For example, if your server is down or takes more than 10 seconds to respond.

If a webhook delivery fails, AssemblyAI will attempt to redeliver it up to 10 times, waiting 10 seconds between each attempt. If all attempts fail, AssemblyAI considers the delivery as permanently failed.


---
title: Select the Region
---

The default region is US, with base URL `api.assemblyai.com`. For EU data residency requirements, you can use our base URL for EU at `api.eu.assemblyai.com`.

<Note>
  The base URL for EU is currently only available for Async transcription and LeMUR.
</Note>

| Region       | Base URL                |
| ------------ | ----------------------- |
| US (default) | `api.assemblyai.com`    |
| EU           | `api.eu.assemblyai.com` |

<br />

To use the EU endpoint, set the base URL for your requests as `api.eu.assemblyai.com`.

<CodeBlocks>

```python title="Python SDK" highlight={4} maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"
aai.settings.base_url = "https://api.eu.assemblyai.com"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig()

transcript = aai.Transcriber(config=config).transcribe(audio_file)

if transcript.status == "error":
  raise RuntimeError(f"Transcription failed: {transcript.error}")

print(transcript.text)
```

```python title="Python" highlight={4} maxLines=15
import requests
import time

base_url = "https://api.eu.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url # You can also use a URL to an audio or video file on the web
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = f"https://api.eu.assemblyai.com/v2/transcript/{transcript_id}"

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(f"Transcript ID:", transcript_id)
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

```

```javascript title="JavaScript SDK" highlight={5} maxLines=15
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
  baseUrl: "https://api.eu.assemblyai.com",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  console.log(transcript.text);
};

run();
```

```javascript title="JavaScript" highlight={4} maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.eu.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

```csharp title="C#" highlight={69} maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
}

class Program
{
    static void Main(string[] args)
    {
        MainAsync(args).GetAwaiter().GetResult();
    }

    static async Task MainAsync(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Add("authorization", "<YOUR-API-KEY>");

            var localFilePath = "audio.mp3";

            Console.WriteLine("Uploading file...");
            var uploadUrl = await UploadFileAsync(localFilePath, httpClient);

            Console.WriteLine("Creating transcript...");
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);

            Console.WriteLine("Waiting for transcript...");
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            Console.WriteLine("Transcription completed!");
            Console.WriteLine("----------------------------------");
            Console.WriteLine(transcript.Text);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var content = new StreamContent(fileStream))
        {
            content.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            var response = await httpClient.PostAsync("https://api.eu.assemblyai.com/v2/upload", content);
            response.EnsureSuccessStatusCode();

            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new { audio_url = audioUrl };
        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.eu.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.eu.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "processing":
                case "queued":
                    Console.WriteLine($"Status: {transcript.Status}... waiting...");
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("Unexpected transcript status.");
            }
        }
    }
}
```

```ruby title="Ruby" highlight={4} maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.eu.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url # You can also use a URL to an audio or video file on the web
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end
```

```php title="PHP" highlight={5} maxLines=15
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.eu.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url // You can also use a URL to an audio or video file on the web
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = "https://api.eu.assemblyai.com/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        echo $transcription_result['text'];
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

</CodeBlocks>


---
title: Livekit
description: Livekit voice agent integration
---

<Note>
  This guide assumes prior knowledge of LiveKit. If you haven't used LiveKit before and are unfamiliar with LiveKit, please check out our [Building a Voice Agent with LiveKit and AssemblyAI guide](https://www.assemblyai.com/docs/speech-to-text/livekit-intro-guide).
</Note>

## Overview

LiveKit is an open source platform for developers building realtime media applications. In this guide, we'll show you how to integrate AssemblyAI's streaming speech-to-text model into your Livekit voice agent using the Agents framework.

<Card 
    title="Livekit" 
    icon={<img src="https://assemblyaiassets.com/images/Livekit.svg" alt="Livekit logo"/>} 
    href="https://docs.livekit.io/agents/integrations/stt/assemblyai/"
>
    View Livekit's AssemblyAI STT plugin documentation.
</Card>

## Quick start

### Installation

Install the plugin from PyPI:

```bash
pip install "livekit-agents[assemblyai]"
```

### Authentication

The AssemblyAI plugin requires an [AssemblyAI API key](https://www.assemblyai.com/docs/api-reference/overview#authorization). Set `ASSEMBLYAI_API_KEY` in your `.env` file.

<Tip>
  You can obtain an AssemblyAI API key by signing up
  [here](https://www.assemblyai.com/dashboard/signup).
</Tip>

### Basic usage

Use AssemblyAI STT in an `AgentSession` or as a standalone transcription service:

```python
from livekit.plugins import assemblyai

session = AgentSession(
    stt = assemblyai.STT(
      end_of_turn_confidence_threshold=0.7,
      min_end_of_turn_silence_when_confident=160,
      max_turn_silence=2400,
    ),
    # ... llm, tts, etc.
    vad=silero.VAD.load(), # VAD Enabled for Interruptions
    turn_detection="stt", # Enable Turn Detection
)
```

## Configuration

### Turn Detection (Key Feature)

AssemblyAI's new turn detection model was built specifically for voice agents and you can tweak it to fit your use case. It processes both audio and linguistic information to determine an end of turn confidence score on every inference, and if that confidence score is past the set threshold, it triggers end of turn.

This custom model was designed to address 2 major issues with voice agents. With traditional VAD (voice activity detection) approaches based on silence alone, there are situations where the agent wouldn't wait for a user to finish their turn even if the audio data suggested it. Think of a situation like "My credit card number is____" - if someone is looking that up, traditional VAD may not wait for the user, where our turn detection model is far better in these situations.

Additionally, in situations where we are certain that the user is done speaking like "What is my credit score?", a high end of turn confidence is returned, greater than the threshold, and triggering end of turn, allowing for minimal turnaround latency in those scenarios.

```python
# STT-based turn detection (recommended)
turn_detection="stt"

stt=assemblyai.STT(
    end_of_turn_confidence_threshold=0.7,
    min_end_of_turn_silence_when_confident=160,  # in ms
    max_turn_silence=2400,  # in ms
)
```

**Parameter tuning:**
- **end_of_turn_confidence_threshold**: Raise or lower the threshold based on how confident you'd like us to be before triggering end of turn based on confidence score
- **min_end_of_turn_silence_when_confident**: Increase or decrease the amount of time we wait to trigger end of turn when confident
- **max_turn_silence**: Lower or raise the amount of time needed to trigger end of turn when end of turn isn't triggered by a high confidence score

<Tip>
  You can also set `turn_detection="vad"` if you'd like turn detection to be based on Silero VAD instead of our advanced turn detection model.
</Tip>

For more information, see our [Universal-Streaming end-of-turn detection guide](https://www.assemblyai.com/docs/speech-to-text/universal-streaming#end-of-turn-detection) and [message-by-message breakdown](https://www.assemblyai.com/docs/speech-to-text/universal-streaming/message-sequence).

### Parameters

<ParamField path="api_key" type="str">
  Your AssemblyAI API key.
</ParamField>

<ParamField path="sample_rate" type="int" default="16000">
  The sample rate of the audio stream
</ParamField>

<ParamField path="encoding" type="str" default="pcm_s16le">
  The encoding of the audio stream. Allowed values: `pcm_s16le`, `pcm_mulaw`
</ParamField>

<ParamField path="format_turns" type="bool" default="True">
  Whether to return formatted final transcripts. If enabled, formatted final
  transcripts will be emitted shortly following an end-of-turn detection.
</ParamField>

<ParamField path="end_of_turn_confidence_threshold" type="float" default="0.7">
  The confidence threshold to use when determining if the end of a turn has been
  reached.  
  In our API the default is 0.4, but the default in LiveKit is set to 0.65.
</ParamField>

<ParamField path="min_end_of_turn_silence_when_confident" type="int" default="160">
  The minimum amount of silence required to detect end of turn when confident.
</ParamField>

<ParamField path="max_turn_silence" type="int" default="2400">
  The maximum amount of silence allowed in a turn before end of turn is triggered.
</ParamField>


---
title: Pipecat
description: Pipecat voice agent integration
---

<Note>
  This guide assumes prior knowledge of Pipecat. If you haven't used Pipecat before and are unfamiliar with Pipecat, please check out our [Building a Voice Agent with Pipecat and AssemblyAI guide](https://www.assemblyai.com/docs/speech-to-text/pipecat-intro-guide).
</Note>

## Overview

Pipecat is an open source platform for developers building realtime media applications. In this guide, we'll show you how to integrate AssemblyAI's streaming speech-to-text model into your Pipecat voice agent using the Pipeline framework.

<Card 
    title="Pipecat" 
    icon={<img src="https://assemblyaiassets.com/images/Pipecat.svg" alt="Pipecat logo"/>} 
    href="https://docs.pipecat.ai/server/services/stt/assemblyai"
>
    View Pipecat's AssemblyAI STT plugin documentation.
</Card>

## Quick start

### Installation

Install the AssemblyAI service from PyPI:

```bash
pip install "pipecat-ai[assemblyai]"
```

### Authentication

The AssemblyAI service requires an [AssemblyAI API key](https://www.assemblyai.com/docs/api-reference/overview#authorization). Set `ASSEMBLYAI_API_KEY` in your `.env` file.

<Tip>
  You can obtain an AssemblyAI API key by signing up
  [here](https://www.assemblyai.com/dashboard/signup).
</Tip>

### Basic usage

Use AssemblyAI STT in a `Pipeline`:

```python
from pipecat.services.assemblyai.stt import AssemblyAISTTService, AssemblyAIConnectionParams

# Configure service
stt = AssemblyAISTTService(
    api_key=os.getenv("ASSEMBLYAI_API_KEY"),
    vad_force_turn_endpoint=False,  # Use AssemblyAI's STT-based turn detection
    connection_params=AssemblyAIConnectionParams(
        end_of_turn_confidence_threshold=0.7,
        min_end_of_turn_silence_when_confident=160,
        max_turn_silence=2400,
    )
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    llm,
    tts,
    transport.output(),
])
```

## Configuration

### Turn Detection (Key Feature)

AssemblyAI's new turn detection model was built specifically for voice agents and you can tweak it to fit your use case. It processes both audio and linguistic information to determine an end of turn confidence score on every inference, and if that confidence score is past the set threshold, it triggers end of turn.

This custom model was designed to address 2 major issues with voice agents. With traditional VAD (voice activity detection) approaches based on silence alone, there are situations where the agent wouldn't wait for a user to finish their turn even if the audio data suggested it. Think of a situation like "My credit card number is____" - if someone is looking that up, traditional VAD may not wait for the user, where our turn detection model is far better in these situations.

Additionally, in situations where we are certain that the user is done speaking like "What is my credit score?", a high end of turn confidence is returned, greater than the threshold, and triggering end of turn, allowing for minimal turnaround latency in those scenarios.

You can set the `vad_force_turn_endpoint` parameter within the `AssemblyAISTTService` constructor:

```python
stt = AssemblyAISTTService(
    api_key=os.getenv("ASSEMBLYAI_API_KEY"),
    vad_force_turn_endpoint=False,  # Use AssemblyAI's STT-based turn detection
    connection_params=AssemblyAIConnectionParams(
        end_of_turn_confidence_threshold=0.7,
        min_end_of_turn_silence_when_confident=160,  # in ms
        max_turn_silence=2400,  # in ms
    )
)
```

**Parameter tuning:**
- **end_of_turn_confidence_threshold**: Raise or lower the threshold based on how confident you'd like us to be before triggering end of turn based on confidence score
- **min_end_of_turn_silence_when_confident**: Increase or decrease the amount of time we wait to trigger end of turn when confident
- **max_turn_silence**: Lower or raise the amount of time needed to trigger end of turn when end of turn isn't triggered by a high confidence score

<Tip>
  You can also set `vad_force_turn_endpoint=True` if you'd like turn detection to be based on VAD instead of our advanced turn detection model.
</Tip>

For more information, see our [Universal-Streaming end-of-turn detection guide](https://www.assemblyai.com/docs/speech-to-text/universal-streaming#end-of-turn-detection) and [message-by-message breakdown](https://www.assemblyai.com/docs/speech-to-text/universal-streaming/message-sequence).

## Parameters

### Constructor Parameters

<ParamField path="api_key" type="str" required>
  Your AssemblyAI API key.
</ParamField>

<ParamField path="connection_params" type="AssemblyAIConnectionParams">
  Connection parameters for the AssemblyAI WebSocket connection. See below for
  details.
</ParamField>

<ParamField path="vad_force_turn_endpoint" type="bool" default="True">
  When true, sends a `ForceEndpoint` event to AssemblyAI when a
  `UserStoppedSpeakingFrame` is received. Requires a VAD (Voice Activity
  Detection) processor in the pipeline to generate these frames.
</ParamField>

<ParamField path="language" type="Language" default="Language.EN">
  Language for transcription. AssemblyAI currently only supports English
  Streaming transcription.
</ParamField>

### Connection Parameters

<ParamField path="sample_rate" type="int" default="16000">
  The sample rate of the audio stream
</ParamField>

<ParamField path="encoding" type="str" default="pcm_s16le">
  The encoding of the audio stream. Allowed values: `pcm_s16le`, `pcm_mulaw`
</ParamField>

<ParamField path="format_turns" type="bool" default="True">
  Whether to return formatted final transcripts. If enabled, formatted final
  transcripts will be emitted shortly following an end-of-turn detection.
</ParamField>

<ParamField path="end_of_turn_confidence_threshold" type="float" default="0.7">
  The confidence threshold to use when determining if the end of a turn has been
  reached.
</ParamField>

<ParamField path="min_end_of_turn_silence_when_confident" type="int"  default="160">
  The minimum amount of silence required to detect end of turn when confident.
</ParamField>

<ParamField path="max_turn_silence" type="int" default="2400">
  The maximum amount of silence allowed in a turn before end of turn is
  triggered.
</ParamField>


---
title: Vapi
description: Vapi voice agent integration
---

## Overview

Vapi is a developer platform for building voice AI agents, they handle the complex backend of voice agents for you so you can focus on creating great voice experiences. In this guide, we'll show you how to integrate AssemblyAI's streaming speech-to-text model into your Vapi voice agent.

<Card
  title="Vapi"
  icon={
    <img src="https://assemblyaiassets.com/images/Vapi.svg" alt="Vapi logo" />
  }
  href="https://docs.vapi.ai/providers/transcriber/assembly-ai"
>
  View Vapi's AssemblyAI STT provider documentation.
</Card>

## Quick start

<Steps>
    **Head to the "Assistants" tab in your Vapi dashboard.**

    <Frame>
        <img src="file:ca47a3c9-e4ac-401a-bda7-c4e76fee79ec" />
    </Frame>

    **Click on your assistant and then the "Transcriber" tab.**

    <Frame>
        <img src="file:7ca3f3dd-db10-463a-afb4-f603cc3dd92b" />
    </Frame>

    **Select "Assembly AI" on the Provider dropdown and make sure the "Universal Streaming API" option is toggled on.**

    <Frame>
        <img src="file:5903fffc-4278-4106-9f7c-0564c1fae228" />
    </Frame>

</Steps>

Your voice agent now uses **AssemblyAI** for speech-to-text (STT) processing.

<Info>
  New to Vapi? Visit the [Quickstart
  Guide](https://docs.vapi.ai/quickstart/introduction) to explore various
  example voice agent workflows. For the easiest way to test a voice agent,
  follow this [simple phone-based guide](https://docs.vapi.ai/quickstart/phone).
  Vapi offers a wide range of example workflows to get you up and running
  quickly.
</Info>

## Recommended settings for optimal latency

### Transcriber settings

For best latency, Format Turns should be turned off (adds about 50ms of delay), and Universal Streaming should be enabled.

<Frame>
  <img src="file:6adf3371-3418-4cbd-bdab-ab5a2b2877f5" />
</Frame>

### Advanced > Start speaking plan

For start speaking plan, raise the `Wait seconds` to prevent false starts. `Smart Endpointing` should be turned off.

<Frame>
  <img src="file:5a05687c-a06d-4c0c-ad12-dc21b9efba34" />
</Frame>

### Advanced > Stop speaking plan

For stop speaking plan, lower `Voice seconds` to allow for faster interruptions.

<Frame>
  <img src="file:8c05935c-87dc-4cf4-abbd-e87a623866b7" />
</Frame>


---
title: "\U0001F99C️\U0001F517 LangChain Python Integration with AssemblyAI"
description: >-
  Transcribe audio in LangChain Python using the built-in integration with
  AssemblyAI.
hide-nav-links: true
---

To apply LLMs to speech, you first need to transcribe the audio to text, which is what the AssemblyAI integration for LangChain helps you with.

<Note>

Looking for the LangChain JavaScript integration?<br />
[Go to the LangChain.JS integration](js).

</Note>

## Quickstart

Install [the AssemblyAI package](https://github.com/langchain-ai/langchain) and [the AssemblyAI Python SDK](https://github.com/AssemblyAI/assemblyai-python-sdk):

```bash
pip install langchain
pip install assemblyai
```

Set your AssemblyAI API key as an environment variable named `ASSEMBLYAI_API_KEY`. You can [get a free AssemblyAI API key from the AssemblyAI dashboard](https://www.assemblyai.com/app/api-keys).

```bash
# Mac/Linux:
export ASSEMBLYAI_API_KEY=<YOUR_API_KEY>

# Windows:
set ASSEMBLYAI_API_KEY=<YOUR_API_KEY>
```

Import the `AssemblyAIAudioTranscriptLoader` from `langchain.document_loaders`.

```python
from langchain.document_loaders
```

1. Pass the local file path or URL as the `file_path` argument of the `AssemblyAIAudioTranscriptLoader`.
2. Call the `load` method to get the transcript as LangChain documents.

```python
audio_file = "https://assembly.ai/sports_injuries.mp3"
# or a local file path: audio_file = "./sports_injuries.mp3"

loader = AssemblyAIAudioTranscriptLoader(file_path=audio_file)

docs = loader.load()
```

The `load` method returns an array of documents, but by default, there's only one document in the array with the full transcript.

The transcribed text is available in the `page_content` attribute:

```python
docs[0].page_content
# Load time, a new president and new congressional makeup. Same old ...
```

The `metadata` contains the full JSON response with more meta information:

```python
{
  'language_code': <LanguageCode.en_us: 'en_us'>,
  'audio_url': 'https://assembly.ai/nbc.mp3',
  'punctuate': True,
  'format_text': True,
  ...
}
```

## Transcript formats

You can specify the `transcript_format` argument to load the transcript in different formats.

Depending on the format, `load_data()` returns either one or more documents. These are the different `TranscriptFormat` options:

- `TEXT`: One document with the transcription text
- `SENTENCES`: Multiple documents, splits the transcription by each sentence
- `PARAGRAPHS`: Multiple documents, splits the transcription by each paragraph
- `SUBTITLES_SRT`: One document with the transcript exported in SRT subtitles format
- `SUBTITLES_VTT`: One document with the transcript exported in VTT subtitles format

```python
from langchain.document_loaders.assemblyai

loader = AssemblyAIAudioTranscriptLoader(
    file_path="./your_file.mp3",
    transcript_format=TranscriptFormat.SENTENCES,
)

docs = loader.load()
```

## Transcription config

You can also specify the `config` argument to use different transcript features and audio intelligence models.
Here's an example of using the `config` argument to enable speaker labels, auto chapters, and entity detection:

```python


config = aai.TranscriptionConfig(
    speaker_labels=True, auto_chapters=True, entity_detection=True
)

loader = AssemblyAIAudioTranscriptLoader(file_path="./your_file.mp3", config=config)
```

<Info>
  For the full list of options, see [Transcript API
  reference](https://assemblyai.com/docs/api-reference/transcripts/submit#request).
</Info>

## Pass the AssemblyAI API key as an argument

Instead of configuring the AssemblyAI API key as the `ASSEMBLYAI_API_KEY` environment variable,
you can also pass it as the `api_key` argument.

```python
loader = AssemblyAIAudioTranscriptLoader(
    file_path="./your_file.mp3", api_key="<YOUR_API_KEY>"
)
```

## Additional resources

You can learn more about using LangChain with AssemblyAI in these resources.

- [LangChain docs for the AssemblyAI document loader](https://python.langchain.com/docs/integrations/document_loaders/assemblyai)
- [How to use audio data in LangChain with Python](https://www.assemblyai.com/blog/load-audio-langchain-python/)
- [Retrieval Augmented Generation on audio data with LangChain and Chroma](https://www.assemblyai.com/blog/retrieval-augmented-generation-audio-langchain/)
- [Build LangChain Audio Apps with Python in 5 Minutes](https://www.youtube.com/watch?v=7w7ysaDz2W4)
- [How to use LangChain for RAG over audio files](https://www.youtube.com/watch?v=l9YJrLg61ac)
- [AssemblyAI Python SDK](https://github.com/AssemblyAI/assemblyai-python-sdk)


---
title: "\U0001F99C️\U0001F517 LangChain JavaScript Integration with AssemblyAI"
description: >-
  Transcribe audio in LangChain.JS using the built-in integration with
  AssemblyAI.
hide-nav-links: true
---

To apply LLMs to speech, you first need to transcribe the audio to text, which is what the AssemblyAI integration for LangChain helps you with.

<Note>

Looking for the Python integration?<br />
[Go to the LangChain Python integration](python).

</Note>

## Quickstart

Add the [AssemblyAI SDK](https://github.com/AssemblyAI/assemblyai-node-sdk) to your project:

<Tabs>
  <Tab title="npm">

```bash
npm install langchain @langchain/community
```

  </Tab>
  <Tab title="yarn">

```bash
yarn add langchain @langchain/community
```

  </Tab>
  <Tab title="pnpm">

```bash
pnpm add langchain @langchain/community
```

  </Tab>
  <Tab title="bun">

```bash
bun add langchain @langchain/community
```

  </Tab>
</Tabs>

To use the loaders, you need an [AssemblyAI account](https://www.assemblyai.com/dashboard/signup) and get your AssemblyAI API key from the [dashboard](https://www.assemblyai.com/app/api-keys).
Configure the API key as the `ASSEMBLYAI_API_KEY` environment variable or the `apiKey` options parameter.

```javascript

  AudioTranscriptLoader,
  // AudioTranscriptParagraphsLoader,
  // AudioTranscriptSentencesLoader
} from "@langchain/community/document_loaders/web/assemblyai";

// You can also use a local file path and the loader will upload it to AssemblyAI for you.
const audioUrl = "https://assembly.ai/espn.m4a";

// Use `AudioTranscriptParagraphsLoader` or `AudioTranscriptSentencesLoader` for splitting the transcript into paragraphs or sentences
const loader = new AudioTranscriptLoader(
  {
    audio: audioUrl,
    // any other parameters as documented here: https://www.assemblyai.com/docs/api-reference/transcript#create-a-transcript
  },
  {
    apiKey: "<ASSEMBLYAI_API_KEY>", // or set the `ASSEMBLYAI_API_KEY` env variable
  }
);
const docs = await loader.load();
console.dir(docs, { depth: Infinity });
```

<Info>
  - You can use the `AudioTranscriptParagraphsLoader` or
  `AudioTranscriptSentencesLoader` to split the transcript into paragraphs or
  sentences. - If the `audio_file` is a local file path, the loader will upload
  it to AssemblyAI for you. - The `audio_file` can also be a video file. See the
  [list of supported file types in the FAQ
  doc](https://support.assemblyai.com/articles/2616970375-what-audio-and-video-file-types-are-supported-by-your-api).
  - If you don't pass in the `apiKey` option, the loader will use the
  `ASSEMBLYAI_API_KEY` environment variable. - You can add more properties in
  addition to `audio`. Find the full list of request parameters in the
  [AssemblyAI API docs](https://www.assemblyai.com/docs/api-reference/overview).
</Info>

<br />

You can also use the `AudioSubtitleLoader` to get `srt` or `vtt` subtitles as a document.

```javascript
// You can also use a local file path and the loader will upload it to AssemblyAI for you.
const audioUrl = "https://assembly.ai/espn.m4a";

const loader = new AudioSubtitleLoader(
  {
    audio: audioUrl,
    // any other parameters as documented here: https://www.assemblyai.com/docs/api-reference/transcript#create-a-transcript
  },
  "srt", // srt or vtt
  {
    apiKey: "<ASSEMBLYAI_API_KEY>", // or set the `ASSEMBLYAI_API_KEY` env variable
  }
);

const docs = await loader.load();
console.dir(docs, { depth: Infinity });
```

## Additional resources

You can learn more about using LangChain with AssemblyAI in these resources:

- [The LangChain docs for the AssemblyAI document loader](https://js.langchain.com/docs/integrations/document_loaders/web_loaders/assemblyai_audio_transcription)
- [How to integrate spoken audio into LangChain.js using AssemblyAI](https://www.assemblyai.com/blog/integrate-audio-langchainjs/)
- [Integrate Audio into LangChain.js apps in 5 Minutes](https://www.youtube.com/watch?v=hNpUSaYZIzs)
- [AssemblyAI JavaScript SDK](https://github.com/AssemblyAI/assemblyai-node-sdk)


---
title: Transcribe Your Amazon Connect Recordings
---

This guide walks through the process of setting up a transcription pipeline for Amazon Connect recordings using AssemblyAI.

### Get Started

Before we begin, make sure you have:

- An AssemblyAI account and an API key. You can [sign up](https://assemblyai.com/dashboard/signup) for a free account and get your API key from your dashboard.
- An AWS account.
- An [Amazon Connect instance](https://docs.aws.amazon.com/connect/latest/adminguide/amazon-connect-instances.html).

## Step-by-Step Instructions

<Steps>
<Step>

In the AWS console, navigate to the **Amazon Connect** services page. Select your instance and then click into the **Data Storage** section. On this page, find the subsection named **Call Recordings** and note the S3 bucket path where your call recordings are stored, you'll need this for later.

<img src="file:186783d2-051a-4256-b30d-3cd0aa371e25" />

</Step>
<Step>

Navigate to the **Lambda** services page, and create a new function. Set the runtime to **Python 3.13**. In the **Change default execution role** section, choose the option to create a new role with basic Lambda permissions. Assign a function name and then click **Create function**.

<img src="file:40f2bc8c-1352-435c-ac75-7b1a9ea9102c" />

</Step>
<Step>

In this new function, scroll down to the **Code Source** section and paste the following code into `lambda_function.py`.

```python
import json
import os
import boto3
import http.client
import time
from urllib.parse import unquote_plus
import logging

# Configure logging

logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Configuration settings

# See config parameters here: https://www.assemblyai.com/docs/api-reference/transcripts/submit

ASSEMBLYAI_CONFIG = {
# 'language_code': 'en_us',
# 'multichannel': True,
# 'redact_pii': True,
}

# Initialize AWS services

s3_client = boto3.client('s3')

def get_presigned_url(bucket, key, expiration=3600):
    """Generate a presigned URL for the S3 object"""

    logger.info({
        "message": "Generating presigned URL",
        "bucket": bucket,
        "key": key,
        "expiration": expiration
    })

    s3_client_with_config = boto3.client(
        's3',
        config=boto3.session.Config(signature_version='s3v4')
    )

    return s3_client_with_config.generate_presigned_url(
        'get_object',
        Params={'Bucket': bucket, 'Key': key},
        ExpiresIn=expiration
    )

def delete_transcript_from_assemblyai(transcript_id, api_key):
    """
    Delete transcript data from AssemblyAI's database using their DELETE endpoint.

    Args:
    transcript_id (str): The AssemblyAI transcript ID to delete
    api_key (str): The AssemblyAI API key

    Returns:
    bool: True if deletion was successful, False otherwise
    """

    headers = {
        "authorization": api_key,
        "content-type": "application/json"
    }

    conn = http.client.HTTPSConnection("api.assemblyai.com")

    try: # Send DELETE request to AssemblyAI API
        conn.request("DELETE", f"/v2/transcript/{transcript_id}", headers=headers)
        response = conn.getresponse()

        # Check if deletion was successful (HTTP 200)
        if response.status == 200:
            response_data = json.loads(response.read().decode())
            logger.info(f"Successfully deleted transcript {transcript_id} from AssemblyAI")
            return True
        else:
            error_message = response.read().decode()
            logger.error(f"Failed to delete transcript {transcript_id}: HTTP {response.status} - {error_message}")
            return False

    except Exception as e:
        logger.info(f"Error deleting transcript {transcript_id}: {str(e)}")
        return False
    finally:
        conn.close()

def transcribe_audio(audio_url, api_key):
    """Transcribe audio using AssemblyAI API with http.client"""
    logger.info({"message": "Starting audio transcription"})

    headers = {
    "authorization": api_key,
    "content-type": "application/json"
    }

    conn = http.client.HTTPSConnection("api.assemblyai.com")

    # Submit the audio file for transcription with config parameters

    request_data = {"audio_url": audio_url}

    # Add all configuration settings

    request_data.update(ASSEMBLYAI_CONFIG)

    json_data = json.dumps(request_data)
    conn.request("POST", "/v2/transcript", json_data, headers)
    response = conn.getresponse()

    if response.status != 200:
        raise Exception(f"Failed to submit audio for transcription: {response.read().decode()}")

    response_data = json.loads(response.read().decode())
    transcript_id = response_data['id']
    logger.info({"message": "Audio submitted for transcription", "transcript_id": transcript_id})

    # Poll for transcription completion

    while True:
        conn = http.client.HTTPSConnection("api.assemblyai.com")
        conn.request("GET", f"/v2/transcript/{transcript_id}", headers=headers)
        polling_response = conn.getresponse()
        polling_data = json.loads(polling_response.read().decode())

       if polling_data['status'] == 'completed':
           conn.close()
           logger.info({"message": "Transcription completed successfully"})
           return polling_data  # Return full JSON response instead of just text
       elif polling_data['status'] == 'error':
           conn.close()
           raise Exception(f"Transcription failed: {polling_data['error']}")

       conn.close()
       time.sleep(3)

def lambda_handler(event, context):
    """Lambda function to handle S3 events and process audio files"""
    try: # Get the AssemblyAI API key from environment variables
        api_key = os.environ.get('ASSEMBLYAI_API_KEY')
        if not api_key:
            raise ValueError("ASSEMBLYAI_API_KEY environment variable is not set")

       # Process each record in the S3 event
       for record in event.get('Records', []):
           # Get the S3 bucket and key
           bucket = record['s3']['bucket']['name']
           key = unquote_plus(record['s3']['object']['key'])

           # Generate a presigned URL for the audio file
           audio_url = get_presigned_url(bucket, key)

           # Get the full transcript JSON from AssemblyAI
           transcript_data = transcribe_audio(audio_url, api_key)

           # Prepare the transcript key - maintaining path structure but changing directory and extension
           transcript_key = key.replace('/CallRecordings/', '/AssemblyAITranscripts/', 1).replace('.wav', '.json')

           # Convert the JSON data to a string
           transcript_json_str = json.dumps(transcript_data, indent=2)

           # Upload the transcript JSON to the same bucket but in transcripts directory
           s3_client.put_object(
               Bucket=bucket,  # Use the same bucket
               Key=transcript_key,
               Body=transcript_json_str,
               ContentType='application/json'
           )
           logger.info({"message": "Transcript uploaded to transcript bucket successfully.", "key": transcript_key})

           # Uncomment the following line to delete transcript data from AssemblyAI after saving to S3
           # https://www.assemblyai.com/docs/api-reference/transcripts/delete
           # delete_transcript_from_assemblyai(transcript_data['id'], api_key)

       return {
           "statusCode": 200,
           "body": json.dumps({
               "message": "Audio file(s) processed successfully",
               "detail": "Transcripts have been stored in the AssemblyAITranscripts directory"
           })
       }

    except Exception as e:
       print(f"Error: {str(e)}")

       return {
            "statusCode": 500,
            "body": json.dumps({
            "message": "Error processing audio file(s)",
            "error": str(e)
            })
       }
```

</Step>
<Step>

At the top of the lambda function, you can edit the config to enable features for your transcripts. To see all available parameters, check out our [API reference](https://www.assemblyai.com/docs/api-reference/transcripts/submit).

```python
ASSEMBLYAI_CONFIG = {
   # 'language_code': 'en_us',
   # 'multichannel': True,
   # 'redact_pii': True,
}
```

<Tip>

If you would like to delete transcripts from AssemblyAI after completion, you can uncomment line **166** to enable the `delete_transcript_from_assemblyai` function. This ensures the transcript data is only saved on your S3 database and not stored on AssemblyAI's database.

</Tip>

Once you have finished editing the lambda function, click **Deploy** to save your changes.

</Step>
<Step>

On the same page, navigate to the **Configuration** section, under **General configuration** adjust the timeout to 15min 0sec and click **Save**. The processing times for transcription will be a lot shorter, but this ensures plenty of time for the function to complete.

<img src="file:4b703c3e-35cb-473a-bc29-a11169790fb3" />

</Step>
<Step>

Now from this page, on the left side panel click **Environment variables**. Click edit and then add an environment variable, `ASSEMBLYAI_API_KEY`, and set the value to your AssemblyAI API key. Then click **Save**.

<img src="file:eddd4561-5904-49f5-9da6-18e6f5751f9c" />

</Step>
<Step>

Now, navigate to the **IAM** services page. On the left side panel under Access Management click **Roles** and search for your Lambda function role (it's structure should look like `function_name-role-id`). Click into the role and then in the **Permissions policies** section click the dropdown for **Add permissions** and then select **Attach policies**.

From this page, find the policy named `AmazonS3FullAccess` and click **Add permissions**.

<img src="file:af78fedd-ea17-4baf-a11a-fcddd6ab8ebb" />

</Step>
<Step>

Now, navigate to the **S3** services page and click into the general purpose bucket where your Amazon Connect recordings are stored. Browse to the **Properties** tab and then scroll down to **Event notifications**. Click **Create event notification**. Give the event a name and then in the prefix section, insert the folder path we noted from Step 1 to ensure the event is triggered for the correct folder.

<img src="file:669b8ba0-71ec-499f-9cdb-50647e35ef14" />

Then in the **Event types** section, select **All object create events**.

<img src="file:0dcaeb15-d3b5-414f-905d-91282ed2c7bd" />

Then scroll down to the **Destination** section, set the destination as **Lambda function** and then select the Lambda function we created in Step 2. Then click **Save changes**.

<img src="file:c8544062-dfd6-4fb6-a281-966ca937518b" />

</Step>
<Step>

To finalise the integration, we'll need to set the recording behaviour from within your AWS Contact Flows. Navigate to your Amazon Connect instance access URL and sign in to your Admin account. In the left side panel, navigate to the **Routing** section and then select **Flows**.

Choose a flow to test with, in this case we'll utilize the `Sample inbound flow (first contact experience)`. You should see the **Block Library** on the left hand side of the page. In this section, search for `Set recording and analytics behaviour` and then drag the block into your flow diagram and connect the arrows.

You can see in our example, we place the block right at the entry of the call flow:

<img src="file:496281bf-9ba4-4393-a35c-a54301b12629" />

After connecting this block, click the 3 vertical dots in the top right of the block and select **Edit settings**. Scroll down to the **Enable recording and analytics** subsection and expand the **Voice** section. Then select `On` and select `Agent and customer` (or whoever you'd like to record). Then click **Save**, click **Save** again in the top right and then click **Publish** to publish the flow.

<img src="file:14456739-aff2-474c-b217-097defa7e30a" />

With this new flow published, you should now receive recordings for your Amazon Connect calls that utilize that flow, and you should now receive AssemblyAI transcripts for those recordings!

<Info>

The Amazon Connect Call Recordings are saved in the S3 bucket with this naming convention:
**/connect/\{instance-name\}/CallRecordings/\{YYYY\}/\{MM\}/\{DD\}/\{contact-id\}\_\{YYYYMMDDThh:mm\}\_UTC.wav**

The AssemblyAI Transcripts will be saved in the S3 bucket with this naming convention:
**/connect/\{instance-name\}/AssemblyAITranscripts/\{YYYY\}/\{MM\}/\{DD\}/\{contact-id\}\_\{YYYYMMDDThh:mm\}\_UTC.json**

</Info>
</Step>
<Step>

To view the logs for this integration, navigate to the **CloudWatch** services page and under the **Logs** section, select **Log groups**. Select the log group that matches your Lambda to view the most recent log stream.

</Step>
</Steps>


---
title: Transcribe Genesys Cloud Recordings with AssemblyAI
---

This guide walks through the process of setting up a transcription pipeline to send audio data from Genesys Cloud to AssemblyAI.

To accomplish this, we'll stream audio through Genesys's [AudioHook Monitor](https://appfoundry.genesys.com/filter/genesyscloud/listing/a3ff6a99-d866-4734-ab7a-16cff2e4308c) integration to a WebSocket server.
Upon call completion, the server will process this audio into a wav file and send it to AssemblyAI's Speech-to-text API for [pre-recorded audio](/docs/speech-to-text/pre-recorded-audio) transcription.

You can find all the necessary code for this guide [here](https://github.com/gsharp-aai/genesys-async-guide).

## Architecture Overview
Here's the general flow our app will follow:
```
+---------------------+     +--------------------+     +----------------------+
| 1. Genesys Cloud    |  →  | 2. WebSocket       |  →  | 3. Convert Raw       |
| (AudioHook Monitor) |     | Server             |     | Audio to WAV         |
+---------------------+     +--------------------+     +----------------------+
                                                                 ↓                                                             
+--------------------+     +---------------------+     +----------------------+
| 6. S3 Bucket       |  ←  | 5. AssemblyAI API   |  ←  | 4. Audio Upload (S3) |
| (Transcript Store) |     | (Transcription)     |     | (Trigger Lambda)     |
+--------------------+     +---------------------+     +----------------------+
```

## Getting started
Before we begin, make sure you have:
- An AssemblyAI account and an API key. You can [sign up](https://assemblyai.com/dashboard/signup) for a free account and get your API key from your [dashboard](https://www.assemblyai.com/dashboard/api-keys).
- An AWS account, an [Access key](https://us-east-1.console.aws.amazon.com/iam/home#/security_credentials), and permissions to S3, Lambda, and CloudWatch.
- A [Genesys Cloud](https://www.genesys.com/genesys-cloud) account with the necessary permissions to create call flows, phone numbers, and routes.
- [ngrok](https://ngrok.com/downloads/mac-os) installed.

## Genesys AudioHook Monitor

In order to stream your voice calls to third party services outside of the Genesys Cloud platform, Genesys offers an official integration called [AudioHook Monitor](https://appfoundry.genesys.com/filter/genesyscloud/listing/a3ff6a99-d866-4734-ab7a-16cff2e4308c).

This integration allows you to specify the URL of a [WebSocket server](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API) that implements the [AudioHook Protocol](https://developer.genesys.cloud/devapps/audiohook/), and once a connection has been established, Genesys will send both text (metadata messages/events encoded as JSON) and binary data (WebSocket frames containing the raw audio data in μ-law (Mu-law, PCMU) format).

An understanding of this integration and protocol is recommended before proceeding with this tutorial. Here are some helpful resources to get started:

- [Genesys App Foundry](https://appfoundry.genesys.com/filter/genesyscloud/listing/a3ff6a99-d866-4734-ab7a-16cff2e4308c)
- [AudioHook Monitor](https://help.mypurecloud.com/articles/about-audiohook-monitor/)
- [AudioHook Protocol](https://developer.genesys.cloud/devapps/audiohook/)
- [AudioHook Sample Service Repo](https://github.com/purecloudlabs/audiohook-reference-implementation#genesys-audiohook-sample-service)

## Step 1: Create a call flow in Genesys (optional)
You may already have an inbound call flow set up in Genesys (if so, skip to [Step 3](/docs/integrations/genesys-cloud#step-3-create-a-s3-bucket)), but we'll create a simple one from scratch for the sake of this tutorial.

<Note title="Adapting existing call flows">
All you need to add is the **Audio Monitoring** step from the toolbox and make sure **Suppress recording for the entire flow** is unchecked in the flow's **Recording and Speech Recognition** settings.
</Note>

<Steps>
<Step>

Within the [Architect tool](https://apps.usw2.pure.cloud/architect/#/inboundcall/flows), click **Add** to create a new call flow. Enter a **Name** for your flow and click **Create Flow**. Select the newly created flow to open the drag and drop editor.

<img src="file:d804c7b9-8ab5-4fa0-97d9-baa95bd89ea1" />

</Step>
<Step>

Create a **Reusable Task** from the bottom left of the left-side menu. From the Toolbox, search for **Audio Monitoring** and drag it just after the **Start** step of our flow. In the right-side menu for this option, make sure **Enable Monitoring** is enabled.

<img src="file:4de35e41-445d-4291-ad0b-b33c31abc571" />

</Step>
<Step>

Back in the Toolbox, search for **Transfer to User** and set that as the next step. In the right-hand menu, under **User** select a caller. Under **Pre-Transfer Audio** and **Failed Transfer Audio**, type your preferred messages.
<img src="file:413c2c30-d924-49bc-aa23-862622937a01" />

</Step>
<Step>

Search for **Disconnect** in the toolbox and drag that as the step following **Failure**.
<img src="file:4bc20f82-3a1e-4a21-909e-f156b10c29cf" />

</Step>
<Step>

Search the Toolbox for **Jump to Reusable Task** and drag this tool to the Main Menu at the top of the left-side menu. Select a **DTMF** and **Speech Recognition** value (this will be used to transfer your call to the agent). Under **Task**, select the task you just created.
<img src="file:f554b942-e67a-456b-88de-0a4e87669222" />

</Step>
<Step>

Under Settings in the left-side menu, navigate to the **Recording and Speech Recognition** section. Make sure **Suppress recording for the entire flow** is unchecked.
<img src="file:c9f38f8a-cbac-434e-bff7-3049217892dd" />

</Step>
<Step>

In the top navbar, make sure to click **Save** and then click **Publish** to have your changes take effect.

</Step>
</Steps>

## Step 2: Setup a phone and routing for your flow (optional)
<Steps>
<Step>

In the Genesys Cloud Admin section, navigate to the [Phones page](https://apps.usw2.pure.cloud/directory/#/admin/telephony/phone-management/phones) under the **Telephony section** and click **Add** to create a new phone. For **Person**, assign the User from your organization that you selected for the **Transfer to User** step in the [previous section](/docs/integrations/genesys-cloud#step-3).
<img src="file:85064df7-4276-43c2-ac6a-39d7dc06a156" />
<img src="file:c1b31a83-ce71-4c14-8eeb-01754ca2551b" />


</Step>
<Step>

Navigate to the [Number Management](https://apps.usw2.pure.cloud/directory/#/admin/telecom/numbers/numbers) page under the **Genesys Cloud Voice** section and select **Purchase Numbers**.
Enter an area code and click **Search**. Select a phone number from the list and click **Complete Purchase**.

<img src="file:29449970-89a0-4432-b07b-17a83a7a12f3" />
<img src="file:8479d851-a982-43e3-bd78-6ccc5d0b88a6" />

</Step>
<Step>

Navigate to the [Call Routing](https://apps.usw2.pure.cloud/directory/#/admin/routing/ivrs) page under the **Routing** section and select **Add**. Under **What call flow should be used?** select your flow. For **Inbound Numbers**, type the number you purchased in the above step. Then click **Create**.

<img src="file:14d909c0-1ac0-4b05-bc2d-c2f13ed036ca" />
<img src="file:665905e6-9ee8-4d6e-8b96-b67559b75c59" />

</Step>
<Step>

Under the **Telephony** section, navigate to the [External Trunks](https://apps.usw2.pure.cloud/directory/#/admin/telephony/trunks/external) page. Click **Create New**. Under **Caller ID**, the **Caller Address** will be an E.164 number and the phone number you created. 

<img src="file:59a1eb92-5221-4178-86a7-de30a21807f7" />
<img src="file:c036ae35-0305-40f9-a1de-c3a180bec372" />

</Step>
<Step>

Under **SIP Access Control**, select **Allow All** (*note: this is only for development and testing purposes, please specify actual IPs in production*). Under the **Media** section, make sure you select **Record calls on this trunk**. Then click **Save External Trunk** (it may take a few moments for your trunk to be ready).

<img src="file:af859dde-a73f-4e85-9a47-0afa517efa04" />
<img src="file:e8db478f-5063-4942-96a3-6313f0e39725" />

</Step>
</Steps>

## Step 3: Create a S3 bucket

After our Genesys call ends, store the audio file in S3.
<Steps>
<Step>

Click **Create bucket**. Give your bucket a name like `your-audiohook-bucket`. Scroll down and click **Create bucket**.

<img src="file:97a3055b-c787-48e8-8d6e-38dbaf387e97" />

</Step>
</Steps>

## Step 4: Create a WebSocket server
In this step, we'll set up a WebSocket server to receive messages and audio data from Genesys as they are sent.
Our server must respond to certain events (i.e. `open`, `close`, `ping`, `pause`, etc.) according to the AudioHook protocol.
Outside of these event messages, audio data is also transferred. We'll capture and temporarily store this audio locally until the connection is closed, at which point the server processes the audio to a `wav` file and uploads both the `wav` and `raw` audio files to a S3 bucket.

The AudioHook Monitor will send requests to a WebSocket URL we specify when setting up the integration in [Step 5](/docs/integrations/genesys-cloud#step-5-setting-up-audiohook-monitor).
When first enabled, AudioHook Monitor will do a quick verification step to ensure that the WebSocket server has implemented the AudioHook protocol correctly.

For this example, the server is written in JavaScript ([Express](https://expressjs.com/)) and hosted locally. We'll use [ngrok](https://ngrok.com/) to create a secure tunnel that exposes it to the internet with a public URL so that Genesys can make a connection.
However, the server can be implemented using your preferred programming language and deployed in whatever environment you choose, provided both support WebSocket TLS connections for secure bidirectional text and binary message exchange.

<Note title="Server implementation">
This server is a method to get up and running quickly for development and testing purposes without the complexity of a production deployment. How you implement this in practice will vary widely depending on your traffic volume, scaling needs, reliability requirements, security concerns, etc.
</Note>

<Steps>
<Step>

Clone this [example repo](https://github.com/gsharp-aai/genesys-async-guide) of a WebSocket server that implements the AudioHook protocol.
Follow the `README` instructions to download the necessary dependencies and start the server.
Make sure to look over `server.js` to get an understanding of how the requests from Genesys are received, processed, and responded to, as well as how the audio is stored, converted, and uploaded to our S3 bucket.

</Step>
<Step>

Make sure to create a `.env` file and set the variables:

```bash
PORT=3000
AWS_REGION=us-east-1 # Region of your S3 bucket
AWS_ACCESS_KEY_ID=<ACCESS_ID> # Found under IAM > Security Credentials
AWS_SECRET_ACCESS_KEY=<SECRET_KEY> # Found under IAM > Security Credentials
S3_BUCKET=your-audiohook-bucket # Name of your S3 bucket
S3_KEY_PREFIX=calls/ # The file structure you want your bucket to follow
API_KEY=<YOUR_API_KEY> # Used for authenticating messages from Genesys
RECORDINGS_DIR=./recordings # Temp file storage location
```

<Note>
`AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` can be found on your account's [IAM > Security Credentials page](https://us-east-1.console.aws.amazon.com/iam/home?region=us-east-1#/security_credentials). `API_KEY` is explained further in the [next step](/docs/integrations/genesys-cloud#step-3-3), but it can be anything you want to verify that the requests are actually originating from Genesys.
</Note>

</Step>
<Step>

Download [ngrok](https://ngrok.com/). Assuming your server is running on `port 3000`, run `ngrok http 3000 --inspect=false` in your terminal.
From the resulting terminal output, note the forwarding url that should look something like this: `https://<id>.ngrok-free.app`. This is our WebSocket server URL that we'll provide to the AudioHook Monitor in the next step.

<img src="file:795a26ee-e83e-4674-8661-ab983744754e" />

</Step>
</Steps>

## Step 5: Setting up AudioHook Monitor

<Steps>
<Step>

In the Genesys Cloud Admin section, navigate to the [Integrations](https://apps.usw2.pure.cloud/directory/#/admin/integrations/apps) page. Add a new integration via the plus sign in the top right corner.
<img src="file:84c3f416-509d-4933-a6fd-d4db7819212d" />

</Step>

<Step>

Search for **AudioHook Monitor** and install.
<img src="file:e3014119-32e5-41f0-a24c-d9e18dc057de" />

</Step>
<Step>

Navigate to the AudioHook Monitor's **Configuration** tab. Under the **Properties section**, make sure both channels are selected and the **Connection URI** is set to the ngrok url from the [previous step](/docs/integrations/genesys-cloud#step-3-2). For the ngrok url, replace `https` with `wss`.

<img src="file:45b31d2c-403f-4387-b6f8-bec61efe8308" />

</Step>
<Step>

In the **Configuration** tab, navigate to the **Credentials** section, and click **Configure**. Here you can set an API key to a value our server will use to verify that requests originated from Genesys. This is done via the `X-API-KEY` request header. Our server will compare this key to the value we set in our `.env` for `API_KEY`, so make sure they match. Click **Save**.
<img src="file:ac04644a-e191-4382-90e5-d94b9b4d0c14" />

</Step>
<Step>

Back on the **Integrations** page, click the toggle button under the **Status** column to activate your AudioHook. Genesys will attempt to verify our server is correctly configured according to the AudioHook protocol. If it is unable to do so, a red error will show with the reason for the failed connection. If it succeeds, the connection will toggle to Active.
<img src="file:10331745-8b4a-4250-a0b4-3cdb0a82a270" />

</Step>
</Steps>

## Step 6: Set up your AssemblyAI API call

<Steps>
<Step>

Navigate to the [Lambda](https://us-east-1.console.aws.amazon.com/lambda/home) services page, and create a new function. Set the runtime to `Node.js 22.x`. In the **Change default execution role** section, choose the option to create a **new role with basic Lambda permissions**. Assign a function name and then click **Create function**.

<img src="file:290124d4-e8b1-41bf-95f8-02f0902faa74" />

</Step>
<Step>

In this new function, scroll down to the **Code Source** section and paste the following code into `index.js`:

```javascript
// Import required AWS SDK modules
import { S3 } from '@aws-sdk/client-s3';
import { getSignedUrl } from '@aws-sdk/s3-request-presigner';
import { GetObjectCommand } from '@aws-sdk/client-s3';

// Configure logging
const logger = {
  info: (data) => console.log(JSON.stringify(data)),
  error: (data) => console.error(JSON.stringify(data))
};

// Configuration settings for AssemblyAI
// See config parameters here: https://www.assemblyai.com/docs/api-reference/transcripts/submit
const ASSEMBLYAI_CONFIG = {
  multichannel: true // Using multichannel here as we told Genesys to send us multichannel audio.
};

// Initialize AWS S3 client
const s3Client = new S3();

/**
 * Generate a presigned URL for the S3 object
 * @param {string} bucket - S3 bucket name
 * @param {string} key - S3 object key
 * @param {number} expiration - URL expiration time in seconds
 * @returns {Promise<string>} Presigned URL
 */
const getPresignedUrl = async (bucket, key, expiration = 3600) => {
  logger.info({
    message: "Generating presigned URL",
    bucket: bucket,
    key: key,
    expiration: expiration
  });

  const command = new GetObjectCommand({
    Bucket: bucket,
    Key: key
  });

  return getSignedUrl(s3Client, command, { expiresIn: expiration });
}

/**
 * Delete transcript data from AssemblyAI's database
 * @param {string} transcriptId - The AssemblyAI transcript ID to delete
 * @param {string} apiKey - The AssemblyAI API key
 * @returns {Promise<boolean>} True if deletion was successful, False otherwise
 */
const deleteTranscriptFromAssemblyAI = async (transcriptId, apiKey) => {
  try {
    const response = await fetch(`https://api.assemblyai.com/v2/transcript/${transcriptId}`, {
      method: 'DELETE',
      headers: {
        'authorization': apiKey,
        'content-type': 'application/json'
      }
    });
    
    if (response.ok) {
      logger.info(`Successfully deleted transcript ${transcriptId} from AssemblyAI`);
      return true;
    } else {
      const errorData = await response.text();
      logger.error(`Failed to delete transcript ${transcriptId}: HTTP ${response.status} - ${errorData}`);
      return false;
    }
  } catch (error) {
    logger.error(`Error deleting transcript ${transcriptId}: ${error.message}`);
    return false;
  }
}

/**
 * Submit audio for transcription
 * @param {object} requestData - Request data including audio URL and config
 * @param {string} apiKey - AssemblyAI API key
 * @returns {Promise<string>} Transcript ID
 */
const submitTranscriptionRequest = async (requestData, apiKey) => {
  const response = await fetch('https://api.assemblyai.com/v2/transcript', {
    method: 'POST',
    headers: {
      'authorization': apiKey,
      'content-type': 'application/json'
    },
    body: JSON.stringify(requestData)
  });

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`Failed to submit audio for transcription: ${errorText}`);
  }

  const responseData = await response.json();
  const transcriptId = responseData.id;
  
  logger.info({
    message: "Audio submitted for transcription",
    transcript_id: transcriptId
  });
  
  return transcriptId;
}

/**
 * Poll for transcription completion
 * @param {string} transcriptId - Transcript ID
 * @param {string} apiKey - AssemblyAI API key
 * @returns {Promise<object>} Transcription data
 */
const pollTranscriptionStatus = async (transcriptId, apiKey) => {
  const sleep = (ms) => new Promise(resolve => setTimeout(resolve, ms));
  
  // Keep polling until we get a completion or error
  while (true) {
    const response = await fetch(`https://api.assemblyai.com/v2/transcript/${transcriptId}`, {
      method: 'GET',
      headers: {
        'authorization': apiKey,
        'content-type': 'application/json'
      }
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Failed to poll transcription status: ${errorText}`);
    }

    const pollingData = await response.json();

    if (pollingData.status === 'completed') {
      logger.info({ message: "Transcription completed successfully" });
      return pollingData;
    } else if (pollingData.status === 'error') {
      throw new Error(`Transcription failed: ${pollingData.error}`);
    }
    
    // Wait before polling again
    await sleep(3000);
  }
}

/**
 * Transcribe audio using AssemblyAI API
 * @param {string} audioUrl - URL of the audio file
 * @param {string} apiKey - AssemblyAI API key
 * @returns {Promise<object>} Transcription data
 */
const transcribeAudio = async (audioUrl, apiKey) => {
  logger.info({ message: "Starting audio transcription" });

  // Prepare request data with config parameters
  const requestData = { audio_url: audioUrl, ...ASSEMBLYAI_CONFIG };
  
  // Submit the audio file for transcription
  const transcriptId = await submitTranscriptionRequest(requestData, apiKey);
  
  // Poll for transcription completion
  return await pollTranscriptionStatus(transcriptId, apiKey);
}

/**
 * Lambda function handler
 * @param {object} event - S3 event
 * @param {object} context - Lambda context
 * @returns {Promise<object>} Response
 */
export const handler = async (event, context) => {
  try {
    // Get the AssemblyAI API key from environment variables
    const apiKey = process.env.ASSEMBLYAI_API_KEY;
    if (!apiKey) {
      throw new Error("ASSEMBLYAI_API_KEY environment variable is not set");
    }

    // Process each record in the S3 event
    const records = event.Records || [];
    
    for (const record of records) {
      // Get the S3 bucket and key
      const bucket = record.s3.bucket.name;
      const key = decodeURIComponent(record.s3.object.key.replace(/\+/g, ' '));
      
      // Generate a presigned URL for the audio file
      const audioUrl = await getPresignedUrl(bucket, key);
      
      // Get the full transcript JSON from AssemblyAI
      const transcriptData = await transcribeAudio(audioUrl, apiKey);
      
      // Prepare the transcript key - maintaining path structure but changing directory and extension
      const transcriptKey = key
        .replace('audio', 'transcripts', 1)
        .replace('.wav', '.json');
      
      // Convert the JSON data to a string
      const transcriptJsonStr = JSON.stringify(transcriptData, null, 2);
      
      // Upload the transcript JSON to the same bucket but in transcripts directory
      await s3Client.putObject({
        Bucket: bucket,  // Use the same bucket
        Key: transcriptKey, // Store under the /transcripts directory
        Body: transcriptJsonStr,
        ContentType: 'application/json'
      });
      
      logger.info({
        message: "Transcript uploaded to transcript bucket successfully.",
        key: transcriptKey
      });
      
      // Uncomment the following line to delete transcript data from AssemblyAI after saving to S3
      // https://www.assemblyai.com/docs/api-reference/transcripts/delete
      // await deleteTranscriptFromAssemblyAI(transcriptData.id, apiKey);
    }

    return {
      statusCode: 200,
      body: JSON.stringify({
        message: "Audio file(s) processed successfully",
        detail: "Transcripts have been stored in the AssemblyAITranscripts directory"
      })
    };
  } catch (error) {
    console.error(`Error: ${error.message}`);
    return {
      statusCode: 500,
      body: JSON.stringify({
        message: "Error processing audio file(s)",
        error: error.message
      })
    };
  }
};
```

</Step>
<Step>

At the top of the Lambda function, you can edit the config to enable features for your transcripts. Since our call is two channels, we'll want to set `multichannel` to `true`. To see all available parameters, check out our [API reference](/docs/api-reference/transcripts/submit).
```javascript
ASSEMBLYAI_CONFIG = {
  'multichannel': true,
  // 'language_code': 'en_us',
  // 'redact_pii': true
  // etc.
}
```

<Tip>
If you would like to delete transcripts from AssemblyAI after completion, you can uncomment `line 212` to enable the `deleteTranscriptFromAssemblyAI` function. This ensures the transcript data is only saved to your S3 bucket and not stored on AssemblyAI's database.
</Tip>

<Note>
For more on our data retention policies, see [this page](https://support.assemblyai.com/articles/2240096256-does-assemblyai-offer-zero-data-retention#default-data-retention-audio-transcripts-4) from our FAQ.
</Note>

</Step>
<Step>

Once you have finished editing the Lambda function, click **Deploy** to save your changes.

<img src="file:068dd076-7df7-4e0e-8a66-7ebc60512313" />

</Step>
<Step>

On the same page, navigate to the **Configuration** section. Under **General configuration**, click **Edit**, and then adjust **Timeout** to `15min 0sec` and click **Save**. The processing times for transcription will be much shorter, but this ensures the function will have plenty of time to run.

<img src="file:a5a42127-ac46-402f-92a6-a091e52ea5fc" />
<img src="file:9a69c4a8-3973-48d7-b372-2c2b99352c45" />

</Step>
<Step>

On the left side panel, click **Environment variables**. Click **Edit**. Add an environment variable, `ASSEMBLYAI_API_KEY`, and set the value to your AssemblyAI [API key](https://www.assemblyai.com/dashboard/api-keys). Then click **Save**.

<img src="file:3f4eea27-5695-49dc-8e53-a8e942964815" />

</Step>
<Step>

Now, navigate to the [IAM](https://us-east-1.console.aws.amazon.com/iam/) services page. On the left side panel under **Access Management**, click **Roles** and search for your Lambda function's role (its structure should look like `<function_name>-<role_id>`). Click the role and then in the **Permissions policies** section click the dropdown for **Add permissions** and then select **Attach policies.**

<img src="file:a4a66818-a3eb-484b-8ff2-10d28c5febb4" />

</Step>
<Step>

From this page, find the policies named `AmazonS3FullAccess` and `CloudWatchEventsFullAccess`. Click **Add permissions** for both.

<img src="file:995417ab-d608-4b60-a45d-c55afdcf1e98" />

<Note>
`CloudWatchEventsFullAccess` is optional, but helpful for debugging purposes. Once your Lambda runs, it should output all logs to [CloudWatch](https://us-east-1.console.aws.amazon.com/cloudwatch) under a Log group `/aws/lambda/<your-lambda-fn>`
</Note>

</Step>
<Step>

Now, navigate to the [S3](https://us-east-1.console.aws.amazon.com/s3) services page and click into the general purpose bucket where your Genesys recordings are stored. Browse to the **Properties** tab and then scroll down to **Event notifications**. Click **Create event notification**.

<img src="file:80fb13a9-9011-4e11-8599-9cac7ec879da" />

</Step>
<Step>

Give the event a name and then in the **Prefix** section enter `calls/` (or whatever `S3_KEY_PREFIX` is set to), and in the **Suffix** section enter `.wav`. This will ensure the event is triggered once our `wav` file has been uploaded. In the **Event types** section, select **All object create events**.

<img src="file:8972bc50-867d-4373-a382-de490d7a8187" />

</Step>
<Step>

Scroll down to the **Destination** section, set the destination as **Lambda function** and then select the Lambda function we created in [Step 6](/docs/integrations/genesys-cloud#step-2-4). Then click **Save changes**.

<img src="file:6517623b-6474-447d-84dc-5f873804c8fb" />

</Step>
</Steps>

## Step 7: Transcribe your first call

<Steps>
<Step>


To test everything is working, call the phone number you linked to this flow in [Step 2](/docs/integrations/genesys-cloud#step-2-setup-a-phone-and-routing-for-your-flow-optional). Referring to the example flow above, press the DTMF value on the key pad or say the Speech Recognition value. Once transferred, your WebSocket server should start to receive data and output to console:

```bash
{
  version: '2',
  id: '<id>',
  type: 'ping',
  seq: 4,
  position: 'PT8.2S',
  parameters: { rtt: 'PT0.035392266S' },
  serverseq: 3
}
Received binary audio data: 3200 bytes 
Received binary audio data: 3200 bytes
...
Processed 146KB of audio data so far
```

</Step>
<Step>

Once the call has ended, you should see the following server logs:

```bash
{
  version: '2',
  id: '<id>',
  type: 'close',
  seq: 5,
  position: 'PT10.2S',
  parameters: { reason: 'end' },
  serverseq: 4
}
Handling close message
Closing file stream
Converting raw audio to WAV: '<wav file name>'

# Skipping ffmpeg output for brevity...

Uploading recording '<raw file>' to S3
Successfully uploaded raw recording to S3: '<raw file>'
Successfully uploaded WAV recording to S3: '<wav file>'
Sent closed response, seq=5
WebSocket closed for session '<session_id>': code=1000, reason=Session Ended
Cleaning up session '<session_id>'
Deleted local raw recording file: '<raw_file>'
Deleted local WAV recording file: '<wav_file>'
```

</Step>
<Step>
  To view the logs for this Lambda function, navigate to the [CloudWatch](https://us-east-1.console.aws.amazon.com/cloudwatch) services page and under the Logs section, select **Log groups**.
  Select the log group that matches your Lambda to view the most recent log stream. This can be very useful for debugging purposes if you run into any issues.

  <img src="file:0614c53f-1f7d-415b-9790-f88651578e9d" />
</Step>
<Step>

Head to your S3 bucket. Within the `/calls` directory, files will be stored under a unique identifier with the following structure:

`your-audiohook-bucket/calls/<timestamp>_<call_id>_<speaker_id>/<file_type>`

with audio files (both `raw` and `wav`) under `/audio` and transcript responses under `/transcripts`.

<img src="file:6ddec148-3f3f-4bde-a2ab-c57767d642c0" />

<Note>
The `raw` file can be nice to have for conversions to other formats in the future, but this step can be omitted to save on storage costs.
</Note>

</Step>

<Step>
**Success!** You have successfully integrated AssemblyAI with Genesys Cloud via AudioHook Monitor. If you run into any issues or have further questions, please reach out to our [Support team](https://www.assemblyai.com/contact/support).
</Step>
</Steps>

## Other considerations

### Supported audio formats
- Audio is sent as binary WebSocket frames containing the raw audio data in the negotiated format. Currently, only μ-law (Mu-law, PCMU) is [supported](https://developer.genesys.cloud/devapps/audiohook/session-walkthrough#audio-streaming).
- Before being uploaded to S3, the audio is converted to `wav` format using [ffmpeg](https://ffmpeg.org/). As a lossless format, `wav` generally results in high transcription accuracy, but is not required. A full list of file formats supported by AssemblyAI's API can be found [here](https://support.assemblyai.com/articles/2616970375-what-audio-and-video-file-types-are-supported-by-your-api).

### Multichannel
- As mentioned in [Step 6](/docs/integrations/genesys-cloud#step-2-4), the `multichannel` parameter should be enabled as the files are stereo utilizing one channel for each participant. When possible, multiple channels are recommended by AssemblyAI for the most accurate transcription results.
- If single channel is preferred, you can simplify the approach to only send a single channel with both speakers (via AudioHook) and adjust your server code to be single channel.


---
title: Transcribe Your Zoom Meetings
---

This guide creates a Node.js service that captures audio from Zoom Real-Time Media Streams (RTMS) and provides both real-time and asynchronous transcription using AssemblyAI.

<Note title="Zoom RTMS Documentation">
  For complete Zoom RTMS documentation, visit
  https://developers.zoom.us/docs/rtms/
</Note>

## Features

- **Real-time Transcription**: Live transcription during meetings using AssemblyAI's streaming API
- **Asynchronous Transcription**: Complete post-meeting transcription with advanced features
- **Flexible Audio Modes**:
  - Mixed stream (all participants combined)
  - Individual participant streams transcribed
- **Multichannel Audio Support**: Separate channels for different participants
- **Configurable Processing**: Enable/disable real-time or async transcription independently

## Setup

### Prerequisites

- Node.js 16+
- FFmpeg installed on your system
- Zoom RTMS Developer Preview access
- AssemblyAI API key
- ngrok (for local development and testing)

### Installation

1. **Clone the example repository and install dependencies**:

```bash
git clone https://github.com/zkleb-aai/assemblyai-zoom-rtms.git
cd assemblyai-zoom-rtms
npm install
```

2. **Configure environment variables**:

```bash
cp .env.example .env
```

Fill in your `.env` file:

```env
# Zoom Configuration
ZM_CLIENT_ID=your_zoom_client_id
ZM_CLIENT_SECRET=your_zoom_client_secret
ZOOM_SECRET_TOKEN=your_webhook_secret_token

# AssemblyAI Configuration
ASSEMBLYAI_API_KEY=your_assemblyai_api_key

# Service Configuration
PORT=8080
REALTIME_ENABLED=true
REALTIME_MODE=mixed
ASYNC_ENABLED=true
AUDIO_CHANNELS=mono
AUDIO_SAMPLE_RATE=16000
TARGET_CHUNK_DURATION_MS=100
```

### Local development with ngrok

For testing and development, you can use ngrok to expose your local server to the internet:

1. **Install ngrok**: Download from [ngrok.com](https://ngrok.com/) or install via package manager:

   ```bash
   # macOS
   brew install ngrok

   # Windows (chocolatey)
   choco install ngrok

   # Or download directly from ngrok.com
   ```

2. **Start your local server**:

   ```bash
   npm start
   ```

3. **In a separate terminal, start ngrok**:

   ```bash
   ngrok http 8080
   ```

4. **Copy the ngrok URL**: ngrok will display a forwarding URL like:

   ```
   Forwarding    https://example-abc123.ngrok-free.app -> http://localhost:8080
   ```

5. **Use the ngrok URL in your Zoom app webhook configuration**:
   ```
   https://example-abc123.ngrok-free.app/webhook
   ```

### Configuration options

#### Real-time transcription

- `REALTIME_ENABLED`: Enable/disable live transcription (default: `true`)
- `REALTIME_MODE`:
  - `mixed`: Single stream with all participants combined
  - `individual`: Separate streams per participant

#### Audio settings

- `AUDIO_CHANNELS`: `mono` or `multichannel`
- `AUDIO_SAMPLE_RATE`: Audio sample rate in Hz (default: `16000`)
- `TARGET_CHUNK_DURATION_MS`: Audio chunk duration for streaming (default: `100`)

#### Async transcription

- `ASYNC_ENABLED`: Enable/disable post-meeting transcription (default: `true`)

## Usage

### Start the service

```bash
npm start
```

The service will start on the configured port (default: 8080) and display:

```
🎧 Zoom RTMS to AssemblyAI Transcription Service
📋 Configuration:
   Real-time: ✅ (mixed)
   Audio: mono @ 16000Hz
   Async: ✅
🚀 Server running on port 8080
📡 Webhook endpoint: http://localhost:8080/webhook
```

### Configure Zoom webhook

1. In your Zoom App configuration, set the webhook endpoint to:

   ```
   # For production
   https://your-domain.com/webhook

   # For local development with ngrok
   https://example-abc123.ngrok-free.app/webhook
   ```

2. Subscribe to these events:
   - `meeting.rtms_started`
   - `meeting.rtms_stopped`

### Testing with ngrok

When using ngrok for testing:

1. **Keep ngrok running**: The ngrok tunnel must remain active during testing
2. **Update webhook URL**: If you restart ngrok, you'll get a new URL that needs to be updated in your Zoom app configuration
3. **Monitor ngrok logs**: ngrok shows incoming webhook requests in its terminal output
4. **Free tier limitations**: The free ngrok tier has some limitations; consider upgrading for heavy testing

### Real-time output

During meetings, you'll see live transcription:

```
🚀 AssemblyAI session started: [abc12345]
🎙️ [abc12345] Hello everyone, welcome to the meeting
📝 [abc12345] FINAL: Hello everyone, welcome to the meeting.
```

### Post-meeting files

After each meeting, the service generates:

- `transcript_[meeting_uuid].json` - Full AssemblyAI response with metadata
- `transcript_[meeting_uuid].txt` - Plain text transcript

## Advanced configuration

### AssemblyAI features

Modify the `ASYNC_CONFIG` object in the code to enable additional features:

```javascript
const ASYNC_CONFIG = {
  speaker_labels: true, // Speaker identification
  auto_chapters: true, // Automatic chapter detection
  sentiment_analysis: true, // Sentiment analysis
  entity_detection: true, // Named entity recognition
  redact_pii: true, // PII redaction
  summarization: true, // Auto-summarization
  auto_highlights: true, // Key highlights
};
```

See [AssemblyAI's API documentation](https://www.assemblyai.com/docs/api-reference/transcripts/submit) for all available options.

### Audio processing modes

#### Mixed mode (default)

- Single audio stream combining all participants
- Most efficient for general transcription
- Best for meetings with clear speakers

#### Individual mode

- Separate transcription stream per participant
- Better speaker attribution
- Higher resource usage

#### Multichannel audio

- Separate audio channels for different participants
- Enables advanced speaker separation
- Requires `AUDIO_CHANNELS=multichannel`

## API endpoints

### `POST` /webhook

Handles Zoom RTMS webhook events:

- URL validation
- Meeting start/stop events
- Automatic RTMS connection setup

## Error handling

The service includes comprehensive error handling:

- Automatic reconnection for dropped connections
- Graceful cleanup on meeting end
- Audio buffer flushing to prevent data loss
- Temporary file cleanup

## Monitoring

### Real-time logs

- Connection status updates
- Audio processing statistics
- Transcription progress
- Error notifications

### Example log output

```
📡 Connecting to Zoom signaling for meeting abc123
✅ Zoom signaling connected for meeting abc123
🎵 Connecting to Zoom media for meeting abc123
✅ Zoom media connected for meeting abc123
🚀 Started audio streaming for meeting abc123
🎵 [abc12345] 100 chunks, 32768 bytes, 10.2s
📝 [abc12345] FINAL: This is the final transcription.
```

### Development workflow

1. Start your local server: `npm start`
2. Start ngrok in another terminal: `ngrok http 8080`
3. Update your Zoom app webhook URL with the ngrok URL
4. Test with Zoom meetings
5. Monitor logs in both your app and ngrok terminals

---
title: Semantic Kernel Integration for AssemblyAI
description: >-
  Transcribe audio in Semantic Kernel for C# .NET using the built-in integration
  with AssemblyAI.
hide-nav-links: true
---

Semantic Kernel is an SDK for multiple programming languages to develop applications with [Large Language Models (LLMs)](https://www.assemblyai.com/blog/introduction-large-language-models-generative-ai/#what-are-language-models).
However, LLMs only operate on textual data and don't understand what is said in audio files.
With the [AssemblyAI integration for Semantic Kernel](https://github.com/AssemblyAI/assemblyai-semantic-kernel), you can use AssemblyAI's transcription models using the `TranscribePlugin` to transcribe your audio and video files.

## Quickstart

Add the [AssemblyAI.SemanticKernel NuGet package](https://www.nuget.org/packages/AssemblyAI.SemanticKernel) to your project.

<Tabs>
  <Tab  title="dotnet CLI">

```bash
dotnet add package AssemblyAI.SemanticKernel
```

  </Tab>
  <Tab title="Package Manager Console">

```powershell
Install-Package AssemblyAI.SemanticKernel
```

  </Tab>
</Tabs>

Next, register the `TranscriptPlugin` into your kernel:

```csharp
using AssemblyAI.SemanticKernel;
using Microsoft.SemanticKernel;

// Build your kernel
var kernel = Kernel.CreateBuilder();

// Get AssemblyAI API key from env variables, or much better, from .NET configuration
string apiKey = Environment.GetEnvironmentVariable("ASSEMBLYAI_API_KEY")
  ?? throw new Exception("ASSEMBLYAI_API_KEY env variable not configured.");

kernel.ImportPluginFromObject(
    new TranscriptPlugin(apiKey: apiKey)
);
```

## Usage

Get the `Transcribe` function from the transcript plugin and invoke it with the context variables.

```csharp
var result = await kernel.InvokeAsync(
    nameof(TranscriptPlugin),
    TranscriptPlugin.TranscribeFunctionName,
    new KernelArguments
    {
        ["INPUT"] = "https://assembly.ai/espn.m4a"
    }
);
Console.WriteLine(result.GetValue<string>());
```

You can get the transcript using `result.GetValue<string>()`.

You can also upload local audio and video file. To do this:

- Set the `TranscriptPlugin.AllowFileSystemAccess` property to `true`.
- Configure the `INPUT` variable with a local file path.

```csharp
kernel.ImportPluginFromObject(
    new TranscriptPlugin(apiKey: apiKey)
    {
        AllowFileSystemAccess = true
    }
);
var result = await kernel.InvokeAsync(
    nameof(TranscriptPlugin),
    TranscriptPlugin.TranscribeFunctionName,
    new KernelArguments
    {
        ["INPUT"] = "https://assembly.ai/espn.m4a"
    }
);
Console.WriteLine(result.GetValue<string>());
```

You can also invoke the function from within a semantic function like this.

```csharp
string prompt = """
                Here is a transcript:
                {{TranscriptPlugin.Transcribe "https://assembly.ai/espn.m4a"}}
                ---
                Summarize the transcript.
                """;
var result = await kernel.InvokePromptAsync(prompt);
Console.WriteLine(result.GetValue<string>());
```

## Additional resources

You can learn more about using Semantic Kernel with AssemblyAI in these resources:

- [Ask .NET Rocks! questions with Semantic Kernel, GPT, and Chroma DB](https://www.assemblyai.com/blog/ask-dotnetrocks-questions-semantic-kernel/)
- [AssemblyAI integration for Semantic Kernel GitHub repository](https://github.com/AssemblyAI/assemblyai-semantic-kernel)


---
title: Integrate Activepieces with AssemblyAI
description: Add Speech AI to your Activepieces flows with the AssemblyAI piece.
hide-nav-links: true
---

[Activepieces](https://www.activepieces.com/) is an open-source, no-code automation platform that enables users to streamline workflows by connecting various applications and automating tasks.

With the AssemblyAI piece for Activepieces, you can use AssemblyAI to transcribe audio data with speech recognition models, analyze the data with audio intelligence models, and build generative features on top of it with LLMs.
You can supply audio to the AssemblyAI piece and connect the output of any of AssemblyAI's models to other services in your Activepieces flow.

## Quickstart

<Steps>
<Step>
Create or edit a flow in Activepiece. Add a trigger of your choosing, and then click the plus-icon to add a new action.
Search for AssemblyAI, click on the AssemblyAI piece, and select the action that you want to use.

![Add an AssemblyAI piece action](file:154a2490-0568-4b1c-95f8-794fb115503d)

</Step>
<Step>

Create a new connection or select an existing one.

![Select a connection to AssemblyAI in Activepieces](file:c2abc406-9229-4c96-9b98-7df0c8c240e1)

In the **API Key** field, enter the API key from your [AssemblyAI dashboard](https://www.assemblyai.com/app/api-keys), and click **Save**.

![Configure your connection to AssemblyAI in Activepieces](file:03d29939-f689-41b4-bbd2-329485b5bddd)

</Step>
<Step>

Finally, configure your AssemblyAI action. Continue reading to learn more about all the available action.

</Step>
</Steps>

## AssemblyAI actions

The AssemblyAI piece for Activepieces provides the following actions:

### Files

#### Upload File

Upload an audio file to AssemblyAI so you can transcribe it.
You can pass the `Upload URL` output field to the `Audio URL` input field of Transcribe an Audio File module.

### Transcripts

#### Transcribe

Transcribe an audio file and wait until the transcript has completed or failed.
Configure the `Audio URL` field with the URL of the audio file you want to transcribe.
The `Audio URL` must be accessible by AssemblyAI's servers.
If you don't have a publicly accessible URL, you can use the Upload a File module to upload the audio file to AssemblyAI.

If you don't want to wait until the transcript is ready, uncheck the `Wait until transcript is ready` parameter.

<Info>
  Configure your desired [Audio Intelligence models](/audio-intelligence) when
  you create the transcript. The results of the models will be included in the
  transcript output.
</Info>

#### Get Transcript

Retrieve a transcript by ID.

#### Get Transcript Paragraphs

Retrieve the paragraphs of a transcript.

<Note>You can only invoke this module after the transcript is completed.</Note>

#### Get Transcript Sentences

Retrieve the sentences of a transcript.

<Note>You can only invoke this module after the transcript is completed.</Note>

#### Get Transcript Subtitles

Create SRT or VTT subtitles for a transcript.

<Note>You can only invoke this module after the transcript is completed.</Note>

#### Get Transcript Redacted Audio

First, you need to configure PII audio redaction using these fields when you create the transcript:

- `Redact PII`: `Checked`
- `Redact PII Audio`: `Checked`
- `Redact PII Policies`: Configure at least one PII policy

Then, you can use this module to retrieve the redacted audio of the transcript.

<Note>You can only invoke this module after the transcript is completed.</Note>

#### Search words in transcript

Search for words in a transcript.

<Note>You can only invoke this module after the transcript is completed.</Note>

#### List transcripts

Paginate over all transcripts.

#### Delete transcript

Delete a transcript by ID.
Deleting a transcript doesn't delete the transcript resource itself, but removes the data from the resource and marks it as deleted.

<Note>
  You can only invoke this module after the transcript status is "completed" or
  "error".
</Note>

### LeMUR

#### Run a Task using LeMUR

Prompt different LLMs over your audio data using LeMUR.
You have to configure either the `Transcript IDs` or `Input Text` input field.

#### Retrieve LeMUR response

Retrieve a LeMUR response that was previously generated.

#### Purge LeMUR request data

Delete the data for a previously submitted LeMUR request.
Response data from the LLM, as well as any context provided in the original request will be removed.

### Other actions

#### Custom API Call

Make your own REST API HTTP requests to the AssemblyAI API using your existing connection.

## Additional resources

You can learn more about using Activepieces with AssemblyAI in these resources:

- [AssemblyAI Integrations on Activepieces](https://www.activepieces.com/pieces/assemblyai)
- [npmjs page for @activepieces/piece-assemblyai](https://www.npmjs.com/package/@activepieces/piece-assemblyai)


---
title: Integrate Make with AssemblyAI
description: >-
  Use our Make (formerly Integromat) app to use AssemblyAI's speech AI in your
  Make scenarios.
hide-nav-links: true
---

[Make](https://make.com/) (formerly Integromat) is a workflow automation tool that lets you integrate various services together without requiring coding knowledge.

With the AssemblyAI app for Make, you can use our AI models to process audio data by transcribing it with speech recognition models, analyzing it with audio intelligence models, and building generative features on top of it with LLMs.
You can supply audio to the AssemblyAI app and connect the output of our models to other services in your Make scenarios.

## Quickstart

<Steps>
<Step>

Create or edit a scenario in Make.
Add a new module, search for AssemblyAI, and select the module that you want to use.

![Search for AssemblyAI modules in Make](file:a528223d-d045-47eb-8270-9ceefbcd3d79)

</Step>

<Step>

Select the module that you want to use.

![Add an AssemblyAI module in Make](file:90d98d83-d46f-4264-be95-791ec4267786)

</Step>

<Step>

Create a new connection or select an existing one.
In **AssemblyAI API Key**, enter the API key from your [AssemblyAI dashboard](https://www.assemblyai.com/app/api-keys), and click **Save**.

![Create a connection to AssemblyAI in Make](file:f1781b19-e664-44f4-957a-c1a9f8f8cbce)

</Step>

<Step>

Finally, configure your AssemblyAI module. Continue reading to learn more about all the available modules.

![Configure an AssemblyAI module in Make](file:fa4b1b4d-d950-4ed8-a8f5-f0c976e91d24)

</Step>
</Steps>

## AssemblyAI app modules

The AssemblyAI app for Make provides the following modules:

### Files

#### Upload a File

Upload an audio file to AssemblyAI so you can transcribe it.
You can pass the `Upload URL` output field to the `Audio URL` input field of [Transcribe an Audio File](#transcribe-an-audio-file) module.

### Transcripts

#### Transcribe an Audio File

Transcribe an audio file and wait until the transcript has completed or failed.
Configure the `Audio URL` field with the URL of the audio file you want to transcribe.
The `Audio URL` must be accessible by AssemblyAI's servers.
If you don't have a publicly accessible URL, you can use the [Upload a File](#upload-a-file) module to upload the audio file to AssemblyAI.

If you don't want to wait until the transcript is ready, change the `Wait until Transcript is Ready` parameter to `No` under **Show advanced settings**.

<Info>
  Configure your desired [Audio Intelligence models](/audio-intelligence) when
  you create the transcript. The results of the models will be included in the
  transcript output.
</Info>

#### Wait until Transcript is Ready

Wait for an existing transcript to be ready.
This module will complete when the status of the transcript changes to "completed" or "error".

#### Watch for Transcript Ready Notification

Create a webhook URL to receive a notification when a transcript is ready.
When the transcript is ready, the webhook will be invoked with the transcript status and ID.
The status will be "completed" or "error".

#### Get a Transcript

Retrieve a transcript by ID.

#### Get Paragraphs of a Transcript

Retrieve the paragraphs of a transcript.

<Note>You can only invoke this module after the transcript is completed.</Note>

#### Get Sentences of a Transcript

Retrieve the sentences of a transcript.

<Note>You can only invoke this module after the transcript is completed.</Note>

#### Get Subtitles for a Transcript

Create SRT or VTT subtitles for a transcript.

<Note>You can only invoke this module after the transcript is completed.</Note>

#### Get Redacted Audio of a Transcript

First, you need to configure PII audio redaction using these fields when you create the transcript:

- `Redact PII`: `Yes`
- `Redact PII Audio`: `Yes`
- `Redact PII Policies`: Configure at least one PII policy

Then, you can use this module to retrieve the redacted audio of the transcript.

<Note>You can only invoke this module after the transcript is completed.</Note>

#### Search for Words in a Transcript

Search for words in a transcript.

<Note>You can only invoke this module after the transcript is completed.</Note>

#### List Transcripts

Paginate over all transcripts.

#### Delete a Transcript

Delete a transcript by ID.
Deleting a transcript does not delete the transcript resource itself, but removes the data from the resource and marks it as deleted.

<Note>
  You can only invoke this module after the transcript status is "completed" or
  "error".
</Note>

### LeMUR

#### Run a Task using LeMUR

Prompt different LLMs over your audio data using LeMUR.
You have to configure either the `Transcript IDs` or `Input Text` input field.

#### Purge a LeMUR Request

Delete the data for a previously submitted LeMUR request.
Response data from the LLM, as well as any context provided in the original request will be removed.

### Other modules

#### Make an API Call

Make your own REST API HTTP requests to the AssemblyAI API using your existing connection.

## Additional resources

You can learn more about using Make with AssemblyAI in these resources:

- [Redact PII in Audio with Make and AssemblyAI](https://www.assemblyai.com/blog/redact-pii-audio-with-make/)
- [Make Integration page for AssemblyAI](https://www.make.com/en/integrations/assembly-ai)
- [AssemblyAI Make App Invitation Link](https://us1.make.com/app/invite/f21437a4d43b63efc8ee9aec385d9f10)


---
title: Account Management
---

On the [AssemblyAI dashboard](https://www.assemblyai.com/app), you can manage your projects and API keys, and see a breakdown of your usage and spend.

## Projects

Projects can be used to isolate data for different environments or applications, e.g., production, staging, or development. Each project has its own API keys, allowing for better organization and data access control.

Transcripts and other project-specific data is accessible only within the project they were created in — An API key from one project will not be able to access historical transcripts or data from another project. This separation maintains data security and prevents unintended cross-project access.

You can create, rename, and delete projects based on your plan:

| Usage limits       | Free | PAYG | Contracted | Enterprise |
| ------------------ | ---- | ---- | ---------- | ---------- |
| Number of projects | 2    | 2    | 5          | Custom     |

## API Keys

API keys are unique credentials that authenticate requests to the API. Each API key is associated with a specific project, ensuring secure and controlled access.

You can create and delete API keys based on your plan:

| Usage limits       | Free | PAYG | Contracted | Enterprise |
| ------------------ | ---- | ---- | ---------- | ---------- |
| Number of API keys | 2    | 4    | 25         | Custom     |

### Create a new API Key

1. Log in to your [AssemblyAI Dashboard](https://www.assemblyai.com/dashboard)
2. Navigate to the "API Keys" section
3. Click the "Create New API Key" button
4. Enter a descriptive name for your API key (e.g., "Production API", "Development API")
5. Click "Create"

### Delete an API Key

1. Log in to your [AssemblyAI Dashboard](https://www.assemblyai.com/dashboard)
2. Navigate to the "API Keys" section
3. Locate the API key you want to delete
4. Click the "Delete" button next to the key
5. Confirm the deletion in the popup dialog

<Error>
  This action cannot be undone. Make sure no active applications are using the
  key before deletion.
</Error>

## Reporting

Get insights into your usage and spend to track and manage costs effectively with the reporting tool in the AssemblyAI dashboard. You can analyze your usage and spend data at different levels of granularity:

- Account
- Product (e.g., Speech-to-text, Streaming, LeMUR)
- Models (e.g., Best, Claude 3.5 Sonnet, etc.)
- Project
- API key

## Usage Limits

<Note>

With the current version of multi-project support, rate limiting is applied at the account level, not at the project level. This means that the rate limits for each API key mirror the rate limits for the account.

Example: If an account has an Async concurrency of 200, each API key for that account will be able to process up to 200 requests concurrently.

</Note>

To ensure a smooth experience for all users, certain operations have per-account usage limits.

- **Concurrency limits** for asynchronous operations
- **Rate limits** for synchronous operations

<Note title="Need a higher concurrency?">

Our services are infinitely scalable and we offer custom concurrency limits that scale to support any workload at no additional cost. If you need a higher concurrency limit please either contact our{" "} <a href="https://www.assemblyai.com/contact" target="_blank"> Sales team </a>{" "} or send an email to our [Support team](https://www.assemblyai.com/contact/support).

</Note>

### Speech-to-Text usage limits

AssemblyAI limits the number of transcriptions being processed at any given time.

| Usage limit               | Free account | Paid account |
| ------------------------- | ------------ | ------------ |
| Concurrent transcriptions | 5            | 200          |

If you submit a transcription that would exceed your usage limit, it'll be added to a queue. Queued transcriptions will be processed automatically as previously submitted transcriptions complete.

If your account balance goes below zero, your concurrency limit will be reduced to 1.

If you exceed your concurrency limit, you'll receive an email stating that your transcripts have been throttled. Note that you'll only receive this email once per day.

![Email notifying user that their transcripts have been throttled due to exceeding the Concurrency limit](file:66fd3d09-0deb-45c5-891f-b6bcacf8367b)

### Streaming Speech-to-Text usage limits

AssemblyAI limits the number of concurrent streaming sessions.

| Usage limit                   | Free account | Paid account |
| ----------------------------- | ------------ | ------------ |
| Concurrent Streaming sessions | 5            | 100          |

<Tip>
Our Streaming STT feature includes automatically scaling concurrency limits for paid accounts.

Anytime you are using 70% or more of your current limit, your concurrency limit will automatically increase and scale up by 10% every 60 seconds.

As your traffic starts to scale back down and you are using less than 50% of your current limit, your concurrency will start to scale back down until it eventually returns to your default value.
</Tip>

If you exceed your current limit, you'll receive an error with an error message: `Unauthorized connection: Too many concurrent sessions`.

<Note title="Properly terminating sessions">

If you're consistently exceeding the limit of concurrent sessions, first make sure that you're terminating sessions properly.

- If you're using the [WebSocket API](https://www.assemblyai.com/docs/api-reference/streaming-api/streaming-api) directly, you need to send a `terminate_session` message.

</Note>

### LeMUR usage limits

LeMUR requests are rate limited within a 60-second time window. For more information, see [Rate limits](https://www.assemblyai.com/docs/api-reference/overview#rate-limits).

| Usage limit         | Free account | Paid account |
| ------------------- | ------------ | ------------ |
| Requests per minute | N/A*         | 30           |

\* LeMUR is only available for paid users.

## Best Practices

- Use different API keys for development and production environments
- Monitor usage and spend patterns for each API key in your dashboard
- Keep your API keys secure and never expose them in client-side code
- Use meaningful names and tags to easily identify the purpose of each key
- Store API keys as environment variables in your application


---
title: Webhooks
hide-nav-links: true
description: Get notified when a transcription is ready.
---

Webhooks are custom HTTP callbacks that you can define to get notified when your transcripts are ready.

To use webhooks, you need to set up your own webhook receiver to handle webhook deliveries.

## Create a webhook for a transcription

<Tip title="Don't have a webhook endpoint yet?">
  Create a test webhook endpoint with [webhook.site](https://webhook.site) to
  test your webhook integration.
</Tip>

<Tabs>
<Tab language="python-sdk" title="Python SDK">

To create a webhook, use `set_webhook()` on the transcription config. The URL must be accessible from AssemblyAI's servers.

Use `submit()` instead of `transcribe()` to create a transcription without waiting for it to complete.

```python
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig().set_webhook("https://example.com/webhook")

aai.Transcriber().submit(audio_file, config)
```

</Tab>
<Tab language="python" title="Python">

To create a webhook, set the `webhook_url` parameter when you create a new transcription. The URL must be accessible from AssemblyAI's servers.

```python
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url,
    "webhook_url": "https://example.com/webhook"
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)
```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">

To create a webhook, include the `webhook_url` parameter when you create a new transcription. The URL must be accessible from AssemblyAI's servers.

Use `submit()` instead of `transcribe()` to create a transcription without waiting for it to complete.

```javascript
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  webhook_url: "https://example.com/webhook",
};

const run = async () => {
  const transcript = await client.transcripts.submit(params);
};

run();
```

</Tab>
<Tab language="javascript" title="JavaScript">

To create a webhook, set the `webhook_url` parameter when you create a new transcription. The URL must be accessible from AssemblyAI's servers.

```javascript
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl,
  webhook_url: "https://example.com/webhook",
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });
```

</Tab>
<Tab language="csharp" title="C#">

To create a webhook, set the `webhook_url` parameter when you create a new transcription. The URL must be accessible from AssemblyAI's servers.

```csharp
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
public string Id { get; set; }
public string Status { get; set; }
public string Text { get; set; }
public string Error { get; set; }
}

async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
{
using (var fileStream = File.OpenRead(filePath))
using (var fileContent = new StreamContent(fileStream))
{
fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent))
        {
            response.EnsureSuccessStatusCode();
            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

}

async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
{
var data = new { audio_url = audioUrl, webhook_url = "https://example.com/webhook" };
var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

    using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
    {
        response.EnsureSuccessStatusCode();
        return await response.Content.ReadFromJsonAsync<Transcript>();
    }

}

// Main execution
using (var httpClient = new HttpClient())
{
httpClient.DefaultRequestHeaders.Authorization =
new AuthenticationHeaderValue("<YOUR-API-KEY>");

    var uploadUrl = await UploadFileAsync("my-audio.mp3", httpClient);
    var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);

}

```

</Tab>
<Tab language="ruby" title="Ruby">

To create a webhook, set the `webhook_url` parameter when you create a new transcription. The URL must be accessible from AssemblyAI's servers.

```ruby
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url,
    "webhook_url" => "https://example.com/webhook"
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end
```

</Tab>
<Tab language="php" title="PHP">

```php
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
"authorization: <YOUR_API_KEY>",
"content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
"audio_url" => $upload_url,
"webhook_url" => "https://example.com/webhook"
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

```

</Tab>
</Tabs>

## Handle webhook deliveries

When the transcript is ready, AssemblyAI will send a `POST` HTTP request to the URL that you specified.

<Note title="Webhooks and PII Audio Redaction">

If using webhooks with PII audio redaction enabled, you'll receive two webhook calls: the first when the redacted audio file is ready and the second when the request for transcription is completed.

</Note>

<Note title="Static Webhook IP addresses">

AssemblyAI sends all webhook deliveries from fixed IP addresses:

| Region | IP Address     |
| ------ | -------------- |
| US     | `44.238.19.20` |
| EU     | `54.220.25.36` |

</Note>

### Delivery payload

The webhook delivery payload contains a JSON object with the following properties:

```json
{
  "transcript_id": "5552493-16d8-42d8-8feb-c2a16b56f6e8",
  "status": "completed"
}
```

| Key             | Type   | Description                                                  |
| --------------- | ------ | ------------------------------------------------------------ |
| `transcript_id` | string | The ID of the transcript.                                    |
| `status`        | string | The status of the transcript. Either `completed` or `error`. |

### Retrieve a transcript with the transcript ID

<Tabs>
<Tab language="python-sdk" title="Python SDK" default>

```python
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

transcript = aai.Transcript.get_by_id("<TRANSCRIPT_ID>")

if transcript.status == "error":
  raise RuntimeError(f"Transcription failed: {transcript.error}")

print(transcript.text)

```

</Tab>
<Tab language="python" title="Python">

```python
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

transcript_id = "<TRANSCRIPT_ID>"

polling_endpoint = f"https://api.assemblyai.com/v2/transcript/{transcript_id}"

transcription_result = requests.get(polling_endpoint, headers=headers).json()

if transcription_result['status'] == 'completed':
  print(f"Transcript ID:", transcript_id)
elif transcription_result['status'] == 'error':
  raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK" default>

```javascript
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

const transcript = await client.transcripts.get("<TRANSCRIPT_ID>");

if (transcript.status === "error") {
  throw new Error(`Transcription failed: ${transcript.error}`);
}

console.log(transcript.text);
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const transcriptId = "<TRANSCRIPT_ID>";
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

const pollingResponse = await axios.get(pollingEndpoint, {
  headers: headers,
});
const transcriptionResult = pollingResponse.data;

if (transcriptionResult.status === "completed") {
  console.log(transcriptionResult.text);
} else if (transcriptionResult.status === "error") {
  throw new Error(`Transcription failed: ${transcriptionResult.error}`);
}
```

</Tab>
<Tab language="csharp" title="C#">

```csharp
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
public string Id { get; set; }
public string Status { get; set; }
public string Text { get; set; }
public string Error { get; set; }
}

async Task<Transcript> RetrieveTranscript(string transcriptId, HttpClient httpClient)
{
var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcriptId}";
    var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
    var transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();
    switch (transcript.Status)
    {
        case "completed":
            return transcript;
        case "error":
            throw new Exception($"Transcription failed: {transcript.Error}");
default:
throw new Exception("This code should not be reachable.");
}
}

// Main execution
using (var httpClient = new HttpClient())
{
httpClient.DefaultRequestHeaders.Authorization =
new AuthenticationHeaderValue("<YOUR-API-KEY>");
var transcript = await RetrieveTranscript("<TRANSCRIPT_ID>", httpClient);
}

```

</Tab>
<Tab language="ruby" title="Ruby">

```ruby
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
}

transcript_id = "<TRANSCRIPT_ID>"
polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
polling_http.use_ssl = true
polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
polling_response = polling_http.request(polling_request)

transcription_result = JSON.parse(polling_response.body)

if transcription_result['status'] == 'completed'
  puts "Transcription text: #{transcription_result['text']}"
elsif transcription_result['status'] == 'error'
  raise "Transcription failed: #{transcription_result['error']}"
else
  puts 'Waiting for transcription to complete...'
  sleep(3)
end
```

</Tab>
<Tab language="php" title="PHP">

```php
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
"authorization: <YOUR_API_KEY>",
"content-type: application/json"
);

$transcript_id = "<TRANSCRIPT_ID>";
$polling_endpoint = "https://api.assemblyai.com/v2/transcript/" . $transcript_id;

$polling_response = curl_init($polling_endpoint);
curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

$transcription_result = json_decode(curl_exec($polling_response), true);

if ($transcription_result['status'] === "completed") {
    echo $transcription_result['text'];
} else if ($transcription_result['status'] === "error") {
throw new Exception("Transcription failed: " . $transcription_result['error']);
}

```

  </Tab>
</Tabs>

## Authenticate webhook deliveries

You can authenticate webhook deliveries from AssemblyAI by including a custom HTTP header in the request.

<Tabs groupId="language">
<Tab language="python" title="Python">

To add an authentication header, include the auth header name and value in `set_webhook()`.

```python {2}
config = aai.TranscriptionConfig().set_webhook(
    "https://example.com/webhook", "X-My-Webhook-Secret", "secret-value"
)

aai.Transcriber().submit(audio_url, config)
```

</Tab>
<Tab language="javascript" title="JavaScript">

To add an authentication header, include the `webhook_auth_header_name` and `webhook_auth_header_value` parameters.

```javascript {5-6}
client.transcripts.submit({
  audio:
    'https://assembly.ai/wildfires.mp3',
  webhook_url: 'https://example.com/webhook'
  webhook_auth_header_name: "X-My-Webhook-Secret",
  webhook_auth_header_value: "secret-value"
})
```

</Tab>
<Tab language="golang" title="Go">

To add an authentication header, include the `WebhookAuthHeaderName` and `WebhookAuthHeaderValue` parameters.

```go {3-4}
client.Transcripts.SubmitFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    WebhookURL:             aai.String("https://example.com/webhook"),
    WebhookAuthHeaderName:  aai.String("X-My-Webhook-Secret"),
    WebhookAuthHeaderValue: aai.String("secret-value"),
})
```

</Tab>
<Tab language="java" title="Java">

To add an authentication header, include the `webhookAuthHeaderName` and `webhookAuthHeaderValue` parameters.

```java {3-4}
var params = TranscriptOptionalParams.builder()
        .webhookUrl("https://example.com/webhook")
        .webhookAuthHeaderName("X-My-Webhook-Secret")
        .webhookAuthHeaderValue("secret-value")
        .build();
```

</Tab>
<Tab language="csharp" title="C#">

To add an authentication header, include the `WebhookAuthHeaderName` and `WebhookAuthHeaderValue` parameters.

```csharp {5-6}
var transcriptParams = new TranscriptParams
{
    AudioUrl = audioUrl,
    WebhookUrl = "https://example.com/webhook",
    WebhookAuthHeaderName = "X-My-Webhook-Secret",
    WebhookAuthHeaderValue = "secret-value"
};
```

</Tab>
<Tab language="ruby" title="Ruby">

To add an authentication header, include the `webhook_auth_header_name` and `webhook_auth_header_value` parameters.

```ruby {4-5}
transcript = client.transcripts.submit(
  audio_url: audio_url,
  webhook_url: 'https://example.com/webhook',
  webhook_auth_header_value: 'X-My-Webhook-Secret',
  webhook_auth_header_name: 'secret-value'
)
```

</Tab>
</Tabs>
<Tabs>
<Tab language="python-sdk" title="Python SDK">

To add an authentication header, include the auth header name and value in `set_webhook()`.

```python
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig().set_webhook("https://example.com/webhook", "X-My-Webhook-Secret", "secret-value")

aai.Transcriber().submit(audio_file, config)
```

</Tab>
<Tab language="python" title="Python">

To add an authentication header, include the auth header name and value in `set_webhook()`.

```python
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url,
    "webhook_url": "https://example.com/webhook",
    "webhook_auth_header_name": "X-My-Webhook-Secret",
    "webhook_auth_header_value": "secret-value"
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)
```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">

To add an authentication header, include the `webhook_auth_header_name` and `webhook_auth_header_value` parameters.

```javascript
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  webhook_url: "https://example.com/webhook",
  webhook_auth_header_name: "X-My-Webhook-Secret",
  webhook_auth_header_value: "secret-value",
};

const run = async () => {
  const transcript = await client.transcripts.submit(params);
};

run();
```

</Tab>
<Tab language="javascript" title="JavaScript">

To add an authentication header, include the `webhook_auth_header_name` and `webhook_auth_header_value` parameters.

```javascript
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl,
  webhook_url: "https://example.com/webhook",
  webhook_auth_header_name: "X-My-Webhook-Secret",
  webhook_auth_header_value: "secret-value",
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });
```

</Tab>
<Tab language="csharp" title="C#">

To add an authentication header, include the `webhook_auth_header_name` and `webhook_auth_header_value` parameters.

```csharp
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
}

async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
{
    using (var fileStream = File.OpenRead(filePath))
    using (var fileContent = new StreamContent(fileStream))
    {
        fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent))
        {
            response.EnsureSuccessStatusCode();
            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }
}

async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
{
    var data = new { audio_url = audioUrl, webhook_url = "https://example.com/webhook", webhook_auth_header_name = "X-My-Webhook-Secret", webhook_auth_header_value = "secret-value" };
    var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

    using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
    {
        response.EnsureSuccessStatusCode();
        return await response.Content.ReadFromJsonAsync<Transcript>();
    }
}

// Main execution
using (var httpClient = new HttpClient())
{
    httpClient.DefaultRequestHeaders.Authorization =
        new AuthenticationHeaderValue("<YOUR-API-KEY>");

    var uploadUrl = await UploadFileAsync("my-audio.mp3", httpClient);
    var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);
}
```

</Tab>
<Tab language="ruby" title="Ruby">

To add an authentication header, include the `webhook_auth_header_name` and `webhook_auth_header_value` parameters.

```ruby
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url,
    "webhook_url" => "https://example.com/webhook",
    "webhook_auth_header_name" => "X-My-Webhook-Secret",
    "webhook_auth_header_value" => "secret-value"
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end
```

</Tab>
<Tab language="php" title="PHP">

To add an authentication header, include the auth header name and value in `set_webhook()`.

```php
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
"authorization: <YOUR_API_KEY>",
"content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
"audio_url" => $upload_url,
"webhook_url" => "https://example.com/webhook",
"webhook_auth_header_name" => "X-My-Webhook-Secret",
"webhook_auth_header_value" => "secret-value"
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

```

</Tab>
</Tabs>

## Add metadata to webhook deliveries

To associate metadata for a specific transcription request, you can add your own query parameters to the webhook URL.

```plain
https://example.com/webhook?customer_id=1234&order_id=5678
```

Now, when you receive the webhook delivery, you'll know the customer who requested it.

## Failed webhook deliveries

Webhook deliveries can fail for multiple reasons. For example, if your server is down or takes more than 10 seconds to respond.

If a webhook delivery fails, AssemblyAI will attempt to redeliver it up to 10 times, waiting 10 seconds between each attempt. If all attempts fail, AssemblyAI considers the delivery as permanently failed.


---
title: Pre-Recorded Audio
description: Transcribe a pre-recorded audio file
---

Our Speech-to-Text model enables you to transcribe pre-recorded audio into written text.

On top of the transcription, you can enable other features and models, such as [Speaker Diarization](/docs/speech-to-text/speaker-diarization), by adding additional parameters to the same transcription request.

<Tip title="Choose Model Class">
You can use the optional `speech_model` parameter to specify the class of models. To learn more, see [Select the speech model](/docs/speech-to-text/pre-recorded-audio/supported-languages).
</Tip>

# Quickstart

The following example transcribes an audio file from a local file.

<CodeBlocks>

```python title="Python SDK" maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

audio_file = "./local_file.mp3"
# audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(speech_model=aai.SpeechModel.slam_1)

transcript = aai.Transcriber(config=config).transcribe(audio_file)

if transcript.status == "error":
  raise RuntimeError(f"Transcription failed: {transcript.error}")

print(transcript.text)
```

```python title="Python" maxLines=15
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "speech_model": "slam-1"
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(f"Transcript ID:", transcript_id)
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

```

```javascript title="JavaScript SDK" maxLines=15
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

const audioFile = "./local_file.mp3";
// const audioFile =
//   'https://assembly.ai/wildfires.mp3'

const params = {
  audio: audioFile,
  speech_model: "slam-1",
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  console.log(transcript.text);
};

run();
```

```javascript title="JavaScript" maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  speech_model: "slam-1",
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

```csharp title="C#" maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
}

async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
{
    using (var fileStream = File.OpenRead(filePath))
    using (var fileContent = new StreamContent(fileStream))
    {
        fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent))
        {
            response.EnsureSuccessStatusCode();
            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }
}

async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
{
    var data = new { audio_url = audioUrl, speech_model = "slam_1" };
    var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

    using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
    {
        response.EnsureSuccessStatusCode();
        return await response.Content.ReadFromJsonAsync<Transcript>();
    }
}

async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
{
    var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

    while (true)
    {
        var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
        transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();
        switch (transcript.Status)
        {
            case "queued" or "processing":
                await Task.Delay(TimeSpan.FromSeconds(3));
                break;
            case "completed":
                return transcript;
            case "error":
                throw new Exception($"Transcription failed: {transcript.Error}");
            default:
                throw new Exception("This code should not be reachable.");
        }
    }
}

// Main execution
using (var httpClient = new HttpClient())
{
    httpClient.DefaultRequestHeaders.Authorization =
        new AuthenticationHeaderValue("<YOUR-API-KEY>");

    var uploadUrl = await UploadFileAsync("my-audio.mp3", httpClient);
    var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);
    transcript = await WaitForTranscriptToProcess(transcript, httpClient);

    Console.WriteLine(transcript.Text);
}
```

```ruby title="Ruby" maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
    "speech_model" => "slam_1"
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end
```

```php title="PHP" maxLines=15
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "speech_model" => "slam_1"
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        echo $transcription_result['text'];
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

</CodeBlocks>
# Example output

```plain maxLines=15
Smoke from hundreds of wildfires in Canada is triggering air quality alerts
throughout the US. Skylines from Maine to Maryland to Minnesota are gray and
smoggy. And...
```

# API reference

You can find the API reference [here](/docs/api-reference/transcripts/submit).


---
title: Transcript Status
---

After you've submitted a file for transcription, your transcript has one of the following statuses:

| Status       | Description                                        |
| ------------ | -------------------------------------------------- |
| `processing` | The audio file is being processed.                 |
| `queued`     | The audio file is waiting to be processed.         |
| `completed`  | The transcription has completed successfully.      |
| `error`      | An error occurred while processing the audio file. |

## Handling errors

If the transcription fails, the status of the transcript is `error`, and the transcript includes an `error` property explaining what went wrong.

<CodeBlocks>

```python title="Python SDK" highlight={12-14} maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig()

transcript = aai.Transcriber().transcribe(audio_file, config)

if transcript.status == aai.TranscriptStatus.error:
    print(f"Transcription failed: {transcript.error}")
    exit(1)

print(transcript.text)
```

```python title="Python" highlight={34-35} maxLines=15
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url # You can also use a URL to an audio or video file on the web
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(f"Transcript: {transcription_result['text']}")
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)
```

```ts title="JavaScript SDK" highlight={17-20}maxLines=15
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  if (transcript.status === "error") {
    console.error(`Transcription failed: ${transcript.error}`);
    process.exit(1);
  }

  console.log(transcript.text);
};

run();
```

```ts title="JavaScript" highlight={36-37} maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

```csharp title="C#" highlight={98-99} maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }

    [JsonPropertyName("language_code")]
    public string LanguageCode { get; set; }

    public string Error { get; set; }
}

class Program
{
    static void Main(string[] args)
    {
        MainAsync(args).GetAwaiter().GetResult();
    }

    static async Task MainAsync(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Add("authorization", "<YOUR-API-KEY>");

            var localFilePath = "audio.mp3"; // Replace with your local file path

            Console.WriteLine("Uploading file...");
            var uploadUrl = await UploadFileAsync(localFilePath, httpClient);

            Console.WriteLine("Creating transcript...");
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);

            Console.WriteLine("Waiting for transcript...");
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            Console.WriteLine("Transcription completed!");
            Console.WriteLine("----------------------------------");
            Console.WriteLine(transcript.Text);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var content = new StreamContent(fileStream))
        {
            content.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", content);
            response.EnsureSuccessStatusCode();

            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new { audio_url = audioUrl };
        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "processing":
                case "queued":
                    Console.WriteLine($"Status: {transcript.Status}... waiting...");
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("Unexpected transcript status.");
            }
        }
    }
}
```

```ruby title="Ruby" highlight={55-56} maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url # You can also use a URL to an audio or video file on the web
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end
```

```php title="PHP" highlight={62-63} maxLines=15
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url // You can also use a URL to an audio or video file on the web
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        echo $transcription_result['text'];
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

</CodeBlocks>

<Note>
A transcription may fail for various reasons:

- Unsupported file format
- Missing audio in file
- Unreachable audio URL

If a transcription fails due to a server error, we recommend that you resubmit the file for transcription to allow another server to process the audio.

</Note>

<Accordion title={`Why am I receiving a "400 Bad Request" error when making an API request?`} theme="dark" iconColor="white" >
  
A "400 Bad Request" error typically indicates that there's a problem with the formatting or content of the API request. Double-check the syntax of your request and ensure that all required parameters are included as described in the [API reference](https://assemblyai.com/docs/api-reference/transcripts). If the issue persists, contact our support team for assistance.

  </Accordion>


---
title: Select the Speech Model
---

We use a combination of models to produce your results. You can select the class of models to use in order to make cost-performance tradeoffs best suited for your application.
You can visit our <a href="https://www.assemblyai.com/pricing" target="_blank">pricing page</a> for more information on our model tiers.

<Tabs>
<Tab language="python-sdk" title="Python SDK">

| Name                    | SDK Parameter               | Description                                                                           |
| ----------------------- | --------------------------- | ------------------------------------------------------------------------------------- |
| **Slam-1**              | `aai.SpeechModel.slam_1`    | Use our most customizable model for your transcription (English only).                |
| **Universal** (default) | `aai.SpeechModel.universal` | Use our fastest, most robust models with the broadest language support.          |

</Tab>
<Tab language="typescript-sdk" title="JavaScript SDK">

| Name                    | SDK Parameter | Description                                                                           |
| ----------------------- | ------------- | ------------------------------------------------------------------------------------- |
| **Slam-1**              | `'slam-1'`    | Use our most customizable model for your transcription (English only).                |
| **Universal** (default) | `'universal'` | Use our fastest, most robust models with the broadest language support.          |

</Tab>
<Tab language="api" title="API">

| Name                    | API Parameter                | Description                                                                           |
| ----------------------- | ---------------------------- | ------------------------------------------------------------------------------------- |
| **Slam-1**              | `"speech_model":"slam-1"`    | Use our most customizable model for your transcription (English only).                |
| **Universal** (default) | `"speech_model":"universal"` | Use our fastest, most robust models with the best results for most languages.          |

</Tab>
</Tabs>
<br />

<Tabs>
<Tab language="python-sdk" title="Python SDK" default>

You can change the model by setting the `speech_model` in the transcription config:

```python highlight={9} maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"


config = aai.TranscriptionConfig(speech_model=aai.SpeechModel.slam_1)

transcript = aai.Transcriber(config=config).transcribe(audio_file)

if transcript.status == "error":
  raise RuntimeError(f"Transcription failed: {transcript.error}")

print(transcript.text)
```

</Tab>
<Tab language="python" title="Python">

You can change the model by setting the `speech_model` in the POST request body:

```python highlight={19} maxLines=15
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "speech_model": "slam-1"
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(transcription_result['text'])
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

```

</Tab>
<Tab language="typescript-sdk" title="JavaScript SDK" default>

You can change the model by setting the `speech_model` in the transcript parameters:

```javascript highlight={12} maxLines=15
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  speech_model: "slam-1",
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  console.log(transcript.text);
};

run();
```

</Tab>
<Tab language="typescript" title="JavaScript">

You can change the model by setting the `speech_model` in the POST request body:

```javascript highlight={19} maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  speech_model: "slam-1",
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

</Tab>
<Tab language="csharp" title="C#">

You can change the model by setting the `speech_model` in the POST request body:

```csharp highlight={69} maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
}

class Program
{
    static void Main(string[] args)
    {
        MainAsync(args).GetAwaiter().GetResult();
    }

    static async Task MainAsync(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Add("authorization", "<YOUR-API-KEY>");

            var localFilePath = "audio.mp3";

            Console.WriteLine("Uploading file...");
            var uploadUrl = await UploadFileAsync(localFilePath, httpClient);

            Console.WriteLine("Creating transcript with speech_model...");
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);

            Console.WriteLine("Waiting for transcript...");
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            Console.WriteLine("Transcription completed!");
            Console.WriteLine("----------------------------------");
            Console.WriteLine(transcript.Text);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var content = new StreamContent(fileStream))
        {
            content.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", content);
            response.EnsureSuccessStatusCode();

            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new
        {
            audio_url = audioUrl,
            speech_model = "slam-1"
        };

        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "processing":
                case "queued":
                    Console.WriteLine($"Status: {transcript.Status}... waiting...");
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("Unexpected transcript status.");
            }
        }
    }
}
```

</Tab>
<Tab language="ruby" title="Ruby">

```ruby highlight={23} maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
    "speech_model" => "slam-1"
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end
```

</Tab>
<Tab language="php" title="PHP">

```php highlight={30} maxLines=15
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
"authorization: <YOUR_API_KEY>",
"content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
"audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
"speech_model" => "slam-1"
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
$polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        echo $transcription_result['text'];
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }

}
```

</Tab>
</Tabs>

For a list of the supported languages for each model, see [Supported languages](/docs/speech-to-text/pre-recorded-audio/supported-languages).


---
title: Improving Transcript Accuracy
---

For optimal transcription accuracy, we recommend using our `slam-1` model, which offers superior performance and fine-tuning capabilities. Here's how to get the best results:

## Using Slam-1 (Recommended)

### Fine-tuning with `keyterms_prompt`

Improve transcription accuracy by leveraging Slam-1's contextual understanding capabilities by prompting the model with certain words or phrases that are likely to appear frequently in your audio file.

Rather than simply increasing the likelihood of detecting specific words, Slam-1's multi-modal architecture actually understands the semantic meaning and context of the terminology you provide, enhancing transcription quality not just of the exact terms you specify, but also related terminology, variations, and contextually similar phrases.

Provide up to 1000 domain-specific words or phrases (maximum 6 words per phrase) that may appear in your audio using the optional `keyterms_prompt` parameter:

<Warning>
  This parameter is only supported when the `speech_model` is set to `"slam-1"`.
</Warning>

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
    ```python
    import requests
    import time

    base_url = "https://api.assemblyai.com"
    headers = {"authorization": "<YOUR_API_KEY>"}

    data = {
        "audio_url": "https://assembly.ai/sports_injuries.mp3",
        "speech_model": "slam-1",
        "keyterms_prompt": ['differential diagnosis', 'hypertension', 'Wellbutrin XL 150mg']
    }

    response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

    if response.status_code != 200:
        print(f"Error: {response.status_code}, Response: {response.text}")
        response.raise_for_status()

    transcript_response = response.json()
    transcript_id = transcript_response["id"]
    polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

    while True:
        transcript = requests.get(polling_endpoint, headers=headers).json()
        if transcript["status"] == "completed":
            print(transcript["text"])
            break
        elif transcript["status"] == "error":
            raise RuntimeError(f"Transcription failed: {transcript['error']}")
        else:
            time.sleep(3)
    ```

  </Tab>
  <Tab language="typescript" title="JavaScript">
    ```javascript
      import axios from 'axios'

      const baseUrl = 'https://api.assemblyai.com'

      const headers = {
        authorization: '<YOUR_API_KEY>'
      }

      const data = {
        audio_url: 'https://assembly.ai/sports_injuries.mp3',
        speech_model: 'slam-1',
        keyterms_prompt: ['differential diagnosis', 'hypertension', 'Wellbutrin XL 150mg']
      }

      const url = `${baseUrl}/v2/transcript`
      const response = await axios.post(url, data, { headers: headers })

      const transcriptId = response.data.id
      const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`

      while (true) {
        const pollingResponse = await axios.get(pollingEndpoint, {
          headers: headers
        })
        const transcriptionResult = pollingResponse.data

        if (transcriptionResult.status === 'completed') {
          console.log(transcriptionResult.text)
          break
        } else if (transcriptionResult.status === 'error') {
          throw new Error(`Transcription failed: ${transcriptionResult.error}`)
        } else {
          await new Promise((resolve) => setTimeout(resolve, 3000))
        }
      }
    ```

  </Tab>
</Tabs>

<Note title="Keyword count limits">
While we support up to 1000 key words and phrases, actual capacity may be lower due to internal tokenization and implementation constraints.
Key points to remember:
- Each word in a multi-word phrase counts towards the 1000 keyword limit
- Capitalization affects capacity (uppercase tokens consume more than lowercase)
- Longer words consume more capacity than shorter words

For optimal results, use shorter phrases when possible and be mindful of your total token count when approaching the keyword limit.

</Note>

## Using Universal or Nano Models

If you're currently using our `universal` or `nano` models and experiencing accuracy issues:

1. **Consider upgrading to Slam-1**: This is the recommended solution for better accuracy, especially with domain-specific content.

2. **Alternative approach (if Slam-1 isn't an option)**:
   If you must continue using `universal` or `nano` models, you can try the LeMUR Custom Vocabulary approach as a workaround, though this may not provide the same level of accuracy as Slam-1 with `keyterms_prompt`.

   <Tip title="LeMUR Custom Vocabulary">
     Learn more about using [LeMUR Custom
     Vocabulary](/docs/guides/custom-vocab-lemur) if you need to improve
     accuracy while using universal or nano models.
   </Tip>


---
title: Select the Region
---

The default region is US, with base URL `api.assemblyai.com`. For EU data residency requirements, you can use our base URL for EU at `api.eu.assemblyai.com`.

<Note>
  The base URL for EU is currently only available for Async transcription and LeMUR.
</Note>

| Region       | Base URL                |
| ------------ | ----------------------- |
| US (default) | `api.assemblyai.com`    |
| EU           | `api.eu.assemblyai.com` |

<br />

To use the EU endpoint, set the base URL for your requests as `api.eu.assemblyai.com`.

<CodeBlocks>

```python title="Python SDK" highlight={4} maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"
aai.settings.base_url = "https://api.eu.assemblyai.com"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig()

transcript = aai.Transcriber(config=config).transcribe(audio_file)

if transcript.status == "error":
  raise RuntimeError(f"Transcription failed: {transcript.error}")

print(transcript.text)
```

```python title="Python" highlight={4} maxLines=15
import requests
import time

base_url = "https://api.eu.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url # You can also use a URL to an audio or video file on the web
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = f"https://api.eu.assemblyai.com/v2/transcript/{transcript_id}"

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(f"Transcript ID:", transcript_id)
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

```

```javascript title="JavaScript SDK" highlight={5} maxLines=15
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
  baseUrl: "https://api.eu.assemblyai.com",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  console.log(transcript.text);
};

run();
```

```javascript title="JavaScript" highlight={4} maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.eu.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

```csharp title="C#" highlight={69} maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
}

class Program
{
    static void Main(string[] args)
    {
        MainAsync(args).GetAwaiter().GetResult();
    }

    static async Task MainAsync(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Add("authorization", "<YOUR-API-KEY>");

            var localFilePath = "audio.mp3";

            Console.WriteLine("Uploading file...");
            var uploadUrl = await UploadFileAsync(localFilePath, httpClient);

            Console.WriteLine("Creating transcript...");
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);

            Console.WriteLine("Waiting for transcript...");
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            Console.WriteLine("Transcription completed!");
            Console.WriteLine("----------------------------------");
            Console.WriteLine(transcript.Text);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var content = new StreamContent(fileStream))
        {
            content.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            var response = await httpClient.PostAsync("https://api.eu.assemblyai.com/v2/upload", content);
            response.EnsureSuccessStatusCode();

            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new { audio_url = audioUrl };
        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.eu.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.eu.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "processing":
                case "queued":
                    Console.WriteLine($"Status: {transcript.Status}... waiting...");
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("Unexpected transcript status.");
            }
        }
    }
}
```

```ruby title="Ruby" highlight={4} maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.eu.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url # You can also use a URL to an audio or video file on the web
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end
```

```php title="PHP" highlight={5} maxLines=15
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.eu.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url // You can also use a URL to an audio or video file on the web
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = "https://api.eu.assemblyai.com/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        echo $transcription_result['text'];
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

</CodeBlocks>


---
title: Supported languages
hide-nav-links: true
description: See a list of supported languages by model
---

This page shows which languages are supported by the AssemblyAI Async API, their `language_code` values, and the features available for that language.

AssemblyAI offers two different levels of speech-to-text models for pre-recorded audio: **Slam-1** and **Universal**. Check out the [Models](/docs/getting-started/models) page of our documentation to learn more about our different models and how to choose the best one for your use case.

Below, you'll find the supported languages for each class of models, as well as instructions on how to change the language for your transcription.

## Supported languages for Slam-1

<iframe
  className="airtable-embed"
  src="https://airtable.com/embed/apptQjwE6ZsC2Zfoi/shr0akLmAjn8bE9RP?backgroundColor=green"
  width="100%"
  height="225"
  style={{ background: "transparent", border: "1px solid #ccc" }}
/>
<noscript>
  <BestLanguageTable />
</noscript>

## Supported languages for Universal

<iframe
  className="airtable-embed"
  src="https://airtable.com/embed/appAHNKtjqyFB83AF/shrhQFfUxq1PROjTf?backgroundColor=green"
  width="100%"
  height="533"
  style={{ background: "transparent", border: "1px solid #ccc" }}
/>
<noscript>
  <BestLanguageTable />
</noscript>

### Breakdown of Universal language support

<AccordionGroup>
  
  <Accordion title="High accuracy (≤ 10% WER)">
    English, Spanish, French, German, Indonesian, Italian, Japanese, Dutch, Polish, Portuguese, Russian, Turkish, Ukrainian, Catalan
  </Accordion>

<Accordion title="Good accuracy (>10% to ≤25% WER)">
  Arabic, Azerbaijani, Bulgarian, Bosnian, Mandarin Chinese, Czech, Danish,
  Greek, Estonian, Finnish, Filipino, Galician, Hindi, Croatian, Hungarian,
  Korean, Macedonian, Malay, Norwegian Bokmål, Romanian, Slovak, Swedish, Thai,
  Urdu, Vietnamese, Cantonese
</Accordion>

<Accordion title="Moderate accuracy (>25% to ≤50% WER)">
  Afrikaans, Belarusian, Welsh, Persian (Farsi), Hebrew, Armenian, Icelandic,
  Kazakh, Lithuanian, Latvian, Māori, Marathi, Slovenian, Swahili, Tamil
</Accordion>

<Accordion title="Fair accuracy (>50% WER)">
  Amharic, Assamese, Bengali, Gujarati, Hausa, Javanese, Georgian, Khmer,
  Kannada, Luxembourgish, Lingala, Lao, Malayalam, Mongolian, Maltese, Burmese,
  Nepali, Occitan, Punjabi, Pashto, Sindhi, Shona, Somali, Serbian, Telugu,
  Tajik, Uzbek, Yoruba
</Accordion>

</AccordionGroup>

## Specify the language

You can use the optional `language_code` parameter to specify the language of the spoken audio in the files that you submit to the API. If you don't include a `language_code` parameter in your request, it defaults to `en_us`.

To learn more about using the `language_code` parameter, see [Set language manually](/docs/speech-to-text/pre-recorded-audio/set-language-manually).

<Tip>

If you're unsure of the spoken language spoken in your audio file, you can use our [Automatic Language Detection](/docs/speech-to-text/pre-recorded-audio/automatic-language-detection) feature to automatically identify the dominant language in your file.

</Tip>

## Specify the speech model

You can use the optional `speech_model` parameter to specify the class of models. To learn more, see [Select the speech model](/docs/speech-to-text/pre-recorded-audio/select-the-speech-model-with-best-and-nano).


---
title: Automatic Language Detection
---

Identify the dominant language spoken in an audio file and use it during the transcription. Enable it to detect any of the [supported languages](/docs/speech-to-text/pre-recorded-audio/supported-languages).

<Note title="Additional language support">
  We will be adding support for all Universal tier languages by the end of Q3 2025. Until then, if you need automatic language detection for an unsupported language please reach out to our Support team at support@assemblyai.com for more information on how that can be accomplished.
</Note>

<Warning>To reliably identify the dominant language, a file must contain **at least 15 seconds** of spoken audio. Results will be improved if there is at least 15-90 seconds of spoken audio in the file.</Warning>

<CodeBlocks>

```python title="Python SDK" highlight={8,13} maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(language_detection=True)

transcript = aai.Transcriber(config=config).transcribe(audio_file)

print(transcript.text)
print(transcript.json_response["language_code"])
```

```python title="Python" highlight={19} maxLines=15
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "language_detection": True
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(f"Transcript ID:", transcript_id)
    print(f"Language Code:", transcription_result['language_code'])
    print(f"Text:", transcription_result['text'])
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

```

```javascript title="JavaScript SDK" highlight={12} maxLines=15
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  language_detection: true,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  console.log(transcript.text);
  console.log(transcript.language_code);
};

run();
```

```javascript title="JavaScript" highlight={19} maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  language_detection: true,
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    console.log(transcriptionResult.language_code);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

```csharp title="C#" highlight={69}  maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
}

class Program
{
    static void Main(string[] args)
    {
        MainAsync(args).GetAwaiter().GetResult();
    }

    static async Task MainAsync(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Add("authorization", "<YOUR-API-KEY>");

            var localFilePath = "audio.mp3";

            Console.WriteLine("Uploading file...");
            var uploadUrl = await UploadFileAsync(localFilePath, httpClient);

            Console.WriteLine("Creating transcript with speech_model...");
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);

            Console.WriteLine("Waiting for transcript...");
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            Console.WriteLine("Transcription completed!");
            Console.WriteLine("----------------------------------");
            Console.WriteLine(transcript.Text);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var content = new StreamContent(fileStream))
        {
            content.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", content);
            response.EnsureSuccessStatusCode();

            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new
        {
            audio_url = audioUrl,
            language_detection = true
        };

        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "processing":
                case "queued":
                    Console.WriteLine($"Status: {transcript.Status}... waiting...");
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("Unexpected transcript status.");
            }
        }
    }
}
```

```ruby title="Ruby" highlight={23} maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
    "language_detection" => true
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    puts "Language code: #{transcription_result['language_code']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end
```

```php title="PHP" highlight={30} maxLines=15
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "language_detection" => true
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        echo $transcription_result['text'];
        echo $transcription_result['language_code'];
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

</CodeBlocks>

## Confidence score

If language detection is enabled, the API returns a confidence score for the detected language. The score ranges from 0.0 (low confidence) to 1.0 (high confidence).

<CodeBlocks>

```python title="Python SDK" highlight={8,13} maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(language_detection=True)

transcript = aai.Transcriber(config=config).transcribe(audio_file)

print(transcript.text)
print(transcript.json_response["language_confidence"])
```

```python title="Python" highlight={33} maxLines=15
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "language_detection": True
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(f"Transcript ID:", transcript_id)
    print(f"Language Confidence:", transcription_result['language_confidence'])
    print(f"Text:", transcription_result['text'])
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

```

```javascript title="JavaScript SDK" highlight={19} maxLines=15
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  language_detection: true,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  console.log(transcript.text);
  console.log(transcript.language_confidence);
};

run();
```

```javascript title="JavaScript" highlight={36} maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  language_detection: true,
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    console.log(transcriptionResult.language_confidence);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

```csharp title="C#" highlight={83} maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
    [JsonPropertyName("language_confidence")]
    public double? LanguageConfidence { get; set; }
}

async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
{
    using (var fileStream = File.OpenRead(filePath))
    using (var fileContent = new StreamContent(fileStream))
    {
        fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent))
        {
            response.EnsureSuccessStatusCode();
            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }
}

async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
{
    var data = new { audio_url = audioUrl, language_detection = true };
    var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

    using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
    {
        response.EnsureSuccessStatusCode();
        return await response.Content.ReadFromJsonAsync<Transcript>();
    }
}

async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
{
    var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

    while (true)
    {
        var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
        transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();
        switch (transcript.Status)
        {
            case "queued" or "processing":
                await Task.Delay(TimeSpan.FromSeconds(3));
                break;
            case "completed":
                return transcript;
            case "error":
                throw new Exception($"Transcription failed: {transcript.Error}");
            default:
                throw new Exception("This code should not be reachable.");
        }
    }
}

// Main execution
using (var httpClient = new HttpClient())
{
    httpClient.DefaultRequestHeaders.Authorization =
        new AuthenticationHeaderValue("<YOUR-API-KEY>");

    var uploadUrl = await UploadFileAsync("my-audio.mp3", httpClient);
    var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);
    transcript = await WaitForTranscriptToProcess(transcript, httpClient);

    Console.WriteLine(transcript.Text);
    Console.WriteLine(transcript.LanguageConfidence);
}
```

```ruby title="Ruby" highlight={55} maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
    "language_detection" => true
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    puts "Language confidence: #{transcription_result['language_confidence']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end
```

```php title="PHP" highlight={62} maxLines=15
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "language_detection" => true
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        echo $transcription_result['text'];
        echo $transcription_result['language_confidence'];
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

</CodeBlocks>

## Set a language confidence threshold

You can set the confidence threshold that must be reached if language detection is enabled. An error will be returned
if the language confidence is below this threshold. Valid values are in the range [0,1] inclusive.

<CodeBlocks>

```python title="Python SDK" highlight={8,13} maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(language_detection=True, language_confidence_threshold=0.8)

transcript = aai.Transcriber(config=config).transcribe(audio_file)

if transcript.status == "error":
  raise RuntimeError(f"Transcription failed: {transcript.error}")
```

```python title="Python" highlight={20} maxLines=15
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "language_detection": True,
    "language_confidence_threshold": 0.8
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(f"Transcript ID:", transcript_id)
    print(f"Text:", transcription_result['text'])
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

```

```javascript title="JavaScript SDK" highlight={13} maxLines=15
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  language_detection: true,
  language_confidence_threshold: 0.8,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  if (transcript.status === "error") {
    throw new Error(`Transcription failed: ${transcript.error}`);
  }

  console.log(transcript.text);
  console.log(transcript.language_confidence);
};

run();
```

```javascript title="JavaScript" highlight={20} maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  language_detection: true,
  language_confidence_threshold: 0.8,
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    console.log(transcriptionResult.language_confidence);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

```csharp title="C#" highlight={39} maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
    [JsonPropertyName("language_confidence")]
    public double? LanguageConfidence { get; set; }
}

async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
{
    using (var fileStream = File.OpenRead(filePath))
    using (var fileContent = new StreamContent(fileStream))
    {
        fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent))
        {
            response.EnsureSuccessStatusCode();
            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }
}

async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
{
    var data = new { audio_url = audioUrl, language_detection = true, language_confidence_threshold = 0.8 };
    var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

    using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
    {
        response.EnsureSuccessStatusCode();
        return await response.Content.ReadFromJsonAsync<Transcript>();
    }
}

async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
{
    var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

    while (true)
    {
        var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
        transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();
        switch (transcript.Status)
        {
            case "queued" or "processing":
                await Task.Delay(TimeSpan.FromSeconds(3));
                break;
            case "completed":
                return transcript;
            case "error":
                throw new Exception($"Transcription failed: {transcript.Error}");
            default:
                throw new Exception("This code should not be reachable.");
        }
    }
}

// Main execution
using (var httpClient = new HttpClient())
{
    httpClient.DefaultRequestHeaders.Authorization =
        new AuthenticationHeaderValue("<YOUR-API-KEY>");

    var uploadUrl = await UploadFileAsync("my-audio.mp3", httpClient);
    var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);
    transcript = await WaitForTranscriptToProcess(transcript, httpClient);

    Console.WriteLine(transcript.Text);
    Console.WriteLine(transcript.LanguageConfidence);
}
```

```ruby title="Ruby" highlight={24} maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
    "language_detection" => true,
    "language_confidence_threshold" => 0.8
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    puts "Language confidence: #{transcription_result['language_confidence']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end
```

```php title="PHP" highlight={31} maxLines=15
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "language_detection" => true,
    "language_confidence_threshold" => 0.8
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        echo $transcription_result['text'];
        echo $transcription_result['language_confidence'];
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

</CodeBlocks>

<Tip title="Fallback to a default language">
  For a workflow that resubmits a transcription request using a default language
  if the threshold is not reached, see [this
  cookbook](/docs/guides/automatic-language-detection-route-default-language).
</Tip>


---
title: Set Language Manually
---

If you already know the dominant language, you can use the `language_code` key to specify the language of the speech in your audio file.

<CodeBlocks>

```python title="Python SDK" highlight={8} maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(language_code="es")

transcript = aai.Transcriber(config=config).transcribe(audio_file)

if transcript.status == "error":
  raise RuntimeError(f"Transcription failed: {transcript.error}")

print(transcript.text)
```

```python title="Python" highlight={19} maxLines=15
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "language_code": "es"
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(f"Transcript ID:", transcript_id)
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

```

```javascript title="JavaScript SDK" highlight={12} maxLines=15
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  language_code: "es",
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  console.log(transcript.text);
};

run();
```

```javascript title="JavaScript" highlight={19} maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  language_code: "es",
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

```csharp title="C#" highlight={69} maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
}

class Program
{
    static void Main(string[] args)
    {
        MainAsync(args).GetAwaiter().GetResult();
    }

    static async Task MainAsync(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Add("authorization", "<YOUR-API-KEY>");

            var localFilePath = "audio.mp3";

            Console.WriteLine("Uploading file...");
            var uploadUrl = await UploadFileAsync(localFilePath, httpClient);

            Console.WriteLine("Creating transcript with speech_model...");
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);

            Console.WriteLine("Waiting for transcript...");
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            Console.WriteLine("Transcription completed!");
            Console.WriteLine("----------------------------------");
            Console.WriteLine(transcript.Text);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var content = new StreamContent(fileStream))
        {
            content.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", content);
            response.EnsureSuccessStatusCode();

            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new
        {
            audio_url = audioUrl,
            language_code = "es" 
        };

        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "processing":
                case "queued":
                    Console.WriteLine($"Status: {transcript.Status}... waiting...");
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("Unexpected transcript status.");
            }
        }
    }
}
```

```ruby title="Ruby" highlight={23} maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
    "language_code" => "es"
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end
```

```php title="PHP" highlight={30} maxLines=15
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "language_code" => "es"
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        echo $transcription_result['text'];
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

</CodeBlocks>

To see all supported languages and their codes, see [Supported languages](/docs/speech-to-text/pre-recorded-audio/supported-languages).

---
title: Speaker Diarization
description: Add speaker labels to your transcript
---

import { LanguageTable } from "../../../assets/components/LanguagesTable";

<Accordion title="Supported languages">
  <LanguageTable
    languages={[
      { name: "Global English", code: "en" },
      { name: "Australian English", code: "en_au" },
      { name: "British English", code: "en_uk" },
      { name: "US English", code: "en_us" },
      { name: "Spanish", code: "es" },
      { name: "French", code: "fr" },
      { name: "German", code: "de" },
      { name: "Italian", code: "it" },
      { name: "Portuguese", code: "pt" },
      { name: "Dutch", code: "nl" },
      { name: "Hindi", code: "hi" },
      { name: "Japanese", code: "ja" },
      { name: "Chinese", code: "zh" },
      { name: "Finnish", code: "fi" },
      { name: "Korean", code: "ko" },
      { name: "Polish", code: "pl" },
      { name: "Russian", code: "ru" },
      { name: "Turkish", code: "tr" },
      { name: "Ukrainian", code: "uk" },
      { name: "Vietnamese", code: "vi" },
    ]}
    columns={2}
  />
  <br />
</Accordion>

The Speaker Diarization model lets you detect multiple speakers in an audio file and what each speaker said.

If you enable Speaker Diarization, the resulting transcript will return a list of _utterances_, where each utterance corresponds to an uninterrupted segment of speech from a single speaker.

<Warning title="Speaker Diarization and Multichannel">

Speaker Diarization doesn't support multichannel transcription. Enabling both Speaker Diarization and [multichannel](/docs/speech-to-text/pre-recorded-audio/multichannel-transcription) will result in an error.

</Warning>

## Quickstart

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>

To enable Speaker Diarization, set `speaker_labels` to `True` in the transcription config.

```python {14,19-20} maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# You can use a local filepath:
# audio_file = "./example.mp3"

# Or use a publicly-accessible URL:
audio_file = (
    "https://assembly.ai/wildfires.mp3"
)

config = aai.TranscriptionConfig(
  speaker_labels=True,
)

transcript = aai.Transcriber().transcribe(audio_file, config)

for utterance in transcript.utterances:
  print(f"Speaker {utterance.speaker}: {utterance.text}")
```

</Tab>
<Tab language="python" title="Python">

To enable Speaker Diarization, set `speaker_labels` to `True` in the POST request body:

```python {19,41,42} maxLines=15
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "speaker_labels": True
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(f"Transcript ID:", transcript_id)
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

for utterance in transcription_result['utterances']:
  print(f"Speaker {utterance['speaker']}: {utterance['text']}")
```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">

To enable Speaker Diarization, set `speaker_labels` to `true` in the transcription config.

```javascript {15,21-23} maxLines=15
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// You can use a local filepath:
// const audioFile = "./example.mp3"

// Or use a publicly-accessible URL:
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  speaker_labels: true,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  for (const utterance of transcript.utterances!) {
    console.log(`Speaker ${utterance.speaker}: ${utterance.text}`);
  }
};

run();
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript highlight={21, 35-37} maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./audio/audio.mp3";
const audioData = await fs.readFile(path);

const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});

const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  speaker_labels: true,
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, { headers });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    for (const utterance of transcriptionResult.utterances) {
      console.log(`Speaker ${utterance.speaker}: ${utterance.text}`);
    }
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

</Tab>
<Tab language="csharp" title="C#">

```csharp highlight={73, 43-49} maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
    public Utterance[] Utterances { get; set; }
}

public class Utterance
{
    public string Speaker { get; set; }
    public string Text { get; set; }
}

class Program
{
    static void Main(string[] args)
    {
        MainAsync(args).GetAwaiter().GetResult();
    }

    static async Task MainAsync(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Add("authorization", "<YOUR-API-KEY>");

            var uploadUrl = await UploadFileAsync("audio.mp3", httpClient);
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            if (transcript.Utterances != null)
            {
                foreach (var utterance in transcript.Utterances)
                {
                    Console.WriteLine($"Speaker {utterance.Speaker}: {utterance.Text}");
                }
            }
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var content = new StreamContent(fileStream))
        {
            content.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", content);
            response.EnsureSuccessStatusCode();

            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new
        {
          audio_url = audioUrl,
          speaker_labels = true
        };

        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "queued":
                case "processing":
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("Unexpected transcript status.");
            }
        }
    }
}
```

</Tab>
<Tab language="ruby" title="Ruby">

```ruby highlight={23, 54-55} maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
'authorization' => '<YOUR_API_KEY>',
'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
"audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
"speaker_labels" => true
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
polling_http.use_ssl = true
polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
polling_response = polling_http.request(polling_request)

transcription_result = JSON.parse(polling_response.body)

if transcription_result['status'] == 'completed'
transcription_result['utterances'].each do |utterance|
puts "Speaker #{utterance['speaker']}: #{utterance['text']}"
end
break
elsif transcription_result['status'] == 'error'
raise "Transcription failed: #{transcription_result['error']}"
else
puts 'Waiting for transcription to complete...'
sleep(3)
end
end

```

</Tab>
<Tab language="php" title="PHP">

```php highlight={30, 61-63} maxLines=15
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "speaker_labels" => true
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        foreach ($transcription_result['utterances'] as $utterance) {
            echo "Speaker {$utterance['speaker']}: {$utterance['text']}\n";
        }
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

</Tab>
</Tabs>

## Set number of speakers expected

You can set the number of speakers expected in the audio file by setting the `speakers_expected` parameter.

Only use this parameter if you are certain about the number of speakers in the audio file.

### Example

```bash {6} maxLines=15
curl https://api.assemblyai.com/v2/transcript \
--header "Authorization: <YOUR_API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "audio_url": "YOUR_AUDIO_URL",
  "speaker_labels": true,
  "speakers_expected": 3
}'
```

## Set a range of possible speakers

You can set a range of possible speakers in the audio file by setting the `speaker_options` parameter. By default, the model will return between 1 and 10 speakers.

This parameter is suitable for use cases where there is a known minimum/maximum number of speakers in the audio file that is outside the bounds of the default value of 1 to 10 speakers.

<Warning>
  Setting `max_speakers_expected` higher than is necessary may hurt model
  accuracy.
</Warning>

### Example

```bash
curl https://api.assemblyai.com/v2/transcript \
--header "Authorization: <YOUR_API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "audio_url": "YOUR_AUDIO_URL",
  "speaker_labels": true,
  "speaker_options": {
    "min_speakers_expected": 3,
    "max_speakers_expected": 5
  }
}'
```

## API reference

### Request

```bash {6} maxLines=15
curl https://api.assemblyai.com/v2/transcript \
--header "Authorization: <YOUR_API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "audio_url": "YOUR_AUDIO_URL",
  "speaker_labels": true,
  "speakers_expected": 3
}'
```

| Key                                     | Type    | Description                                                |
| --------------------------------------- | ------- | ---------------------------------------------------------- |
| `speaker_labels`                        | boolean | Enable Speaker Diarization.                                |
| `speakers_expected`                     | number  | Set number of speakers.                                    |
| `speaker_options`                       | object  | Set range of possible speakers.                            |
| `speaker_options.min_speakers_expected` | number  | The minimum number of speakers expected in the audio file. |
| `speaker_options.max_speakers_expected` | number  | The maximum number of speakers expected in the audio file. |

### Response

<Json
  json={{
    utterances: [
      {
        confidence: 0.9359033333333334,
        end: 26950,
        speaker: "A",
        start: 250,
        text: "Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines from Maine to Maryland to Minnesota are gray and smoggy. And in some places, the air quality warnings include the warning to stay inside. We wanted to better understand what's happening here and why, so we called Peter de Carlo, an associate professor in the Department of Environmental Health and Engineering at Johns Hopkins University Varsity. Good morning, professor.",
        words: [
          {
            text: "Smoke",
            start: 250,
            end: 650,
            confidence: 0.97503,
            speaker: "A",
          },
          {
            text: "from",
            start: 730,
            end: 1022,
            confidence: 0.99999,
            speaker: "A",
          },
          {
            text: "hundreds",
            start: 1076,
            end: 1418,
            confidence: 0.99843,
            speaker: "A",
          },
          {
            text: "of",
            start: 1434,
            end: 1614,
            confidence: 0.85,
            speaker: "A",
          },
          {
            text: "wildfires",
            start: 1652,
            end: 2346,
            confidence: 0.89657,
            speaker: "A",
          },
          {
            text: "in",
            start: 2378,
            end: 2526,
            confidence: 0.99994,
            speaker: "A",
          },
          {
            text: "Canada",
            start: 2548,
            end: 3130,
            confidence: 0.93864,
            speaker: "A",
          },
          {
            text: "is",
            start: 3210,
            end: 3454,
            confidence: 0.999,
            speaker: "A",
          },
          {
            text: "triggering",
            start: 3492,
            end: 3946,
            confidence: 0.75366,
            speaker: "A",
          },
          {
            text: "air",
            start: 3978,
            end: 4174,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "quality",
            start: 4212,
            end: 4558,
            confidence: 0.87745,
            speaker: "A",
          },
          {
            text: "alerts",
            start: 4644,
            end: 5114,
            confidence: 0.94739,
            speaker: "A",
          },
          {
            text: "throughout",
            start: 5162,
            end: 5466,
            confidence: 0.99726,
            speaker: "A",
          },
          {
            text: "the",
            start: 5498,
            end: 5694,
            confidence: 0.79,
            speaker: "A",
          },
          {
            text: "US.",
            start: 5732,
            end: 6382,
            confidence: 0.88,
            speaker: "A",
          },
          {
            text: "Skylines",
            start: 6516,
            end: 7226,
            confidence: 0.94906,
            speaker: "A",
          },
          {
            text: "from",
            start: 7258,
            end: 7454,
            confidence: 0.99997,
            speaker: "A",
          },
          {
            text: "Maine",
            start: 7492,
            end: 7914,
            confidence: 0.46929,
            speaker: "A",
          },
          {
            text: "to",
            start: 7962,
            end: 8174,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "Maryland",
            start: 8212,
            end: 8634,
            confidence: 0.99855,
            speaker: "A",
          },
          {
            text: "to",
            start: 8682,
            end: 8894,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "Minnesota",
            start: 8932,
            end: 9578,
            confidence: 0.93339,
            speaker: "A",
          },
          {
            text: "are",
            start: 9674,
            end: 9934,
            confidence: 0.99978,
            speaker: "A",
          },
          {
            text: "gray",
            start: 9972,
            end: 10186,
            confidence: 0.68,
            speaker: "A",
          },
          {
            text: "and",
            start: 10218,
            end: 10414,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "smoggy.",
            start: 10452,
            end: 11050,
            confidence: 0.99223,
            speaker: "A",
          },
          {
            text: "And",
            start: 11130,
            end: 11326,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "in",
            start: 11348,
            end: 11534,
            confidence: 0.9947,
            speaker: "A",
          },
          {
            text: "some",
            start: 11572,
            end: 11774,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "places,",
            start: 11812,
            end: 12106,
            confidence: 0.99991,
            speaker: "A",
          },
          {
            text: "the",
            start: 12138,
            end: 12286,
            confidence: 0.9,
            speaker: "A",
          },
          {
            text: "air",
            start: 12308,
            end: 12494,
            confidence: 0.99995,
            speaker: "A",
          },
          {
            text: "quality",
            start: 12532,
            end: 12830,
            confidence: 0.9966,
            speaker: "A",
          },
          {
            text: "warnings",
            start: 12900,
            end: 13434,
            confidence: 0.99939,
            speaker: "A",
          },
          {
            text: "include",
            start: 13482,
            end: 13866,
            confidence: 0.99998,
            speaker: "A",
          },
          {
            text: "the",
            start: 13898,
            end: 13998,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "warning",
            start: 14004,
            end: 14234,
            confidence: 0.94734,
            speaker: "A",
          },
          {
            text: "to",
            start: 14282,
            end: 14542,
            confidence: 0.95,
            speaker: "A",
          },
          {
            text: "stay",
            start: 14596,
            end: 14910,
            confidence: 0.99999,
            speaker: "A",
          },
          {
            text: "inside.",
            start: 14980,
            end: 15646,
            confidence: 0.99997,
            speaker: "A",
          },
          {
            text: "We",
            start: 15828,
            end: 16126,
            confidence: 0.99997,
            speaker: "A",
          },
          {
            text: "wanted",
            start: 16148,
            end: 16334,
            confidence: 0.99817,
            speaker: "A",
          },
          {
            text: "to",
            start: 16372,
            end: 16526,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "better",
            start: 16548,
            end: 16734,
            confidence: 0.99999,
            speaker: "A",
          },
          {
            text: "understand",
            start: 16772,
            end: 17070,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "what's",
            start: 17140,
            end: 17466,
            confidence: 0.99955,
            speaker: "A",
          },
          {
            text: "happening",
            start: 17498,
            end: 17742,
            confidence: 0.99999,
            speaker: "A",
          },
          {
            text: "here",
            start: 17796,
            end: 17966,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "and",
            start: 17988,
            end: 18174,
            confidence: 0.54,
            speaker: "A",
          },
          {
            text: "why,",
            start: 18212,
            end: 18414,
            confidence: 0.99999,
            speaker: "A",
          },
          {
            text: "so",
            start: 18452,
            end: 18606,
            confidence: 0.99974,
            speaker: "A",
          },
          {
            text: "we",
            start: 18628,
            end: 18766,
            confidence: 0.98861,
            speaker: "A",
          },
          {
            text: "called",
            start: 18788,
            end: 18926,
            confidence: 0.99987,
            speaker: "A",
          },
          {
            text: "Peter",
            start: 18948,
            end: 19274,
            confidence: 0.99971,
            speaker: "A",
          },
          {
            text: "de",
            start: 19322,
            end: 19486,
            confidence: 0.59364,
            speaker: "A",
          },
          {
            text: "Carlo,",
            start: 19508,
            end: 19930,
            confidence: 0.73053,
            speaker: "A",
          },
          {
            text: "an",
            start: 20010,
            end: 20254,
            confidence: 0.97545,
            speaker: "A",
          },
          {
            text: "associate",
            start: 20292,
            end: 20746,
            confidence: 0.98965,
            speaker: "A",
          },
          {
            text: "professor",
            start: 20778,
            end: 21194,
            confidence: 0.99117,
            speaker: "A",
          },
          {
            text: "in",
            start: 21242,
            end: 21358,
            confidence: 0.99997,
            speaker: "A",
          },
          {
            text: "the",
            start: 21364,
            end: 21486,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "Department",
            start: 21508,
            end: 21834,
            confidence: 0.99998,
            speaker: "A",
          },
          {
            text: "of",
            start: 21882,
            end: 22046,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "Environmental",
            start: 22068,
            end: 22666,
            confidence: 0.53495,
            speaker: "A",
          },
          {
            text: "Health",
            start: 22698,
            end: 22942,
            confidence: 0.9976,
            speaker: "A",
          },
          {
            text: "and",
            start: 22996,
            end: 23214,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "Engineering",
            start: 23252,
            end: 23706,
            confidence: 0.99983,
            speaker: "A",
          },
          {
            text: "at",
            start: 23738,
            end: 23934,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "Johns",
            start: 23972,
            end: 24266,
            confidence: 0.9997,
            speaker: "A",
          },
          {
            text: "Hopkins",
            start: 24298,
            end: 24714,
            confidence: 0.99874,
            speaker: "A",
          },
          {
            text: "University",
            start: 24762,
            end: 25042,
            confidence: 0.93732,
            speaker: "A",
          },
          {
            text: "Varsity.",
            start: 25066,
            end: 25490,
            confidence: 0.47413,
            speaker: "A",
          },
          {
            text: "Good",
            start: 25570,
            end: 25766,
            confidence: 0.73108,
            speaker: "A",
          },
          {
            text: "morning,",
            start: 25788,
            end: 26022,
            confidence: 0.99997,
            speaker: "A",
          },
          {
            text: "professor.",
            start: 26076,
            end: 26950,
            confidence: 0.99999,
            speaker: "A",
          },
        ],
      },
      {
        confidence: 0.9929600000000001,
        end: 28840,
        speaker: "B",
        start: 27850,
        text: "Good morning.",
        words: [
          {
            text: "Good",
            start: 27850,
            end: 28214,
            confidence: 0.99999,
            speaker: "B",
          },
          {
            text: "morning.",
            start: 28252,
            end: 28840,
            confidence: 0.98593,
            speaker: "B",
          },
        ],
      },
      {
        confidence: 0.8863582608695653,
        end: 37400,
        speaker: "A",
        start: 29610,
        text: "What is it about the conditions right now that have caused this round of wildfires to affect so many people so far away?",
        words: [
          {
            text: "What",
            start: 29610,
            end: 29926,
            confidence: 0.9999,
            speaker: "A",
          },
          {
            text: "is",
            start: 29948,
            end: 30134,
            confidence: 0.88563,
            speaker: "A",
          },
          {
            text: "it",
            start: 30172,
            end: 30278,
            confidence: 0.99993,
            speaker: "A",
          },
          {
            text: "about",
            start: 30284,
            end: 30502,
            confidence: 0.61503,
            speaker: "A",
          },
          {
            text: "the",
            start: 30556,
            end: 30726,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "conditions",
            start: 30748,
            end: 31314,
            confidence: 0.77568,
            speaker: "A",
          },
          {
            text: "right",
            start: 31362,
            end: 31622,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "now",
            start: 31676,
            end: 32086,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "that",
            start: 32188,
            end: 32406,
            confidence: 0.99998,
            speaker: "A",
          },
          {
            text: "have",
            start: 32428,
            end: 32614,
            confidence: 0.76635,
            speaker: "A",
          },
          {
            text: "caused",
            start: 32652,
            end: 33026,
            confidence: 0.81005,
            speaker: "A",
          },
          {
            text: "this",
            start: 33058,
            end: 33254,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "round",
            start: 33292,
            end: 33506,
            confidence: 0.98965,
            speaker: "A",
          },
          {
            text: "of",
            start: 33538,
            end: 33782,
            confidence: 0.63,
            speaker: "A",
          },
          {
            text: "wildfires",
            start: 33836,
            end: 34546,
            confidence: 0.80401,
            speaker: "A",
          },
          {
            text: "to",
            start: 34578,
            end: 34726,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "affect",
            start: 34748,
            end: 35074,
            confidence: 0.9824,
            speaker: "A",
          },
          {
            text: "so",
            start: 35122,
            end: 35334,
            confidence: 0.66533,
            speaker: "A",
          },
          {
            text: "many",
            start: 35372,
            end: 35574,
            confidence: 0.99999,
            speaker: "A",
          },
          {
            text: "people",
            start: 35612,
            end: 36054,
            confidence: 0.5659,
            speaker: "A",
          },
          {
            text: "so",
            start: 36172,
            end: 36502,
            confidence: 0.9931,
            speaker: "A",
          },
          {
            text: "far",
            start: 36556,
            end: 36774,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "away?",
            start: 36812,
            end: 37400,
            confidence: 0.90331,
            speaker: "A",
          },
        ],
      },
      {
        confidence: 0.92182406779661,
        end: 56142,
        speaker: "B",
        start: 38970,
        text: "Well, there's a couple of things. The season has been pretty dry already. And then the fact that we're getting hit in the US. Is because there's a couple of weather systems that are essentially channeling the smoke from those Canadian wildfires through Pennsylvania into the Mid Atlantic and the Northeast and kind of just dropping the smoke there.",
        words: [
          {
            text: "Well,",
            start: 38970,
            end: 39334,
            confidence: 0.98456,
            speaker: "B",
          },
          {
            text: "there's",
            start: 39372,
            end: 39538,
            confidence: 0.99997,
            speaker: "B",
          },
          {
            text: "a",
            start: 39554,
            end: 39686,
            confidence: 0.97,
            speaker: "B",
          },
          {
            text: "couple",
            start: 39708,
            end: 39846,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "of",
            start: 39868,
            end: 40006,
            confidence: 0.99,
            speaker: "B",
          },
          {
            text: "things.",
            start: 40028,
            end: 40694,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "The",
            start: 40892,
            end: 41302,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "season",
            start: 41356,
            end: 41634,
            confidence: 0.99996,
            speaker: "B",
          },
          {
            text: "has",
            start: 41682,
            end: 41846,
            confidence: 0.99998,
            speaker: "B",
          },
          {
            text: "been",
            start: 41868,
            end: 42006,
            confidence: 0.99999,
            speaker: "B",
          },
          {
            text: "pretty",
            start: 42028,
            end: 42130,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "dry",
            start: 42140,
            end: 42374,
            confidence: 0.76633,
            speaker: "B",
          },
          {
            text: "already.",
            start: 42422,
            end: 42922,
            confidence: 0.99996,
            speaker: "B",
          },
          {
            text: "And",
            start: 43056,
            end: 43306,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "then",
            start: 43328,
            end: 43610,
            confidence: 0.56978,
            speaker: "B",
          },
          {
            text: "the",
            start: 43680,
            end: 43914,
            confidence: 0.6,
            speaker: "B",
          },
          {
            text: "fact",
            start: 43952,
            end: 44106,
            confidence: 0.99998,
            speaker: "B",
          },
          {
            text: "that",
            start: 44128,
            end: 44266,
            confidence: 0.99706,
            speaker: "B",
          },
          {
            text: "we're",
            start: 44288,
            end: 44486,
            confidence: 0.92122,
            speaker: "B",
          },
          {
            text: "getting",
            start: 44518,
            end: 44714,
            confidence: 0.99998,
            speaker: "B",
          },
          {
            text: "hit",
            start: 44752,
            end: 45002,
            confidence: 0.51955,
            speaker: "B",
          },
          {
            text: "in",
            start: 45056,
            end: 45178,
            confidence: 0.99999,
            speaker: "B",
          },
          {
            text: "the",
            start: 45184,
            end: 45306,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "US.",
            start: 45328,
            end: 45898,
            confidence: 0.87,
            speaker: "B",
          },
          {
            text: "Is",
            start: 46064,
            end: 46442,
            confidence: 0.55822,
            speaker: "B",
          },
          {
            text: "because",
            start: 46496,
            end: 46714,
            confidence: 0.99996,
            speaker: "B",
          },
          {
            text: "there's",
            start: 46752,
            end: 46998,
            confidence: 0.9991,
            speaker: "B",
          },
          {
            text: "a",
            start: 47014,
            end: 47098,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "couple",
            start: 47104,
            end: 47226,
            confidence: 0.99361,
            speaker: "B",
          },
          {
            text: "of",
            start: 47248,
            end: 47338,
            confidence: 0.86,
            speaker: "B",
          },
          {
            text: "weather",
            start: 47344,
            end: 47606,
            confidence: 0.99983,
            speaker: "B",
          },
          {
            text: "systems",
            start: 47638,
            end: 47958,
            confidence: 0.99999,
            speaker: "B",
          },
          {
            text: "that",
            start: 47974,
            end: 48106,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "are",
            start: 48128,
            end: 48218,
            confidence: 0.99996,
            speaker: "B",
          },
          {
            text: "essentially",
            start: 48224,
            end: 48502,
            confidence: 0.66886,
            speaker: "B",
          },
          {
            text: "channeling",
            start: 48566,
            end: 48998,
            confidence: 0.9995,
            speaker: "B",
          },
          {
            text: "the",
            start: 49014,
            end: 49146,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "smoke",
            start: 49168,
            end: 49398,
            confidence: 0.99997,
            speaker: "B",
          },
          {
            text: "from",
            start: 49414,
            end: 49546,
            confidence: 0.99937,
            speaker: "B",
          },
          {
            text: "those",
            start: 49568,
            end: 49706,
            confidence: 0.99991,
            speaker: "B",
          },
          {
            text: "Canadian",
            start: 49728,
            end: 50086,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "wildfires",
            start: 50118,
            end: 51062,
            confidence: 0.49043,
            speaker: "B",
          },
          {
            text: "through",
            start: 51206,
            end: 51610,
            confidence: 0.99999,
            speaker: "B",
          },
          {
            text: "Pennsylvania",
            start: 51680,
            end: 52326,
            confidence: 0.99493,
            speaker: "B",
          },
          {
            text: "into",
            start: 52358,
            end: 52506,
            confidence: 0.99998,
            speaker: "B",
          },
          {
            text: "the",
            start: 52528,
            end: 52618,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "Mid",
            start: 52624,
            end: 52758,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "Atlantic",
            start: 52784,
            end: 53178,
            confidence: 0.68127,
            speaker: "B",
          },
          {
            text: "and",
            start: 53194,
            end: 53278,
            confidence: 0.71,
            speaker: "B",
          },
          {
            text: "the",
            start: 53284,
            end: 53406,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "Northeast",
            start: 53428,
            end: 53866,
            confidence: 0.98714,
            speaker: "B",
          },
          {
            text: "and",
            start: 53898,
            end: 54190,
            confidence: 0.99,
            speaker: "B",
          },
          {
            text: "kind",
            start: 54260,
            end: 54398,
            confidence: 0.99994,
            speaker: "B",
          },
          {
            text: "of",
            start: 54404,
            end: 54574,
            confidence: 0.53,
            speaker: "B",
          },
          {
            text: "just",
            start: 54612,
            end: 54766,
            confidence: 0.99999,
            speaker: "B",
          },
          {
            text: "dropping",
            start: 54788,
            end: 55146,
            confidence: 0.74603,
            speaker: "B",
          },
          {
            text: "the",
            start: 55178,
            end: 55278,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "smoke",
            start: 55284,
            end: 55594,
            confidence: 0.99134,
            speaker: "B",
          },
          {
            text: "there.",
            start: 55642,
            end: 56142,
            confidence: 0.99999,
            speaker: "B",
          },
        ],
      },
      {
        confidence: 0.9296276470588234,
        end: 61070,
        speaker: "A",
        start: 56276,
        text: "So what is it in this haze that makes it harmful? And I'm assuming it is harmful.",
        words: [
          {
            text: "So",
            start: 56276,
            end: 56574,
            confidence: 0.57369,
            speaker: "A",
          },
          {
            text: "what",
            start: 56612,
            end: 56814,
            confidence: 0.98951,
            speaker: "A",
          },
          {
            text: "is",
            start: 56852,
            end: 56958,
            confidence: 0.99994,
            speaker: "A",
          },
          {
            text: "it",
            start: 56964,
            end: 57086,
            confidence: 0.98926,
            speaker: "A",
          },
          {
            text: "in",
            start: 57108,
            end: 57246,
            confidence: 0.86342,
            speaker: "A",
          },
          {
            text: "this",
            start: 57268,
            end: 57454,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "haze",
            start: 57492,
            end: 57866,
            confidence: 0.64143,
            speaker: "A",
          },
          {
            text: "that",
            start: 57898,
            end: 58094,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "makes",
            start: 58132,
            end: 58334,
            confidence: 0.99999,
            speaker: "A",
          },
          {
            text: "it",
            start: 58372,
            end: 58718,
            confidence: 0.88877,
            speaker: "A",
          },
          {
            text: "harmful?",
            start: 58804,
            end: 59274,
            confidence: 0.99998,
            speaker: "A",
          },
          {
            text: "And",
            start: 59322,
            end: 59438,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "I'm",
            start: 59444,
            end: 59626,
            confidence: 0.99865,
            speaker: "A",
          },
          {
            text: "assuming",
            start: 59658,
            end: 59978,
            confidence: 0.99992,
            speaker: "A",
          },
          {
            text: "it",
            start: 59994,
            end: 60078,
            confidence: 1.0,
            speaker: "A",
          },
          {
            text: "is",
            start: 60084,
            end: 60254,
            confidence: 0.88077,
            speaker: "A",
          },
          {
            text: "harmful.",
            start: 60292,
            end: 61070,
            confidence: 0.97834,
            speaker: "A",
          },
        ],
      },
      {
        confidence: 0.9549970909090907,
        end: 82950,
        speaker: "B",
        start: 62290,
        text: "It is. The levels outside right now in Baltimore are considered unhealthy. And most of that is due to what's called particulate matter, which are tiny particles, microscopic smaller than the width of your hair that can get into your lungs and impact your respiratory system, your cardiovascular system, and even your neurological your brain.",
        words: [
          {
            text: "It",
            start: 62290,
            end: 62606,
            confidence: 0.76898,
            speaker: "B",
          },
          {
            text: "is.",
            start: 62628,
            end: 63150,
            confidence: 0.99647,
            speaker: "B",
          },
          {
            text: "The",
            start: 63300,
            end: 63566,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "levels",
            start: 63588,
            end: 64042,
            confidence: 0.99769,
            speaker: "B",
          },
          {
            text: "outside",
            start: 64106,
            end: 64490,
            confidence: 0.99999,
            speaker: "B",
          },
          {
            text: "right",
            start: 64580,
            end: 64786,
            confidence: 0.99999,
            speaker: "B",
          },
          {
            text: "now",
            start: 64808,
            end: 64946,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "in",
            start: 64968,
            end: 65058,
            confidence: 0.99994,
            speaker: "B",
          },
          {
            text: "Baltimore",
            start: 65064,
            end: 65534,
            confidence: 0.56902,
            speaker: "B",
          },
          {
            text: "are",
            start: 65582,
            end: 65746,
            confidence: 0.99996,
            speaker: "B",
          },
          {
            text: "considered",
            start: 65768,
            end: 66046,
            confidence: 0.99992,
            speaker: "B",
          },
          {
            text: "unhealthy.",
            start: 66078,
            end: 67010,
            confidence: 0.92699,
            speaker: "B",
          },
          {
            text: "And",
            start: 67750,
            end: 68114,
            confidence: 0.96,
            speaker: "B",
          },
          {
            text: "most",
            start: 68152,
            end: 68306,
            confidence: 0.99998,
            speaker: "B",
          },
          {
            text: "of",
            start: 68328,
            end: 68466,
            confidence: 0.71,
            speaker: "B",
          },
          {
            text: "that",
            start: 68488,
            end: 68626,
            confidence: 0.99995,
            speaker: "B",
          },
          {
            text: "is",
            start: 68648,
            end: 68834,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "due",
            start: 68872,
            end: 69038,
            confidence: 0.99999,
            speaker: "B",
          },
          {
            text: "to",
            start: 69054,
            end: 69282,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "what's",
            start: 69336,
            end: 69566,
            confidence: 0.99943,
            speaker: "B",
          },
          {
            text: "called",
            start: 69598,
            end: 69794,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "particulate",
            start: 69832,
            end: 70238,
            confidence: 0.99076,
            speaker: "B",
          },
          {
            text: "matter,",
            start: 70254,
            end: 70482,
            confidence: 0.99998,
            speaker: "B",
          },
          {
            text: "which",
            start: 70536,
            end: 70706,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "are",
            start: 70728,
            end: 70866,
            confidence: 0.99999,
            speaker: "B",
          },
          {
            text: "tiny",
            start: 70888,
            end: 71166,
            confidence: 0.99994,
            speaker: "B",
          },
          {
            text: "particles,",
            start: 71198,
            end: 71902,
            confidence: 0.9992,
            speaker: "B",
          },
          {
            text: "microscopic",
            start: 72046,
            end: 72894,
            confidence: 0.63173,
            speaker: "B",
          },
          {
            text: "smaller",
            start: 72942,
            end: 73246,
            confidence: 0.99984,
            speaker: "B",
          },
          {
            text: "than",
            start: 73278,
            end: 73426,
            confidence: 0.99999,
            speaker: "B",
          },
          {
            text: "the",
            start: 73448,
            end: 73538,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "width",
            start: 73544,
            end: 73726,
            confidence: 0.99994,
            speaker: "B",
          },
          {
            text: "of",
            start: 73758,
            end: 74002,
            confidence: 0.97,
            speaker: "B",
          },
          {
            text: "your",
            start: 74056,
            end: 74226,
            confidence: 0.99999,
            speaker: "B",
          },
          {
            text: "hair",
            start: 74248,
            end: 75002,
            confidence: 0.85775,
            speaker: "B",
          },
          {
            text: "that",
            start: 75166,
            end: 75542,
            confidence: 0.9669,
            speaker: "B",
          },
          {
            text: "can",
            start: 75596,
            end: 75766,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "get",
            start: 75788,
            end: 75926,
            confidence: 0.99999,
            speaker: "B",
          },
          {
            text: "into",
            start: 75948,
            end: 76086,
            confidence: 0.99999,
            speaker: "B",
          },
          {
            text: "your",
            start: 76108,
            end: 76246,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "lungs",
            start: 76268,
            end: 76546,
            confidence: 0.86865,
            speaker: "B",
          },
          {
            text: "and",
            start: 76578,
            end: 76870,
            confidence: 0.99,
            speaker: "B",
          },
          {
            text: "impact",
            start: 76940,
            end: 77314,
            confidence: 0.99879,
            speaker: "B",
          },
          {
            text: "your",
            start: 77362,
            end: 77622,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "respiratory",
            start: 77676,
            end: 78226,
            confidence: 0.99164,
            speaker: "B",
          },
          {
            text: "system,",
            start: 78258,
            end: 78454,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "your",
            start: 78492,
            end: 78646,
            confidence: 0.99997,
            speaker: "B",
          },
          {
            text: "cardiovascular",
            start: 78668,
            end: 79522,
            confidence: 0.99909,
            speaker: "B",
          },
          {
            text: "system,",
            start: 79586,
            end: 80198,
            confidence: 0.99999,
            speaker: "B",
          },
          {
            text: "and",
            start: 80364,
            end: 80646,
            confidence: 1.0,
            speaker: "B",
          },
          {
            text: "even",
            start: 80668,
            end: 81046,
            confidence: 0.60772,
            speaker: "B",
          },
          {
            text: "your",
            start: 81148,
            end: 81414,
            confidence: 0.9985,
            speaker: "B",
          },
          {
            text: "neurological",
            start: 81452,
            end: 82034,
            confidence: 0.939,
            speaker: "B",
          },
          {
            text: "your",
            start: 82082,
            end: 82246,
            confidence: 0.78769,
            speaker: "B",
          },
          {
            text: "brain.",
            start: 82268,
            end: 82950,
            confidence: 0.99951,
            speaker: "B",
          },
        ],
      },
    ],
  }}
/>


| Key                                 | Type   | Description                                                                                                                                                |
| ----------------------------------- | ------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `utterances`                        | array  | A turn-by-turn temporal sequence of the transcript, where the i-th element is an object containing information about the i-th utterance in the audio file. |
| `utterances[i].confidence`          | number | The confidence score for the transcript of this utterance.                                                                                                 |
| `utterances[i].end`                 | number | The ending time, in milliseconds, of the utterance in the audio file.                                                                                      |
| `utterances[i].speaker`             | string | The speaker of this utterance, where each speaker is assigned a sequential capital letter. For example, "A" for Speaker A, "B" for Speaker B, and so on.   |
| `utterances[i].start`               | number | The starting time, in milliseconds, of the utterance in the audio file.                                                                                    |
| `utterances[i].text`                | string | The transcript for this utterance.                                                                                                                         |
| `utterances[i].words`               | array  | A sequential array for the words in the transcript, where the j-th element is an object containing information about the j-th word in the utterance.       |
| `utterances[i].words[j].text`       | string | The text of the j-th word in the i-th utterance.                                                                                                           |
| `utterances[i].words[j].start`      | number | The starting time for when the j-th word is spoken in the i-th utterance, in milliseconds.                                                                 |
| `utterances[i].words[j].end`        | number | The ending time for when the j-th word is spoken in the i-th utterance, in milliseconds.                                                                   |
| `utterances[i].words[j].confidence` | number | The confidence score for the transcript of the j-th word in the i-th utterance.                                                                            |
| `utterances[i].words[j].speaker`    | string | The speaker who uttered the j-th word in the i-th utterance.                                                                                               |

The response also includes the request parameters used to generate the transcript.

## Frequently asked questions & troubleshooting

<AccordionGroup>
  <Accordion title="How can I improve the performance of the Speaker Diarization model?" theme="dark" iconColor="white" >  
  To improve the performance of the Speaker Diarization model, it's recommended to ensure that each speaker speaks for at least 30 seconds uninterrupted. Avoiding scenarios where a person only speaks a few short phrases like “Yeah”, “Right”, or “Sounds good” can also help. If possible, avoiding cross-talking can also improve performance.
  </Accordion>

{" "}

<Accordion
  title="How many speakers can the model handle?"
  theme="dark"
  iconColor="white"
>
  By default, the upper limit on the number of speakers for Speaker Diarization is 10. If you expect more than 10 speakers, you can use [`speaker_options`](/docs/api-reference/transcripts/submit#request.body.speaker_options) to set a range of possible speakers.
  Please note, setting `max_speakers_expected` higher than necessary may hurt model accuracy.
</Accordion>

{" "}

<Accordion
  title="How accurate is the Speaker Diarization model?"
  theme="dark"
  iconColor="white"
>
  The accuracy of the Speaker Diarization model depends on several factors,
  including the quality of the audio, the number of speakers, and the length of
  the audio file. Ensuring that each speaker speaks for at least 30 seconds
  uninterrupted and avoiding scenarios where a person only speaks a few short
  phrases can improve accuracy. However, it's important to note that the model
  isn't perfect and may make mistakes, especially in more challenging scenarios.
</Accordion>

  <Accordion title="Why is the speaker diarization not performing as expected?" theme="dark" iconColor="white" >
  The speaker diarization may be performing poorly if a speaker only speaks once or infrequently throughout the audio file. Additionally, if the speaker speaks in short or single-word utterances, the model may struggle to create separate clusters for each speaker. Lastly, if the speakers sound similar, there may be difficulties in accurately identifying and separating them. Background noise, cross-talk, or an echo may also cause issues.
  </Accordion>
</AccordionGroup>


---
title: Multichannel Transcription
---

<Note title="Supported Languages">
  Multichannel transcription is supported for all languages
</Note>

If you have a multichannel audio file with multiple speakers, you can transcribe each of them separately.

The response includes an `audio_channels` property with the number of different channels, and an additional `utterances` property, containing a list of turn-by-turn utterances.

Each utterance contains channel information, starting at 1.

Additionally, each word in the `words` array contains the channel identifier.

<CodeBlocks>

```python title="Python SDK" highlight={8, 15-16} maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(multichannel=True)

transcript = aai.Transcriber(config=config).transcribe(audio_file)

if transcript.status == "error":
  raise RuntimeError(f"Transcription failed: {transcript.error}")

for utterance in transcript.utterances:
  print(f"Channel {utterance.speaker}: {utterance.text}")
```

```python title="Python" highlight={19,41-42} maxLines=15
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "multichannel": True
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(f"Transcript ID:", transcript_id)
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

for utterance in transcription_result['utterances']:
  print(f"Channel {utterance['speaker']}: {utterance['text']}")
```

```javascript title="JavaScript SDK" highlight={12,18-20} maxLines=15
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  multichannel: true,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  for (const utterance of transcript.utterances!) {
    console.log(`Channel ${utterance.speaker}: ${utterance.text}`);
  }
};

run();
```

```javascript title="JavaScript" highlight={19,35-37} maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  multichannel: true,
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    for (const utterance of transcriptionResult.utterances) {
      console.log(`Channel ${utterance.speaker}: ${utterance.text}`);
    }
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

```csharp title="C#" highlight={43-49,72} maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
    public Utterance[] Utterances { get; set; }
}

public class Utterance
{
    public string Speaker { get; set; }
    public string Text { get; set; }
}

class Program
{
    static void Main(string[] args)
    {
        MainAsync(args).GetAwaiter().GetResult();
    }

    static async Task MainAsync(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Add("authorization", "YOUR_API_KEY_HERE");

            var uploadUrl = await UploadFileAsync("my-audio.mp3", httpClient);
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            if (transcript.Utterances != null)
            {
                foreach (var utterance in transcript.Utterances)
                {
                    Console.WriteLine($"Channel {utterance.Speaker}: {utterance.Text}");
                }
            }
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var content = new StreamContent(fileStream))
        {
            content.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", content);
            response.EnsureSuccessStatusCode();

            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new { 
          audio_url = audioUrl, 
          multichannel = true 
        };
        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "queued":
                case "processing":
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("Unexpected transcript status.");
            }
        }
    }
}
```

```ruby title="Ruby" highlight={23,54-56} maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
    "multichannel" => true
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    transcription_result['utterances'].each do |utterance|
      puts "Channel #{utterance['speaker']}: #{utterance['text']}"
    end
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end
```

```php title="PHP" highlight={30, 61-63} maxLines=15
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "multichannel" => true
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        foreach ($transcription_result['utterances'] as $utterance) {
            echo "Speaker {$utterance['speaker']}: {$utterance['text']}\n";
        }
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

</CodeBlocks>

<Note>
  Multichannel audio increases the transcription time by approximately 25%.
</Note>


---
title: Export SRT or VTT Caption Files
---

import { LanguageTable } from "../../../assets/components/LanguagesTable";

<Accordion title="Supported languages">
  <LanguageTable
    languages={[
      { name: "Global English", code: "en" },
      { name: "Australian English", code: "en_au" },
      { name: "British English", code: "en_uk" },
      { name: "US English", code: "en_us" },
      { name: "Spanish", code: "es" },
      { name: "French", code: "fr" },
      { name: "German", code: "de" },
      { name: "Italian", code: "it" },
      { name: "Portuguese", code: "pt" },
      { name: "Dutch", code: "nl" },
      { name: "Hindi", code: "hi" },
      { name: "Finnish", code: "fi" },
      { name: "Polish", code: "pl" },
      { name: "Russian", code: "ru" },
      { name: "Ukrainian", code: "uk" },
      { name: "Vietnamese", code: "vi" },
      { name: "Albanian", code: "sq" },
      { name: "Amharic", code: "am" },
      { name: "Assamese", code: "as" },
      { name: "Bashkir", code: "ba" },
      { name: "Basque", code: "eu" },
      { name: "Belarusian", code: "be" },
      { name: "Bosnian", code: "bs" },
      { name: "Breton", code: "br" },
      { name: "Bulgarian", code: "bg" },
      { name: "Catalan", code: "ca" },
      { name: "Croatian", code: "hr" },
      { name: "Czech", code: "cs" },
      { name: "Danish", code: "da" },
      { name: "Estonian", code: "et" },
      { name: "Faroese", code: "fo" },
      { name: "Galician", code: "gl" },
      { name: "Greek", code: "el" },
      { name: "Gujarati", code: "gu" },
      { name: "Haitian", code: "ht" },
      { name: "Hausa", code: "ha" },
      { name: "Hawaiian", code: "haw" },
      { name: "Hebrew", code: "he" },
      { name: "Hungarian", code: "hu" },
      { name: "Icelandic", code: "is" },
      { name: "Indonesian", code: "id" },
      { name: "Javanese", code: "jw" },
      { name: "Kazakh", code: "kk" },
      { name: "Lao", code: "lo" },
      { name: "Latin", code: "la" },
      { name: "Latvian", code: "lv" },
      { name: "Lingala", code: "ln" },
      { name: "Lithuanian", code: "lt" },
      { name: "Luxembourgish", code: "lb" },
      { name: "Macedonian", code: "mk" },
      { name: "Malagasy", code: "mg" },
      { name: "Malay", code: "ms" },
      { name: "Maltese", code: "mt" },
      { name: "Maori", code: "mi" },
      { name: "Norwegian", code: "no" },
      { name: "Norwegian Nynorsk", code: "nn" },
      { name: "Occitan", code: "oc" },
      { name: "Pashto", code: "ps" },
      { name: "Romanian", code: "ro" },
      { name: "Sanskrit", code: "sa" },
      { name: "Serbian", code: "sr" },
      { name: "Shona", code: "sn" },
      { name: "Sindhi", code: "sd" },
      { name: "Slovak", code: "sk" },
      { name: "Slovenian", code: "sl" },
      { name: "Somali", code: "so" },
      { name: "Sundanese", code: "su" },
      { name: "Swahili", code: "sw" },
      { name: "Swedish", code: "sv" },
      { name: "Tagalog", code: "tl" },
      { name: "Tajik", code: "tg" },
      { name: "Tatar", code: "tt" },
      { name: "Tibetan", code: "bo" },
      { name: "Turkmen", code: "tk" },
      { name: "Uzbek", code: "uz" },
      { name: "Welsh", code: "cy" },
      { name: "Yiddish", code: "yi" },
      { name: "Yoruba", code: "yo" },
    ]}
    columns={3}
  />
  <br />
</Accordion>

You can export completed transcripts in SRT or VTT format, which can be used for subtitles and closed captions in videos.

You can also customize the maximum number of characters per caption by specifying the `chars_per_caption` parameter.

<CodeBlocks>

```python title="Python SDK" highlight={15-18} maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig()

transcript = aai.Transcriber(config=config).transcribe(audio_file)

if transcript.status == "error":
  raise RuntimeError(f"Transcription failed: {transcript.error}")

srt = transcript.export_subtitles_srt(
  # Optional: Customize the maximum number of characters per caption
  chars_per_caption=32
  )

with open(f"transcript_{transcript.id}.srt", "w") as srt_file:
  srt_file.write(srt)

# vtt = transcript.export_subtitles_vtt()

# with open(f"transcript_{transcript_id}.vtt", "w") as vtt_file:
#   vtt_file.write(vtt)

```

```python title="Python" highlight={41} maxLines=15
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url # You can also use a URL to an audio or video file on the web
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(f"Transcript ID:", transcript_id)
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

# chars_per_caption is optional
srt_response = requests.get(f"{polling_endpoint}/srt?chars_per_caption=32", headers=headers)

with open(f"transcript_{transcript_id}.srt", "w") as srt_file:
  srt_file.write(srt_response.text)

# vtt_response = requests.get(f"{polling_endpoint}/vtt", headers=headers)

# with open(f"transcript_{transcript_id}.vtt", "w") as vtt_file:
#   vtt_file.write(vtt_response.text)
```

```javascript title="JavaScript SDK" highlight={19} maxLines=15
import { AssemblyAI } from "assemblyai";
import fs from "fs";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  let srt = await client.transcripts.subtitles(transcript.id, "srt", 32);
  fs.writeFileSync(`transcript_${transcript.id}.srt`, srt);

  // let vtt = await client.transcripts.subtitles(transcript.id, 'vtt', 32)
  // fs.writeFileSync(`transcript_${transcript.id}.vtt`, vtt)
};

run();
```

```javascript title="JavaScript" highlight={44} maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}

const srtEndpoint = `${baseUrl}/v2/transcript/${transcriptId}/srt?chars_per_caption=32`;
const srtResponse = await axios.get(srtEndpoint, { headers });
const srt = srtResponse.data;

fs.writeFileSync(`transcript_${transcriptId}.srt`, srt);

// const vttEndpoint = `${baseUrl}/v2/transcript/${transcriptId}/vtt?chars_per_caption=32`
// const vttResponse = await axios.get(vttEndpoint, { headers })
// const vtt = vttResponse.data

// fs.writeFileSync(`transcript_${transcriptId}.vtt`, vtt)
```

```csharp title="C#" highlight={109-123} maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
}

class Program
{
    static void Main(string[] args)
    {
        MainAsync(args).GetAwaiter().GetResult();
    }

    static async Task MainAsync(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Add("authorization", "<YOUR-API-KEY> ");

            var localFilePath = "audio.mp3"; // Update your file name here

            Console.WriteLine("Uploading file...");
            var uploadUrl = await UploadFileAsync(localFilePath, httpClient);

            Console.WriteLine("Creating transcript...");
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);

            Console.WriteLine("Waiting for transcription to complete...");
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            Console.WriteLine("Fetching SRT subtitles...");
            var srt = await GetSrtAsync(transcript.Id, httpClient);
            File.WriteAllText($"transcript_{transcript.Id}.srt", srt);
            Console.WriteLine($"SRT file saved as transcript_{transcript.Id}.srt");

            // Optional: fetch and save VTT
            // Console.WriteLine("Fetching VTT subtitles...");
            // var vtt = await GetVttAsync(transcript.Id, httpClient);
            // File.WriteAllText($"transcript_{transcript.Id}.vtt", vtt);
            // Console.WriteLine($"VTT file saved as transcript_{transcript.Id}.vtt");
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var fileContent = new StreamContent(fileStream))
        {
            fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent);
            response.EnsureSuccessStatusCode();

            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new { audio_url = audioUrl };
        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "queued":
                case "processing":
                    Console.WriteLine($"Status: {transcript.Status}... waiting...");
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("Unexpected transcript status.");
            }
        }
    }

    static async Task<string> GetSrtAsync(string transcriptId, HttpClient httpClient, int charsPerCaption = 32)
    {
        var url = $"https://api.assemblyai.com/v2/transcript/{transcriptId}/srt?chars_per_caption={charsPerCaption}";
        var response = await httpClient.GetAsync(url);
        response.EnsureSuccessStatusCode();
        return await response.Content.ReadAsStringAsync();
    }

    static async Task<string> GetVttAsync(string transcriptId, HttpClient httpClient, int charsPerCaption = 32)
    {
        var url = $"https://api.assemblyai.com/v2/transcript/{transcriptId}/vtt?chars_per_caption={charsPerCaption}";
        var response = await httpClient.GetAsync(url);
        response.EnsureSuccessStatusCode();
        return await response.Content.ReadAsStringAsync();
    }
}
```

```ruby title="Ruby" highlight={66} maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url # You can also use a URL to an audio or video file on the web
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end

# chars_per_caption is optional
srt_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}/srt?chars_per_caption=32")

srt_request = Net::HTTP::Get.new(srt_endpoint.request_uri, headers)
srt_response = polling_http.request(srt_request)

srt = srt_response.body

File.write("transcript_#{transcript_id}.srt", srt)

# vtt_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}/vtt?chars_per_caption=32")

# vtt_request = Net::HTTP::Get.new(vtt_endpoint.request_uri, headers)
# vtt_response = polling_http.request(vtt_request)
# vtt = vtt_response.body

# File.write("transcript_#{transcript_id}.vtt", vtt)
```

```php title="PHP" highlight={75} maxLines=15
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url // You can also use a URL to an audio or video file on the web
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        echo $transcription_result['text'];
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}

$srt_endpoint = $base_url . "/v2/transcript/" . $transcript_id . "/srt?chars_per_caption=32";

$srt_request = curl_init($srt_endpoint);
curl_setopt($srt_request, CURLOPT_HTTPHEADER, $headers);
curl_setopt($srt_request, CURLOPT_RETURNTRANSFER, true);

$srt_response = curl_exec($srt_request);

file_put_contents("transcript_" . $transcript_id . ".srt", $srt_response);

/* $vtt_endpoint = $base_url . "/v2/transcript/" . $transcript_id . "/vtt?chars_per_caption=32";

$vtt_request = curl_init($vtt_endpoint);
curl_setopt($vtt_request, CURLOPT_HTTPHEADER, $headers);
curl_setopt($vtt_request, CURLOPT_RETURNTRANSFER, true);

$vtt_response = curl_exec($vtt_request);

file_put_contents("transcript_" . $transcript_id . ".vtt", $vtt_response); 
 */
```

</CodeBlocks>


---
title: Export Paragraphs and Sentences
---

import { LanguageTable } from "../../../assets/components/LanguagesTable";

<Accordion title="Supported languages">
  <LanguageTable
    languages={[
      { name: "Global English", code: "en" },
      { name: "Australian English", code: "en_au" },
      { name: "British English", code: "en_uk" },
      { name: "US English", code: "en_us" },
      { name: "Spanish", code: "es" },
      { name: "French", code: "fr" },
      { name: "German", code: "de" },
      { name: "Italian", code: "it" },
      { name: "Portuguese", code: "pt" },
      { name: "Dutch", code: "nl" },
      { name: "Hindi", code: "hi" },
      { name: "Finnish", code: "fi" },
      { name: "Polish", code: "pl" },
      { name: "Russian", code: "ru" },
      { name: "Ukrainian", code: "uk" },
      { name: "Vietnamese", code: "vi" },
      { name: "Albanian", code: "sq" },
      { name: "Amharic", code: "am" },
      { name: "Assamese", code: "as" },
      { name: "Bashkir", code: "ba" },
      { name: "Basque", code: "eu" },
      { name: "Belarusian", code: "be" },
      { name: "Bosnian", code: "bs" },
      { name: "Breton", code: "br" },
      { name: "Bulgarian", code: "bg" },
      { name: "Catalan", code: "ca" },
      { name: "Croatian", code: "hr" },
      { name: "Czech", code: "cs" },
      { name: "Danish", code: "da" },
      { name: "Estonian", code: "et" },
      { name: "Faroese", code: "fo" },
      { name: "Galician", code: "gl" },
      { name: "Greek", code: "el" },
      { name: "Gujarati", code: "gu" },
      { name: "Haitian", code: "ht" },
      { name: "Hausa", code: "ha" },
      { name: "Hawaiian", code: "haw" },
      { name: "Hebrew", code: "he" },
      { name: "Hungarian", code: "hu" },
      { name: "Icelandic", code: "is" },
      { name: "Indonesian", code: "id" },
      { name: "Javanese", code: "jw" },
      { name: "Kazakh", code: "kk" },
      { name: "Lao", code: "lo" },
      { name: "Latin", code: "la" },
      { name: "Latvian", code: "lv" },
      { name: "Lingala", code: "ln" },
      { name: "Lithuanian", code: "lt" },
      { name: "Luxembourgish", code: "lb" },
      { name: "Macedonian", code: "mk" },
      { name: "Malagasy", code: "mg" },
      { name: "Malay", code: "ms" },
      { name: "Maltese", code: "mt" },
      { name: "Maori", code: "mi" },
      { name: "Norwegian", code: "no" },
      { name: "Norwegian Nynorsk", code: "nn" },
      { name: "Occitan", code: "oc" },
      { name: "Pashto", code: "ps" },
      { name: "Romanian", code: "ro" },
      { name: "Sanskrit", code: "sa" },
      { name: "Serbian", code: "sr" },
      { name: "Shona", code: "sn" },
      { name: "Sindhi", code: "sd" },
      { name: "Slovak", code: "sk" },
      { name: "Slovenian", code: "sl" },
      { name: "Somali", code: "so" },
      { name: "Sundanese", code: "su" },
      { name: "Swahili", code: "sw" },
      { name: "Swedish", code: "sv" },
      { name: "Tagalog", code: "tl" },
      { name: "Tajik", code: "tg" },
      { name: "Tatar", code: "tt" },
      { name: "Tibetan", code: "bo" },
      { name: "Turkmen", code: "tk" },
      { name: "Uzbek", code: "uz" },
      { name: "Welsh", code: "cy" },
      { name: "Yiddish", code: "yi" },
      { name: "Yoruba", code: "yo" },
    ]}
    columns={3}
  />
  <br />
</Accordion>

You can retrieve transcripts that are automatically segmented into paragraphs. The text of the transcript is broken down by paragraphs, along with additional metadata.

## Export paragraphs

<CodeBlocks>

```python title="Python SDK" highlight={15-18} maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig()

transcript = aai.Transcriber(config=config).transcribe(audio_file)

if transcript.status == "error":
  raise RuntimeError(f"Transcription failed: {transcript.error}")

paragraphs = transcript.get_paragraphs()
for paragraph in paragraphs:
  print(paragraph.text)
  print()
```

```python title="Python" highlight={40-43} maxLines=15
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url # You can also use a URL to an audio or video file on the web
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(f"Transcript ID:", transcript_id)
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

paragraphs = requests.get(polling_endpoint + '/paragraphs', headers=headers).json()['paragraphs']
for paragraph in paragraphs:
  print(paragraph['text'])
  print()
```

```javascript title="JavaScript SDK" highlight={16-19} maxLines=15
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);
  const { paragraphs } = await client.transcripts.paragraphs(transcript.id);
  for (const paragraph of paragraphs) {
    console.log(paragraph.text);
  }
};

run();
```

```javascript title="JavaScript" highlight={43-51} maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}

const paragraphsEndpoint = `${baseUrl}/v2/transcript/${transcriptId}/paragraphs`;

const paragraphsResponse = await axios.get(paragraphsEndpoint, { headers });
const paragraphs = paragraphsResponse.data.paragraphs;

for (const paragraph of paragraphs) {
  console.log(paragraph.text);
  console.log();
}
```

```csharp title="C#" highlight={123-131} maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
}

public class Paragraph
{
    public string Text { get; set; }
    public int Start { get; set; }
    public int End { get; set; }
    public double Confidence { get; set; }
    public string Speaker { get; set; }
}

public class ParagraphsResponse
{
    public Paragraph[] Paragraphs { get; set; }
}

class Program
{
    static void Main(string[] args)
    {
        MainAsync(args).GetAwaiter().GetResult();
    }

    static async Task MainAsync(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Add("authorization", "<YOUR-API-KEY>");

            var localFilePath = "audio.mp3"; // replace with your audio file name

            Console.WriteLine("Uploading file...");
            var uploadUrl = await UploadFileAsync(localFilePath, httpClient);

            Console.WriteLine("Creating transcript...");
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);

            Console.WriteLine("Waiting for transcription...");
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            Console.WriteLine("Fetching paragraphs...");
            var paragraphs = await GetParagraphsAsync(transcript.Id, httpClient);

            Console.WriteLine();
            Console.WriteLine("=== Paragraphs ===");
            foreach (var paragraph in paragraphs)
            {
                Console.WriteLine(paragraph.Text);
                Console.WriteLine();
            }
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var fileContent = new StreamContent(fileStream))
        {
            fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent);
            response.EnsureSuccessStatusCode();

            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new { audio_url = audioUrl };
        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "queued":
                case "processing":
                    Console.WriteLine($"Status: {transcript.Status}... waiting...");
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("Unexpected transcript status.");
            }
        }
    }

    static async Task<Paragraph[]> GetParagraphsAsync(string transcriptId, HttpClient httpClient)
    {
        var url = $"https://api.assemblyai.com/v2/transcript/{transcriptId}/paragraphs";
        var response = await httpClient.GetAsync(url);
        response.EnsureSuccessStatusCode();

        var result = await response.Content.ReadFromJsonAsync<ParagraphsResponse>();
        return result.Paragraphs;
    }
}
```

```ruby title="Ruby" highlight={65-72} maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url # You can also use a URL to an audio or video file on the web
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end

paragraphs_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}/paragraphs")

paragraphs_request = Net::HTTP::Get.new(paragraphs_endpoint.request_uri, headers)
paragraphs_response = polling_http.request(paragraphs_request)

paragraphs = JSON.parse(paragraphs_response.body)['paragraphs']

paragraphs.each do |paragraph|
  puts paragraph['text']
  puts
end
```

```php title="PHP" highlight={75-80} maxLines=15
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url // You can also use a URL to an audio or video file on the web
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        echo $transcription_result['text'];
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}

$paragraphs_endpoint = $base_url . "/v2/transcript/" . $transcript_id . "/paragraphs";

$paragraphs_request = curl_init($paragraphs_endpoint);
curl_setopt($paragraphs_request, CURLOPT_HTTPHEADER, $headers);
curl_setopt($paragraphs_request, CURLOPT_RETURNTRANSFER, true);

$paragraphs_response = curl_exec($paragraphs_request);
$paragraphs = json_decode($paragraphs_response, true)['paragraphs'];

foreach ($paragraphs as $paragraph) {
    echo $paragraph['text'] . "\n";
}
```

</CodeBlocks>

## Export sentences

<CodeBlocks>

```python title="Python SDK" highlight={15-18} maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig()

transcript = aai.Transcriber(config=config).transcribe(audio_file)

if transcript.status == "error":
  raise RuntimeError(f"Transcription failed: {transcript.error}")

sentences = transcript.get_sentences()
for sentence in sentences:
  print(sentence.text)
  print()
```

```python title="Python" highlight={40-43} maxLines=15
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url # You can also use a URL to an audio or video file on the web
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(f"Transcript ID:", transcript_id)
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

sentences = requests.get(polling_endpoint + '/sentences', headers=headers).json()['sentences']
for sentence in sentences:
  print(sentence['text'])
  print()
```

```javascript title="JavaScript SDK" highlight={16-20} maxLines=15
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);
  const { sentences } = await client.transcripts.sentences(transcript.id);
  for (const sentence of sentences) {
    console.log(sentence.text);
  }
};

run();
```

```javascript title="JavaScript" highlight={45-51} maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}

const sentencesEndpoint = `${baseUrl}/v2/transcript/${transcriptId}/sentences`;

const sentencesResponse = await axios.get(sentencesEndpoint, { headers });
const sentences = sentencesResponse.data.sentences;

for (const sentence of sentences) {
  console.log(sentence.text);
  console.log();
}
```

```csharp title="C#" highlight={123-131} maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
}

public class Sentence
{
    public string Text { get; set; }
    public int Start { get; set; }
    public int End { get; set; }
    public double Confidence { get; set; }
    public string Speaker { get; set; }
}

public class SentencesResponse
{
    public Sentence[] Sentences { get; set; }
}

class Program
{
    static void Main(string[] args)
    {
        MainAsync(args).GetAwaiter().GetResult();
    }

    static async Task MainAsync(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Add("authorization", "<YOUR-API-KEY>");

            var localFilePath = "audio.mp3"; // replace if needed

            Console.WriteLine("Uploading file...");
            var uploadUrl = await UploadFileAsync(localFilePath, httpClient);

            Console.WriteLine("Creating transcript...");
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);

            Console.WriteLine("Waiting for transcription...");
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            Console.WriteLine("Fetching sentences...");
            var sentences = await GetSentencesAsync(transcript.Id, httpClient);

            Console.WriteLine();
            Console.WriteLine("=== Sentences ===");
            foreach (var sentence in sentences)
            {
                Console.WriteLine(sentence.Text);
                Console.WriteLine();
            }
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var fileContent = new StreamContent(fileStream))
        {
            fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent);
            response.EnsureSuccessStatusCode();

            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new { audio_url = audioUrl };
        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "queued":
                case "processing":
                    Console.WriteLine($"Status: {transcript.Status}... waiting...");
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("Unexpected transcript status.");
            }
        }
    }

    static async Task<Sentence[]> GetSentencesAsync(string transcriptId, HttpClient httpClient)
    {
        var url = $"https://api.assemblyai.com/v2/transcript/{transcriptId}/sentences";
        var response = await httpClient.GetAsync(url);
        response.EnsureSuccessStatusCode();

        var result = await response.Content.ReadFromJsonAsync<SentencesResponse>();
        return result.Sentences;
    }
}
```

```ruby title="Ruby" highlight={65-72} maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url # You can also use a URL to an audio or video file on the web
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end

sentences_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}/sentences")

sentences_request = Net::HTTP::Get.new(sentences_endpoint.request_uri, headers)
sentences_response = polling_http.request(sentences_request)

sentences = JSON.parse(sentences_response.body)['sentences']

sentences.each do |sentence|
  puts sentence['text']
  puts
end
```

```php title="PHP" highlight={75-80} maxLines=15
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url // You can also use a URL to an audio or video file on the web
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        echo $transcription_result['text'];
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}

$sentences_endpoint = $base_url . "/v2/transcript/" . $transcript_id . "/sentences";

$sentences_request = curl_init($sentences_endpoint);
curl_setopt($sentences_request, CURLOPT_HTTPHEADER, $headers);
curl_setopt($sentences_request, CURLOPT_RETURNTRANSFER, true);

$sentences_response = curl_exec($sentences_request);
$sentences = json_decode($sentences_response, true)['sentences'];

foreach ($sentences as $sentence) {
    echo $sentence['text'] . "\n";
}
```

</CodeBlocks>

You can retrieve transcripts that are automatically segmented into sentences, for a more reader-friendly experience. The text of the transcript is broken down by sentences, along with additional metadata.

The response is an array of objects, each representing a sentence or a paragraph in the transcript. See the [API reference](https://www.assemblyai.com/docs/api-reference/transcripts/get-sentences) for more info.


---
title: Word-Level Timestamps
---

The response also includes an array with information about each word:

<CodeBlocks>

```python title="Python SDK" highlight={13} maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig()

transcript = aai.Transcriber().transcribe(audio_file, config)

for word in transcript.words:
  print(f"Word: {word.text}, Start: {word.start}, End: {word.end}, Confidence: {word.confidence}")
```

```python title="Python" highlight={31-32} maxLines=15
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url # You can also use a URL to an audio or video file on the web
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    for word in transcription_result['words']:
      print(f"Word: {word['word']}, Start: {word['start']}, End: {word['end']}, Confidence: {word['confidence']}")
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

```

```javascript title="JavaScript SDK" highlight={22} maxLines=15
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  console.log(transcript.text);

  // Print word-level details
  for (const word of transcript.words) {
    console.log(
      `Word: ${word.text}, Start: ${word.start}, End: ${word.end}, Confidence: ${word.confidence}`
    );
  }
};

run();
```

```javascript title="JavaScript" highlight={36-39} maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    // Print word-level details
    for (const word of transcriptionResult.words) {
      console.log(
        `Word: ${word.text}, Start: ${word.start}, End: ${word.end}, Confidence: ${word.confidence}`
      );
    }
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

```csharp title="C#" highlight={61-65} maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public Word[] Words { get; set; }

    [JsonPropertyName("language_code")]
    public string LanguageCode { get; set; }

    public string Error { get; set; }
}

public class Word
{
    public string Text { get; set; }
    public int Start { get; set; }
    public int End { get; set; }
    public float Confidence { get; set; }
}

class Program
{
    static void Main(string[] args)
    {
        MainAsync(args).GetAwaiter().GetResult();
    }

    static async Task MainAsync(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Add("authorization", "<YOUR-API-KEY>");

            var localFilePath = "audio.mp3"; // Update your audio file name

            Console.WriteLine("Uploading file...");
            var uploadUrl = await UploadFileAsync(localFilePath, httpClient);

            Console.WriteLine("Creating transcript...");
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);

            Console.WriteLine("Waiting for transcript...");
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            Console.WriteLine();
            Console.WriteLine("=== Transcript Text ===");
            Console.WriteLine(transcript.Text);
            Console.WriteLine();

            Console.WriteLine("=== Word-level details ===");
            foreach (var word in transcript.Words)
            {
                Console.WriteLine($"Word: {word.Text}, Start: {word.Start}, End: {word.End}, Confidence: {word.Confidence}");
            }
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var fileContent = new StreamContent(fileStream))
        {
            fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent);
            response.EnsureSuccessStatusCode();

            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new { audio_url = audioUrl };
        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "queued":
                case "processing":
                    Console.WriteLine($"Status: {transcript.Status}... waiting...");
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("Unexpected transcript status.");
            }
        }
    }
}
```

```ruby title="Ruby" highlight={55-56} maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url # You can also use a URL to an audio or video file on the web
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    # Print word-level details
    transcription_result['words'].each do |word|
      puts "Word: #{word['text']}, Start: #{word['start']}, End: #{word['end']}, Confidence: #{word['confidence']}"
    end
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end
```

```php title="PHP" highlight={62-65} maxLines=15
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url // You can also use a URL to an audio or video file on the web
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        echo $transcription_result['text'];
        // Print word-level details
        foreach ($transcription_result['words'] as $word) {
            echo sprintf("Word: %s, Start: %d, End: %d, Confidence: %f\n",
                $word['text'], $word['start'], $word['end'], $word['confidence']);
        }
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

</CodeBlocks>

## API Reference

<Json
  json={{
    words: [
      {
        text: "Smoke",
        start: 240,
        end: 640,
        confidence: 0.70473,
        speaker: null,
      },
      {
        text: "from",
        start: 680,
        end: 968,
        confidence: 0.99967,
        speaker: null,
      },
      {
        text: "hundreds",
        start: 1024,
        end: 1416,
        confidence: 0.99795,
        speaker: null,
      },
      {
        text: "of",
        start: 1448,
        end: 1592,
        confidence: 0.99926,
        speaker: null,
      },
      {
        text: "wildfires",
        start: 1616,
        end: 2248,
        confidence: 0.99838,
        speaker: null,
      },
      {
        text: "in",
        start: 2264,
        end: 2440,
        confidence: 0.99782,
        speaker: null,
      },
      {
        text: "Canada",
        start: 2480,
        end: 2968,
        confidence: 0.99977,
        speaker: null,
      },
      {
        text: "is",
        start: 3104,
        end: 3400,
        confidence: 0.99652,
        speaker: null,
      },
      {
        text: "triggering",
        start: 3440,
        end: 3864,
        confidence: 0.99949,
        speaker: null,
      },
      {
        text: "air",
        start: 3912,
        end: 4120,
        confidence: 0.99988,
        speaker: null,
      },
      {
        text: "quality",
        start: 4160,
        end: 4584,
        confidence: 0.56204,
        speaker: null,
      },
      {
        text: "alerts",
        start: 4632,
        end: 5064,
        confidence: 0.70445,
        speaker: null,
      },
      {
        text: "throughout",
        start: 5112,
        end: 5416,
        confidence: 0.99892,
        speaker: null,
      },
      {
        text: "the",
        start: 5448,
        end: 5640,
        confidence: 0.99965,
        speaker: null,
      },
      {
        text: "US",
        start: 5680,
        end: 6264,
        confidence: 0.97691,
        speaker: null,
      },
      {
        text: "Skylines",
        start: 6432,
        end: 7208,
        confidence: 0.99802,
        speaker: null,
      },
      {
        text: "from",
        start: 7224,
        end: 7400,
        confidence: 0.99862,
        speaker: null,
      },
      {
        text: "Maine",
        start: 7440,
        end: 7928,
        confidence: 0.70316,
        speaker: null,
      },
      {
        text: "to",
        start: 8024,
        end: 8232,
        confidence: 0.99752,
        speaker: null,
      },
      {
        text: "Maryland",
        start: 8256,
        end: 8632,
        confidence: 0.99361,
        speaker: null,
      },
      {
        text: "to",
        start: 8696,
        end: 8872,
        confidence: 0.99604,
        speaker: null,
      },
      {
        text: "Minnesota",
        start: 8896,
        end: 9592,
        confidence: 0.99908,
        speaker: null,
      },
      {
        text: "are",
        start: 9656,
        end: 9880,
        confidence: 0.99898,
        speaker: null,
      },
      {
        text: "gray",
        start: 9920,
        end: 10136,
        confidence: 0.50474,
        speaker: null,
      },
      {
        text: "and",
        start: 10168,
        end: 10360,
        confidence: 0.99928,
        speaker: null,
      },
      {
        text: "smoggy.",
        start: 10400,
        end: 11000,
        confidence: 0.99366,
        speaker: null,
      },
    ],
  }}
/>


---
title: Word Search
---

<Note title="Supported languages">
  Word search is supported for all languages.
</Note>

You can search through a completed transcript for a specific set of keywords, which is useful for quickly finding relevant information.

The parameter can be a list of words, numbers, or phrases up to five words.

<CodeBlocks>

```python title="Python SDK" highlight={16,18,20-21} maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig()

transcript = aai.Transcriber(config=config).transcribe(audio_file)

if transcript.status == "error":
  raise RuntimeError(f"Transcription failed: {transcript.error}")

# Set the words you want to search for
words = ["foo", "bar", "foo bar", "42"]

matches = transcript.word_search(words)

for match in matches:
  print(f"Found '{match.text}' {match.count} times in the transcript")
```

```python title="Python" highlight={40,42,44,46-47} maxLines=15
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url # You can also use a URL to an audio or video file on the web
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(f"Transcript ID:", transcript_id)
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

words = ['foo', 'bar', 'foo bar', '42']

word_search_endpoint = f"{base_url}/v2/transcript/{transcript_id}/word-search?words={','.join(words)}"

response_json = requests.get(word_search_endpoint, headers=headers).json()

for match in response_json['matches']:
  print(f"Found '{match['text']}' {match['count']} times in the transcript")
```

```javascript title="JavaScript SDK" highlight={18,20,22-24} maxLines=15
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  // Set the words you want to search for.
  const words = ["foo", "bar", "foo bar", "42"];

  const { matches } = await client.transcripts.wordSearch(transcript.id, words);

  for (const match of matches) {
    console.log(`Found '${match.text}' ${match.count} times in the transcript`);
  }
};

run();
```

```javascript title="JavaScript" highlight={43,45-48,50-52} maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
};

const url = `${baseUrl}/v2/transcript`;
const transcript = await axios.post(url, data, { headers: headers });

const transcriptId = transcript.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}

const words = ["foo", "bar", "foo bar", "42"];

const response = await axios.get(
  `${baseUrl}/v2/transcript/${transcriptId}/word-search?words=${words.join(",")}`,
  { headers }
);

for (const match of response.data.matches) {
  console.log(`Found '${match.text}' ${match.count} times in the transcript`);
}
```

```csharp title="C#" highlight={112-118} maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
}

public class WordSearchMatch
{
    public string Text { get; set; }
    public int Count { get; set; }
    public int[][] Timestamps { get; set; }
    public int[] Indexes { get; set; }
}

public class WordSearchResponse
{
    public string Id { get; set; }
    public int TotalCount { get; set; }
    public WordSearchMatch[] Matches { get; set; }
}

class Program
{
    static void Main(string[] args)
    {
        MainAsync(args).GetAwaiter().GetResult();
    }

    static async Task MainAsync(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Add("authorization", "<YOUR-API-KEY>");

            var uploadUrl = await UploadFileAsync("audio.mp3", httpClient);
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            var wordSearchResponse = await WordSearchAsync(transcript.Id, new[] { "foo", "bar", "foo bar", "42" }, httpClient);

            foreach (var match in wordSearchResponse.Matches)
            {
                Console.WriteLine($"Found '{match.Text}' {match.Count} times in the transcript");
            }
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var fileContent = new StreamContent(fileStream))
        {
            fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent);
            response.EnsureSuccessStatusCode();

            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new { audio_url = audioUrl };
        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "queued":
                case "processing":
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("Unexpected transcript status.");
            }
        }
    }

    static async Task<WordSearchResponse> WordSearchAsync(string transcriptId, string[] words, HttpClient httpClient)
    {
        var wordSearchEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcriptId}/word-search?words={string.Join(',', words)}";
        var response = await httpClient.GetAsync(wordSearchEndpoint);
        response.EnsureSuccessStatusCode();
        return await response.Content.ReadFromJsonAsync<WordSearchResponse>();
    }
}
```

```ruby title="Ruby" highlight={63,65,73-75} maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url # You can also use a URL to an audio or video file on the web
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end

words = ['foo', 'bar', 'foo bar', '42']

uri = URI.parse("#{base_url}/v2/transcript/#{transcript_id}/word-search?words=#{words.join(',')}")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Get.new(uri.request_uri, headers)
response = http.request(request)
response_body = JSON.parse(response.body)

response_body['matches'].each do |match|
  puts "Found '#{match['text']}' #{match['count']} times in the transcript"
end

```

```php title="PHP" highlight={83-85,87,102-104} maxLines=15
<?php

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./audio.mp3";
if (!file_exists($path)) {
    throw new Exception("Audio file not found at $path");
}

// Upload the audio file
$ch = curl_init();
curl_setopt($ch, CURLOPT_URL, "$base_url/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
if (curl_errno($ch)) {
    throw new Exception("Upload cURL error: " . curl_error($ch));
}

$response_data = json_decode($response, true);
if (!isset($response_data['upload_url'])) {
    throw new Exception("Upload failed: " . $response);
}
$upload_url = $response_data["upload_url"];
curl_close($ch);

// Start transcription
$data = array("audio_url" => $upload_url);
$ch = curl_init("$base_url/v2/transcript");
curl_setopt($ch, CURLOPT_POST, true);
curl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($ch);
if (curl_errno($ch)) {
    throw new Exception("Transcription start cURL error: " . curl_error($ch));
}

$response_data = json_decode($response, true);
if (!isset($response_data['id'])) {
    throw new Exception("Failed to create transcription: " . $response);
}

$transcript_id = $response_data['id'];
echo "Transcript ID: $transcript_id\n";
curl_close($ch);

// Poll for completion
$polling_endpoint = "$base_url/v2/transcript/$transcript_id";
while (true) {
    $ch = curl_init($polling_endpoint);
    curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

    $polling_response = curl_exec($ch);
    if (curl_errno($ch)) {
        throw new Exception("Polling cURL error: " . curl_error($ch));
    }

    $transcription_result = json_decode($polling_response, true);
    curl_close($ch);

    if ($transcription_result['status'] === "completed") {
        echo "Transcription complete:\n" . $transcription_result['text'] . "\n";
        break;
    } elseif ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3); // Wait and poll again
    }
}

// Optional: Word search
$words = ['foo', 'bar', 'foo bar', '42'];
$encoded_words = array_map('urlencode', $words);
$word_query = implode(',', $encoded_words);

$ch = curl_init("$base_url/v2/transcript/$transcript_id/word-search?words=$word_query");
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($ch);
if (curl_errno($ch)) {
    throw new Exception("Word search cURL error: " . curl_error($ch));
}

$response_data = json_decode($response, true);
curl_close($ch);

if (!isset($response_data['matches'])) {
    echo "No matches found or error in word search response:\n" . json_encode($response_data) . "\n";
} else {
    foreach ($response_data['matches'] as $match) {
        echo "Found '" . $match['text'] . "' " . $match['count'] . " times in the transcript\n";
    }
}
```

</CodeBlocks>


---
title: Automatic Punctuation and Casing
---

<Note title="Supported languages">
  Automatic Punctuation and Casing is supported for all languages
</Note>

By default, the API automatically punctuates the transcription text and formats proper nouns, as well as converts numbers to their numerical form.

You can disable this behavior by setting `punctuate` and `format_text` to `False`.

<Info title="Disabling punctuation and casing">
  Disabling punctuation and casing is most effective when used for English, Spanish, and German. Exact results of disabling these default features can vary by language.
</Info>

<Tabs>
  <Tab language="python-sdk" title="Python SDK" default>

```python highlight={9} maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"

audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(punctuate=False, format_text=False)

transcript = aai.Transcriber(config=config).transcribe(audio_file)

if transcript.status == "error":
raise RuntimeError(f"Transcription failed: {transcript.error}")

print(transcript.text)

```

  </Tab>
  <Tab language="python" title="Python">

```python highlight={19-20} maxLines=15
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "punctuate": False,
    "format_text": False
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(f"Transcript ID:", transcript_id)
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

```

  </Tab>
  <Tab language="javascript-sdk" title="JavaScript SDK" default>

```javascript highlight={12-13} maxLines=15
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  punctuate: false,
  format_text: false,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);
  console.log(transcript.text);
};

run();
```

  </Tab>
  <Tab language="javascript" title="JavaScript">

```javascript highlight={19-20} maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  punctuate: false,
  format_text: false,
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp highlight={69-70} maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
}

class Program
{
    static void Main(string[] args)
    {
        MainAsync(args).GetAwaiter().GetResult();
    }

    static async Task MainAsync(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Add("authorization", "<YOUR-API-KEY>");

            var localFilePath = "audio.mp3";

            Console.WriteLine("Uploading file...");
            var uploadUrl = await UploadFileAsync(localFilePath, httpClient);

            Console.WriteLine("Creating transcript with speech_model...");
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);

            Console.WriteLine("Waiting for transcript...");
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            Console.WriteLine("Transcription completed!");
            Console.WriteLine("----------------------------------");
            Console.WriteLine(transcript.Text);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var content = new StreamContent(fileStream))
        {
            content.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", content);
            response.EnsureSuccessStatusCode();

            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new
        {
            audio_url = audioUrl,
            punctuate = false, 
            format_text = false 
        };

        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "processing":
                case "queued":
                    Console.WriteLine($"Status: {transcript.Status}... waiting...");
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("Unexpected transcript status.");
            }
        }
    }
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby highlight={23-24} maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
    "punctuate" => false,
    "format_text" => false
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end
```

  </Tab>
  <Tab language="php" title="PHP">

```php highlight={30-31} maxLines=15
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
"authorization: <YOUR_API_KEY>",
"content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
"audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
"punctuate" => false,
"format_text" => false
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
$polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        echo $transcription_result['text'];
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }

}

```

  </Tab>
</Tabs>


---
title: Custom Spelling
---

<Note title="Supported languages">
  Custom Spelling is supported for all languages
</Note>

Custom Spelling lets you customize how words are spelled or formatted in the transcript.

<Tabs>
  <Tab language="python-sdk" title="Python SDK" default>
To use Custom Spelling, pass a dictionary to `set_custom_spelling()` on the transcription config. Each key-value pair specifies a mapping from a word or phrase to a new spelling or format of a word. The key specifies the new spelling or format, and the corresponding value is the word or phrase you want to replace.

```python highlight={9-14} maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig()
config.set_custom_spelling(
  {
    "Gettleman": ["gettleman"],
    "SQL": ["Sequel"],
  }
)

transcript = aai.Transcriber(config=config).transcribe(audio_file)

if transcript.status == "error":
  raise RuntimeError(f"Transcription failed: {transcript.error}")

print(transcript.text)
```

  </Tab>
  <Tab language="python" title="Python">

To use Custom Spelling, include `custom_spelling` in your transcription parameters. The parameter should be a list of dictionaries, with each dictionary specifying a mapping from a word or phrase to a new spelling or format of a word.

```python highlight={19-28} maxLines=15
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "custom_spelling": [
      {
        "from": ["Decarlo"],
        "to": "DeCarlo"
      },
      {
        "from": ["SQL"],
        "to": "Sequel"
      }
    ]
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(f"Transcript ID:", transcript_id)
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

```

  </Tab>
  <Tab language="javascript-sdk" title="JavaScript SDK" default>

To use Custom Spelling, include `custom_spelling` in your transcription parameters. The parameter should be an array of objects, with each object specifying a mapping from a word or phrase to a new spelling or format of a word.

```javascript highlight={12-21} maxLines=15
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  custom_spelling: [
    {
      from: ["Decarlo"],
      to: "DeCarlo",
    },
    {
      from: ["Sequel"],
      to: "SQL",
    },
  ],
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  console.log(transcript.text);
};

run();
```

  </Tab>
  <Tab language="javascript" title="JavaScript">

To use Custom Spelling, include `custom_spelling` in your transcription parameters. The parameter should be an array of objects, with each object specifying a mapping from a word or phrase to a new spelling or format of a word.

```javascript highlight={21-30} maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);

const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});

const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  custom_spelling: [
    {
      from: ["Decarlo"],
      to: "DeCarlo",
    },
    {
      from: ["SQL"],
      to: "Sequel",
    },
  ],
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, { headers });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

  </Tab>
  <Tab language="csharp" title="C#">

To use Custom Spelling, include `custom_spelling` in your transcription request. The parameter should be an array of objects, with each object specifying a mapping from a word or phrase to a new spelling or format of a word.

```csharp highlight={60-64} maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
}

class Program
{
    static void Main(string[] args)
    {
        MainAsync(args).GetAwaiter().GetResult();
    }

    static async Task MainAsync(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Add("authorization", "<YOUR-API-KEY>");

            var uploadUrl = await UploadFileAsync("audio.mp3", httpClient);
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            Console.WriteLine(transcript.Text);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var fileContent = new StreamContent(fileStream))
        {
            fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent);
            response.EnsureSuccessStatusCode();

            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new
        {
            audio_url = audioUrl,
            custom_spelling = new[]
            {
                new { from = new[] { "Decarlo" }, to = "DeCarlo" },
                new { from = new[] { "Sequel" }, to = "SQL" }
            }
        };

        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "queued":
                case "processing":
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("Unexpected transcript status.");
            }
        }
    }
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">

To use Custom Spelling, include `custom_spelling` in your transcription parameters. The parameter should be an array of hashes, with each hash specifying a mapping from a word or phrase to a new spelling or format of a word.

```ruby highlight={23-32} maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
'authorization' => '<YOUR_API_KEY>',
'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
"audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
"custom_spelling" => [
{
"from" => ["Decarlo"],
"to" => "DeCarlo"
},
{
"from" => ["Sequel"],
"to" => "SQL"
}
]
}
uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
polling_http.use_ssl = true
polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
polling_response = polling_http.request(polling_request)

transcription_result = JSON.parse(polling_response.body)

if transcription_result['status'] == 'completed'
puts "Transcription text: #{transcription_result['text']}"
break
elsif transcription_result['status'] == 'error'
raise "Transcription failed: #{transcription_result['error']}"
else
puts 'Waiting for transcription to complete...'
sleep(3)
end
end

```

  </Tab>
  <Tab language="php" title="PHP">

To use Custom Spelling, include `custom_spelling` in your transcription parameters. The parameter should be an array of arrays, with each inner array specifying a mapping from a word or phrase to a new spelling or format of a word.

```php highlight={30-39} maxLines=15
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "custom_spelling" => array(
      array(
        "from" => array("Decarlo"),
        "to" => "DeCarlo"
      ),
      array(
        "from" => array("Sequel"),
        "to" => "SQL"
      )
    )
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        echo $transcription_result['text'];
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

  </Tab>
</Tabs>

<Note>

The value in the `to` key is case-sensitive, but the value in the `from` key isn't. Additionally, the `to` key must only contain one word, while the `from` key can contain multiple words.

</Note>


---
title: Filler Words
---

import { LanguageTable } from "../../../assets/components/LanguagesTable";

<Accordion title="Supported languages">
  <LanguageTable
    languages={[
      { name: "Global English", code: "en" },
      { name: "Australian English", code: "en_au" },
      { name: "British English", code: "en_uk" },
      { name: "US English", code: "en_us" },
    ]}
    columns={2}
  />
  <br />
</Accordion>

The following filler words are removed by default:

- "um"
- "uh"
- "hmm"
- "mhm"
- "uh-huh"
- "ah"
- "huh"
- "hm"
- "m"

If you want to keep filler words in the transcript, you can set the `disfluencies` to `true` in the transcription config.

<CodeBlocks>

```python title="Python SDK" highlight={8} maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(disfluencies=True)

transcript = aai.Transcriber(config=config).transcribe(audio_file)

if transcript.status == "error":
  raise RuntimeError(f"Transcription failed: {transcript.error}")

print(transcript.text)
```

```python title="Python" highlight={19} maxLines=15
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "disfluencies": True
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(f"Transcript ID:", transcript_id)
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

```

```javascript title="JavaScript SDK" highlight={12} maxLines=15
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  disfluencies: true,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  console.log(transcript.text);
};

run();
```

```javascript title="JavaScript" highlight={19} maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  disfluencies: true,
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

```csharp title="C#" highlight={69} maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
}

class Program
{
    static void Main(string[] args)
    {
        MainAsync(args).GetAwaiter().GetResult();
    }

    static async Task MainAsync(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Add("authorization", "<YOUR-API-KEY>");

            var localFilePath = "audio.mp3";

            Console.WriteLine("Uploading file...");
            var uploadUrl = await UploadFileAsync(localFilePath, httpClient);

            Console.WriteLine("Creating transcript with speech_model...");
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);

            Console.WriteLine("Waiting for transcript...");
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            Console.WriteLine("Transcription completed!");
            Console.WriteLine("----------------------------------");
            Console.WriteLine(transcript.Text);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var content = new StreamContent(fileStream))
        {
            content.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", content);
            response.EnsureSuccessStatusCode();

            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new
        {
            audio_url = audioUrl,
            disfluencies = true 
        };

        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "processing":
                case "queued":
                    Console.WriteLine($"Status: {transcript.Status}... waiting...");
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("Unexpected transcript status.");
            }
        }
    }
}
```

```ruby title="Ruby" highlight={23} maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
    "disfluencies" => true
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end
```

```php title="PHP" highlight={30} maxLines=15
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "disfluencies" => true
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        echo $transcription_result['text'];
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

</CodeBlocks>


---
title: Key Terms Prompting
---

# Fine-tuning with `keyterms_prompt`

Improve transcription accuracy by leveraging Slam-1's contextual understanding capabilities by prompting the model with certain words or phrases that are likely to appear frequently in your audio file.

Rather than simply increasing the likelihood of detecting specific words, Slam-1's multi-modal architecture actually understands the semantic meaning and context of the terminology you provide, enhancing transcription quality not just of the exact terms you specify, but also related terminology, variations, and contextually similar phrases.

Provide up to 1000 domain-specific words or phrases (maximum 6 words per phrase) that may appear in your audio using the optional `keyterms_prompt` parameter:

<Warning>
  This parameter is only supported when the `speech_model` is set to `"slam-1"`.
</Warning>

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
    ```python
    import requests
    import time

    base_url = "https://api.assemblyai.com"
    headers = {"authorization": "<YOUR_API_KEY>"}

    data = {
        "audio_url": "https://assembly.ai/sports_injuries.mp3",
        "speech_model": "slam-1",
        "keyterms_prompt": ['differential diagnosis', 'hypertension', 'Wellbutrin XL 150mg']
    }

    response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

    if response.status_code != 200:
        print(f"Error: {response.status_code}, Response: {response.text}")
        response.raise_for_status()

    transcript_response = response.json()
    transcript_id = transcript_response["id"]
    polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

    while True:
        transcript = requests.get(polling_endpoint, headers=headers).json()
        if transcript["status"] == "completed":
            print(transcript["text"])
            break
        elif transcript["status"] == "error":
            raise RuntimeError(f"Transcription failed: {transcript['error']}")
        else:
            time.sleep(3)
    ```

  </Tab>
  <Tab language="javascript" title="JavaScript">
    ```javascript
      import axios from 'axios'

      const baseUrl = 'https://api.assemblyai.com'

      const headers = {
        authorization: '<YOUR_API_KEY>'
      }

      const data = {
        audio_url: 'https://assembly.ai/sports_injuries.mp3',
        speech_model: 'slam-1',
        keyterms_prompt: ['differential diagnosis', 'hypertension', 'Wellbutrin XL 150mg']
      }

      const url = `${baseUrl}/v2/transcript`
      const response = await axios.post(url, data, { headers: headers })

      const transcriptId = response.data.id
      const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`

      while (true) {
        const pollingResponse = await axios.get(pollingEndpoint, {
          headers: headers
        })
        const transcriptionResult = pollingResponse.data

        if (transcriptionResult.status === 'completed') {
          console.log(transcriptionResult.text)
          break
        } else if (transcriptionResult.status === 'error') {
          throw new Error(`Transcription failed: ${transcriptionResult.error}`)
        } else {
          await new Promise((resolve) => setTimeout(resolve, 3000))
        }
      }
    ```

  </Tab>
</Tabs>

<Note title="Keyword Count Limits">

While we support up to 1000 key words and phrases, actual capacity may be lower due to internal tokenization and implementation constraints.

Key points to remember:

- Each word in a multi-word phrase counts towards the 1000 keyword limit
- Capitalization affects capacity (uppercase tokens consume more than lowercase)
- Longer words consume more capacity than shorter words

For optimal results, use shorter phrases when possible and be mindful of your total token count when approaching the keyword limit.

</Note>


---
title: Profanity Filtering
---

import { LanguageTable } from "../../../assets/components/LanguagesTable";

<Accordion title="Supported languages">
  <LanguageTable
    languages={[
      { name: "Global English", code: "en" },
      { name: "Australian English", code: "en_au" },
      { name: "British English", code: "en_uk" },
      { name: "US English", code: "en_us" },
      { name: "Spanish", code: "es" },
      { name: "French", code: "fr" },
      { name: "German", code: "de" },
      { name: "Italian", code: "it" },
      { name: "Portuguese", code: "pt" },
      { name: "Dutch", code: "nl" },
      { name: "Hindi", code: "hi" },
      { name: "Japanese", code: "ja" },
    ]}
    columns={2}
  />
  <br />
</Accordion>

You can automatically filter out profanity from the transcripts by setting `filter_profanity` to `true` in your transcription config.

Any profanity in the returned `text` will be replaced with asterisks.

<CodeBlocks>

```python title="Python SDK" highlight={8} maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(filter_profanity=True)

transcript = aai.Transcriber(config=config).transcribe(audio_file)

if transcript.status == "error":
  raise RuntimeError(f"Transcription failed: {transcript.error}")

print(transcript.text)
```

```python title="Python" highlight={19} maxLines=15
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "filter_profanity": True
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(f"Transcript ID:", transcript_id)
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

```

```javascript title="JavaScript SDK" highlight={12} maxLines=15
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  filter_profanity: true,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  console.log(transcript.text);
};

run();
```

```javascript title="JavaScript" highlight={19} maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  filter_profanity: true,
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

```csharp title="C#" highlight={69} maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
}

class Program
{
    static void Main(string[] args)
    {
        MainAsync(args).GetAwaiter().GetResult();
    }

    static async Task MainAsync(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Add("authorization", "<YOUR-API-KEY>");

            var localFilePath = "audio.mp3";

            Console.WriteLine("Uploading file...");
            var uploadUrl = await UploadFileAsync(localFilePath, httpClient);

            Console.WriteLine("Creating transcript with speech_model...");
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);

            Console.WriteLine("Waiting for transcript...");
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            Console.WriteLine("Transcription completed!");
            Console.WriteLine("----------------------------------");
            Console.WriteLine(transcript.Text);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var content = new StreamContent(fileStream))
        {
            content.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", content);
            response.EnsureSuccessStatusCode();

            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new
        {
            audio_url = audioUrl,
            filter_profanity = true
        };

        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "processing":
                case "queued":
                    Console.WriteLine($"Status: {transcript.Status}... waiting...");
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("Unexpected transcript status.");
            }
        }
    }
}

```

```ruby title="Ruby" highlight={23} maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
    "filter_profanity" => true
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end
```

```php title="PHP" highlight={30} maxLines=15
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "filter_profanity" => true
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        echo $transcription_result['text'];
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

</CodeBlocks>

<Note>
  Profanity filter isn't perfect. Certain words may still be missed or
  improperly filtered.
</Note>


---
title: Set the Start and End of the Transcript
---

If you only want to transcribe a portion of your file, you can set the `audio_start_from` and the `audio_end_at` parameters in your transcription config.

<CodeBlocks>

```python title="Python SDK" highlight={9-10} maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(
  audio_start_from=5000,  # The start time of the transcription in milliseconds
  audio_end_at=15000  # The end time of the transcription in milliseconds
)

transcript = aai.Transcriber(config=config).transcribe(audio_file)

if transcript.status == "error":
  raise RuntimeError(f"Transcription failed: {transcript.error}")

print(transcript.text)
```

```python title="Python" highlight={19,20} maxLines=15
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "audio_start_from": 5000, # The start time of the transcription in milliseconds
    "audio_end_at": 15000 # The end time of the transcription in milliseconds
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(f"Transcript ID:", transcript_id)
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

```

```javascript title="JavaScript SDK" highlight={12-13} maxLines=15
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  audio_start_from: 5000, // The start time of the transcription in milliseconds
  audio_end_at: 15000, // The end time of the transcription in milliseconds
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  console.log(transcript.text);
};

run();
```

```javascript title="JavaScript" highlight={19-20} maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  audio_start_from: 5000, // The start time of the transcription in milliseconds
  audio_end_at: 15000, // The end time of the transcription in milliseconds
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

```csharp title="C#" highlight={69-70} maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
}

class Program
{
    static void Main(string[] args)
    {
        MainAsync(args).GetAwaiter().GetResult();
    }

    static async Task MainAsync(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Add("authorization", "<YOUR-API-KEY>");

            var localFilePath = "audio.mp3";

            Console.WriteLine("Uploading file...");
            var uploadUrl = await UploadFileAsync(localFilePath, httpClient);

            Console.WriteLine("Creating transcript with speech_model...");
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);

            Console.WriteLine("Waiting for transcript...");
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            Console.WriteLine("Transcription completed!");
            Console.WriteLine("----------------------------------");
            Console.WriteLine(transcript.Text);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var content = new StreamContent(fileStream))
        {
            content.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", content);
            response.EnsureSuccessStatusCode();

            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new
        {
            audio_url = audioUrl,
            audio_start_from = 5000, 
            audio_end_at = 15000
        };

        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "processing":
                case "queued":
                    Console.WriteLine($"Status: {transcript.Status}... waiting...");
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("Unexpected transcript status.");
            }
        }
    }
}
```

```ruby title="Ruby" highlight={23-24} maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
    "audio_start_from" => 5000, # The start time of the transcription in milliseconds
    "audio_end_at" => 15000 # The end time of the transcription in milliseconds
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end
```

```php title="PHP" highlight={30-31} maxLines=15
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "audio_start_from" => 5000, # The start time of the transcription in milliseconds
    "audio_end_at" => 15000 # The end time of the transcription in milliseconds
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        echo $transcription_result['text'];
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

</CodeBlocks>


---
title: Speech Threshold
---

<Note title="Supported languages">
  Speech threshold is supported for all languages
</Note>

To only transcribe files that contain at least a specified percentage of spoken audio, you can set the `speech_threshold` parameter. You can pass any value between 0 and 1.

If the percentage of speech in the audio file is below the provided threshold, the value of `text` is `None` and the response contains an `error` message:

```plain maxLines=15
Audio speech threshold 0.9461 is below the requested speech threshold value 1.0
```

<Warning>A file must contain at least 30 seconds of audio for the amount of spoken audio to reliably be determined.</Warning>

<CodeBlocks>

```python title="Python SDK" highlight={8} maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(speech_threshold=0.5)

transcript = aai.Transcriber(config=config).transcribe(audio_file)

if transcript.status == "error":
  raise RuntimeError(f"Transcription failed: {transcript.error}")

print(transcript.text)
```

```python title="Python" highlight={19} maxLines=15
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "speech_threshold": 0.5
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(f"Transcript ID:", transcript_id)
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

```

```javascript title="JavaScript SDK" highlight={12} maxLines=15
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  speech_threshold: 0.5,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  if (transcript.status === "error") {
    throw new Error(`Transcription failed: ${transcript.error}`);
  }

  console.log(transcript.text);
};

run();
```

```javascript title="JavaScript" highlight={19} maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  speech_threshold: 0.5,
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

```csharp title="C#" highlight={69} maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
}

class Program
{
    static void Main(string[] args)
    {
        MainAsync(args).GetAwaiter().GetResult();
    }

    static async Task MainAsync(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Add("authorization", "<YOUR-API-KEY>");

            var localFilePath = "audio.mp3";

            Console.WriteLine("Uploading file...");
            var uploadUrl = await UploadFileAsync(localFilePath, httpClient);

            Console.WriteLine("Creating transcript with speech_model...");
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);

            Console.WriteLine("Waiting for transcript...");
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            Console.WriteLine("Transcription completed!");
            Console.WriteLine("----------------------------------");
            Console.WriteLine(transcript.Text);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var content = new StreamContent(fileStream))
        {
            content.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", content);
            response.EnsureSuccessStatusCode();

            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new
        {
            audio_url = audioUrl,
            speech_threshold = 0.5
        };

        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "processing":
                case "queued":
                    Console.WriteLine($"Status: {transcript.Status}... waiting...");
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("Unexpected transcript status.");
            }
        }
    }
}
```

```ruby title="Ruby" highlight={23} maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
    "speech_threshold" => 0.5
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end
```

```php title="PHP" highlight={30} maxLines=15
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "speech_threshold" => 0.5
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        echo $transcription_result['text'];
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

</CodeBlocks>


---
title: Delete Transcripts
---

You can remove the data from the transcript and mark it as deleted.

<CodeBlocks>

```python title="Python SDK" highlight={17} maxLines=15
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig()

transcript = aai.Transcriber(config=config).transcribe(audio_file)

if transcript.status == "error":
  raise RuntimeError(f"Transcription failed: {transcript.error}")

print(transcript.text)

transcript.delete_by_id(transcript.id)

transcript = aai.Transcript.get_by_id(transcript.id)
print(transcript.text)
```

```python title="Python" highlight={40} maxLines=15
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url # You can also use a URL to an audio or video file on the web
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(f"Transcript ID:", transcript_id)
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

response = requests.delete(polling_endpoint, headers=headers)

print(response.json()['text'])
```

```javascript title="JavaScript SDK" highlight={19} maxLines=15
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  console.log(transcript.text);

  const res = await client.transcripts.delete(transcript.id);

  console.log(res);
};

run();
```

```javascript title="JavaScript" highlight={43-45} maxLines=15
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}

const res = await axios.delete(pollingEndpoint, {
  headers: headers,
});

console.log(res.data.text);
```

```csharp title="C#" highlight={95-101} maxLines=15
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
}

class Program
{
    static void Main(string[] args)
    {
        MainAsync(args).GetAwaiter().GetResult();
    }

    static async Task MainAsync(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Add("authorization", "<YOUR-API-KEY>");

            var uploadUrl = await UploadFileAsync("audio.mp3", httpClient);
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            Console.WriteLine(transcript.Text);

            var deleteResponse = await DeleteTranscriptAsync(transcript.Id, httpClient);
            Console.WriteLine(deleteResponse.RootElement.GetProperty("text").GetString());
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var fileContent = new StreamContent(fileStream))
        {
            fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent);
            response.EnsureSuccessStatusCode();

            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new { audio_url = audioUrl };
        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "queued":
                case "processing":
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("Unexpected transcript status.");
            }
        }
    }

    static async Task<JsonDocument> DeleteTranscriptAsync(string transcriptId, HttpClient httpClient)
    {
        var url = $"https://api.assemblyai.com/v2/transcript/{transcriptId}";
        var response = await httpClient.DeleteAsync(url);
        response.EnsureSuccessStatusCode();
        return await response.Content.ReadFromJsonAsync<JsonDocument>();
    }
}
```

```ruby title="Ruby" highlight={63} maxLines=15
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url # You can also use a URL to an audio or video file on the web
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['text']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    puts 'Waiting for transcription to complete...'
    sleep(3)
  end
end

delete_request = Net::HTTP::Delete.new(polling_endpoint.request_uri, headers)
delete_response = polling_http.request(delete_request)
delete_response_body = JSON.parse(delete_response.body)

puts "Transcript deleted: #{delete_response_body['text']}"
```

```php title="PHP" highlight={75} maxLines=15
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url // You can also use a URL to an audio or video file on the web
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        echo $transcription_result['text'];
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}

$delete_request = curl_init($polling_endpoint);

curl_setopt($delete_request, CURLOPT_CUSTOMREQUEST, "DELETE");
curl_setopt($delete_request, CURLOPT_HTTPHEADER, $headers);
curl_setopt($delete_request, CURLOPT_RETURNTRANSFER, true);

$delete_response = curl_exec($delete_request);
$delete_response_data = json_decode($delete_response, true);

curl_close($delete_request);

echo "Transcript deleted: " . $delete_response_data['text'];

```

</CodeBlocks>

<Note title="Account-level TTL value">
Starting on 11-26-2024, the platform will assign an account-level Time to Live (TTL) for customers who have executed a Business Associate Agreement (BAA) with AssemblyAI. For those customers, all transcripts generated via the async transcription endpoint will be deleted after the TTL period.

As of the feature launch date:

- The TTL is set to 3 days (subject to change).
- Customers can still manually delete transcripts before the TTL period by using the deletion endpoint.
  However, they cannot keep transcripts on the platform after the TTL
  period has expired.

BAAs are limited to customers who process PHI, subject to HIPAA. If you are processing PHI and require a BAA, please reach out to sales@assemblyai.com.

</Note>


---
title: Livekit
description: Livekit voice agent integration
---

<Note>
  This guide assumes prior knowledge of LiveKit. If you haven't used LiveKit before and are unfamiliar with LiveKit, please check out our [Building a Voice Agent with LiveKit and AssemblyAI guide](https://www.assemblyai.com/docs/speech-to-text/livekit-intro-guide).
</Note>

## Overview

LiveKit is an open source platform for developers building realtime media applications. In this guide, we'll show you how to integrate AssemblyAI's streaming speech-to-text model into your Livekit voice agent using the Agents framework.

<Card 
    title="Livekit" 
    icon={<img src="https://assemblyaiassets.com/images/Livekit.svg" alt="Livekit logo"/>} 
    href="https://docs.livekit.io/agents/integrations/stt/assemblyai/"
>
    View Livekit's AssemblyAI STT plugin documentation.
</Card>

## Quick start

### Installation

Install the plugin from PyPI:

```bash
pip install "livekit-agents[assemblyai]"
```

### Authentication

The AssemblyAI plugin requires an [AssemblyAI API key](https://www.assemblyai.com/docs/api-reference/overview#authorization). Set `ASSEMBLYAI_API_KEY` in your `.env` file.

<Tip>
  You can obtain an AssemblyAI API key by signing up
  [here](https://www.assemblyai.com/dashboard/signup).
</Tip>

### Basic usage

Use AssemblyAI STT in an `AgentSession` or as a standalone transcription service:

```python
from livekit.plugins import assemblyai

session = AgentSession(
    stt = assemblyai.STT(
      end_of_turn_confidence_threshold=0.7,
      min_end_of_turn_silence_when_confident=160,
      max_turn_silence=2400,
    ),
    # ... llm, tts, etc.
    vad=silero.VAD.load(), # VAD Enabled for Interruptions
    turn_detection="stt", # Enable Turn Detection
)
```

## Configuration

### Turn Detection (Key Feature)

AssemblyAI's new turn detection model was built specifically for voice agents and you can tweak it to fit your use case. It processes both audio and linguistic information to determine an end of turn confidence score on every inference, and if that confidence score is past the set threshold, it triggers end of turn.

This custom model was designed to address 2 major issues with voice agents. With traditional VAD (voice activity detection) approaches based on silence alone, there are situations where the agent wouldn't wait for a user to finish their turn even if the audio data suggested it. Think of a situation like "My credit card number is____" - if someone is looking that up, traditional VAD may not wait for the user, where our turn detection model is far better in these situations.

Additionally, in situations where we are certain that the user is done speaking like "What is my credit score?", a high end of turn confidence is returned, greater than the threshold, and triggering end of turn, allowing for minimal turnaround latency in those scenarios.

```python
# STT-based turn detection (recommended)
turn_detection="stt"

stt=assemblyai.STT(
    end_of_turn_confidence_threshold=0.7,
    min_end_of_turn_silence_when_confident=160,  # in ms
    max_turn_silence=2400,  # in ms
)
```

**Parameter tuning:**
- **end_of_turn_confidence_threshold**: Raise or lower the threshold based on how confident you'd like us to be before triggering end of turn based on confidence score
- **min_end_of_turn_silence_when_confident**: Increase or decrease the amount of time we wait to trigger end of turn when confident
- **max_turn_silence**: Lower or raise the amount of time needed to trigger end of turn when end of turn isn't triggered by a high confidence score

<Tip>
  You can also set `turn_detection="vad"` if you'd like turn detection to be based on Silero VAD instead of our advanced turn detection model.
</Tip>

For more information, see our [Universal-Streaming end-of-turn detection guide](https://www.assemblyai.com/docs/speech-to-text/universal-streaming#end-of-turn-detection) and [message-by-message breakdown](https://www.assemblyai.com/docs/speech-to-text/universal-streaming/message-sequence).

### Parameters

<ParamField path="api_key" type="str">
  Your AssemblyAI API key.
</ParamField>

<ParamField path="sample_rate" type="int" default="16000">
  The sample rate of the audio stream
</ParamField>

<ParamField path="encoding" type="str" default="pcm_s16le">
  The encoding of the audio stream. Allowed values: `pcm_s16le`, `pcm_mulaw`
</ParamField>

<ParamField path="format_turns" type="bool" default="True">
  Whether to return formatted final transcripts. If enabled, formatted final
  transcripts will be emitted shortly following an end-of-turn detection.
</ParamField>

<ParamField path="end_of_turn_confidence_threshold" type="float" default="0.7">
  The confidence threshold to use when determining if the end of a turn has been
  reached.  
  In our API the default is 0.4, but the default in LiveKit is set to 0.65.
</ParamField>

<ParamField path="min_end_of_turn_silence_when_confident" type="int" default="160">
  The minimum amount of silence required to detect end of turn when confident.
</ParamField>

<ParamField path="max_turn_silence" type="int" default="2400">
  The maximum amount of silence allowed in a turn before end of turn is triggered.
</ParamField>


---
title: Pipecat
description: Pipecat voice agent integration
---

<Note>
  This guide assumes prior knowledge of Pipecat. If you haven't used Pipecat before and are unfamiliar with Pipecat, please check out our [Building a Voice Agent with Pipecat and AssemblyAI guide](https://www.assemblyai.com/docs/speech-to-text/pipecat-intro-guide).
</Note>

## Overview

Pipecat is an open source platform for developers building realtime media applications. In this guide, we'll show you how to integrate AssemblyAI's streaming speech-to-text model into your Pipecat voice agent using the Pipeline framework.

<Card 
    title="Pipecat" 
    icon={<img src="https://assemblyaiassets.com/images/Pipecat.svg" alt="Pipecat logo"/>} 
    href="https://docs.pipecat.ai/server/services/stt/assemblyai"
>
    View Pipecat's AssemblyAI STT plugin documentation.
</Card>

## Quick start

### Installation

Install the AssemblyAI service from PyPI:

```bash
pip install "pipecat-ai[assemblyai]"
```

### Authentication

The AssemblyAI service requires an [AssemblyAI API key](https://www.assemblyai.com/docs/api-reference/overview#authorization). Set `ASSEMBLYAI_API_KEY` in your `.env` file.

<Tip>
  You can obtain an AssemblyAI API key by signing up
  [here](https://www.assemblyai.com/dashboard/signup).
</Tip>

### Basic usage

Use AssemblyAI STT in a `Pipeline`:

```python
from pipecat.services.assemblyai.stt import AssemblyAISTTService, AssemblyAIConnectionParams

# Configure service
stt = AssemblyAISTTService(
    api_key=os.getenv("ASSEMBLYAI_API_KEY"),
    vad_force_turn_endpoint=False,  # Use AssemblyAI's STT-based turn detection
    connection_params=AssemblyAIConnectionParams(
        end_of_turn_confidence_threshold=0.7,
        min_end_of_turn_silence_when_confident=160,
        max_turn_silence=2400,
    )
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    llm,
    tts,
    transport.output(),
])
```

## Configuration

### Turn Detection (Key Feature)

AssemblyAI's new turn detection model was built specifically for voice agents and you can tweak it to fit your use case. It processes both audio and linguistic information to determine an end of turn confidence score on every inference, and if that confidence score is past the set threshold, it triggers end of turn.

This custom model was designed to address 2 major issues with voice agents. With traditional VAD (voice activity detection) approaches based on silence alone, there are situations where the agent wouldn't wait for a user to finish their turn even if the audio data suggested it. Think of a situation like "My credit card number is____" - if someone is looking that up, traditional VAD may not wait for the user, where our turn detection model is far better in these situations.

Additionally, in situations where we are certain that the user is done speaking like "What is my credit score?", a high end of turn confidence is returned, greater than the threshold, and triggering end of turn, allowing for minimal turnaround latency in those scenarios.

You can set the `vad_force_turn_endpoint` parameter within the `AssemblyAISTTService` constructor:

```python
stt = AssemblyAISTTService(
    api_key=os.getenv("ASSEMBLYAI_API_KEY"),
    vad_force_turn_endpoint=False,  # Use AssemblyAI's STT-based turn detection
    connection_params=AssemblyAIConnectionParams(
        end_of_turn_confidence_threshold=0.7,
        min_end_of_turn_silence_when_confident=160,  # in ms
        max_turn_silence=2400,  # in ms
    )
)
```

**Parameter tuning:**
- **end_of_turn_confidence_threshold**: Raise or lower the threshold based on how confident you'd like us to be before triggering end of turn based on confidence score
- **min_end_of_turn_silence_when_confident**: Increase or decrease the amount of time we wait to trigger end of turn when confident
- **max_turn_silence**: Lower or raise the amount of time needed to trigger end of turn when end of turn isn't triggered by a high confidence score

<Tip>
  You can also set `vad_force_turn_endpoint=True` if you'd like turn detection to be based on VAD instead of our advanced turn detection model.
</Tip>

For more information, see our [Universal-Streaming end-of-turn detection guide](https://www.assemblyai.com/docs/speech-to-text/universal-streaming#end-of-turn-detection) and [message-by-message breakdown](https://www.assemblyai.com/docs/speech-to-text/universal-streaming/message-sequence).

## Parameters

### Constructor Parameters

<ParamField path="api_key" type="str" required>
  Your AssemblyAI API key.
</ParamField>

<ParamField path="connection_params" type="AssemblyAIConnectionParams">
  Connection parameters for the AssemblyAI WebSocket connection. See below for
  details.
</ParamField>

<ParamField path="vad_force_turn_endpoint" type="bool" default="True">
  When true, sends a `ForceEndpoint` event to AssemblyAI when a
  `UserStoppedSpeakingFrame` is received. Requires a VAD (Voice Activity
  Detection) processor in the pipeline to generate these frames.
</ParamField>

<ParamField path="language" type="Language" default="Language.EN">
  Language for transcription. AssemblyAI currently only supports English
  Streaming transcription.
</ParamField>

### Connection Parameters

<ParamField path="sample_rate" type="int" default="16000">
  The sample rate of the audio stream
</ParamField>

<ParamField path="encoding" type="str" default="pcm_s16le">
  The encoding of the audio stream. Allowed values: `pcm_s16le`, `pcm_mulaw`
</ParamField>

<ParamField path="format_turns" type="bool" default="True">
  Whether to return formatted final transcripts. If enabled, formatted final
  transcripts will be emitted shortly following an end-of-turn detection.
</ParamField>

<ParamField path="end_of_turn_confidence_threshold" type="float" default="0.7">
  The confidence threshold to use when determining if the end of a turn has been
  reached.
</ParamField>

<ParamField path="min_end_of_turn_silence_when_confident" type="int"  default="160">
  The minimum amount of silence required to detect end of turn when confident.
</ParamField>

<ParamField path="max_turn_silence" type="int" default="2400">
  The maximum amount of silence allowed in a turn before end of turn is
  triggered.
</ParamField>


---
title: Vapi
description: Vapi voice agent integration
---

## Overview

Vapi is a developer platform for building voice AI agents, they handle the complex backend of voice agents for you so you can focus on creating great voice experiences. In this guide, we'll show you how to integrate AssemblyAI's streaming speech-to-text model into your Vapi voice agent.

<Card
  title="Vapi"
  icon={
    <img src="https://assemblyaiassets.com/images/Vapi.svg" alt="Vapi logo" />
  }
  href="https://docs.vapi.ai/providers/transcriber/assembly-ai"
>
  View Vapi's AssemblyAI STT provider documentation.
</Card>

## Quick start

<Steps>
    **Head to the "Assistants" tab in your Vapi dashboard.**

    <Frame>
        <img src="file:ca47a3c9-e4ac-401a-bda7-c4e76fee79ec" />
    </Frame>

    **Click on your assistant and then the "Transcriber" tab.**

    <Frame>
        <img src="file:7ca3f3dd-db10-463a-afb4-f603cc3dd92b" />
    </Frame>

    **Select "Assembly AI" on the Provider dropdown and make sure the "Universal Streaming API" option is toggled on.**

    <Frame>
        <img src="file:5903fffc-4278-4106-9f7c-0564c1fae228" />
    </Frame>

</Steps>

Your voice agent now uses **AssemblyAI** for speech-to-text (STT) processing.

<Info>
  New to Vapi? Visit the [Quickstart
  Guide](https://docs.vapi.ai/quickstart/introduction) to explore various
  example voice agent workflows. For the easiest way to test a voice agent,
  follow this [simple phone-based guide](https://docs.vapi.ai/quickstart/phone).
  Vapi offers a wide range of example workflows to get you up and running
  quickly.
</Info>

## Recommended settings for optimal latency

### Transcriber settings

For best latency, Format Turns should be turned off (adds about 50ms of delay), and Universal Streaming should be enabled.

<Frame>
  <img src="file:6adf3371-3418-4cbd-bdab-ab5a2b2877f5" />
</Frame>

### Advanced > Start speaking plan

For start speaking plan, raise the `Wait seconds` to prevent false starts. `Smart Endpointing` should be turned off.

<Frame>
  <img src="file:5a05687c-a06d-4c0c-ad12-dc21b9efba34" />
</Frame>

### Advanced > Stop speaking plan

For stop speaking plan, lower `Voice seconds` to allow for faster interruptions.

<Frame>
  <img src="file:8c05935c-87dc-4cf4-abbd-e87a623866b7" />
</Frame>


---
title: Turn detection
description: Intelligent turn detection with Streaming Speech-to-Text
---

### Overview

AssemblyAI's end-of-turn detection functionality is integrated into our Streaming STT model, leveraging both acoustic and semantic features, and is coupled with a traditional silence-based heuristic approach. Both mechanisms work jointly and either can trigger end-of-turn detection throughout the audio stream. This joint approach significantly enhances the speed and accuracy of end-of-turn detection while allowing this functionality to fall back to the traditional method when the model makes a misprediction.

This functionality is built natively into the STT model, making it ideal for addressing two common problems voice agent developers face: awkward pauses (long silence) or the risk of making premature interruptions (short silence), both of which disrupt the natural flow of conversation.

This approach allows for:

- Semantic understanding of natural speech patterns rather than simple silence thresholds
- Support for natural pauses and thinking time without premature responses
- Smooth conversation flow without awkward interruptions or artificial delays

<Note>
  End-of-turn and end-of-utterances refer to the same thing and may be used
  interchangeably in these docs.
</Note>

### Model-based detection

Triggers when **all** conditions are met:

#### EOT token predicted

- Model predicts semantic end-of-turn with a probability greater than `end_of_turn_confidence_threshold`
- Default: `0.7` (user configurable)

#### Minimum silence duration has passed

- After the last non-silence word token, `min_end_of_turn_silence_when_confident` milliseconds must pass
- Default: `160 ms` (user configurable)
- Format: `milliseconds`

#### Minimum speech duration spoken

- The user must speak for at least `80 ms` since the last end-of-turn (ensures at least one word)
- Set to `80 ms` (internal)

#### Word finalized

- Last word in `turn.words` has been finalized
- Internal configuration

### Silence-based detection

Triggers when **all** conditions are met:

#### Minimum speech duration spoken

- The user must speak for at least `80 ms` since the last end-of-turn (ensures at least one word)
- Set to `80 ms` (internal)

#### Maximum silence duration has passed

- After the last non-silence word token, `max_turn_silence` milliseconds must pass
- Default: `2400 ms` (user configurable)
- Format: `milliseconds`

### Disable turn detection

To disable model-based turn detection, set `end_of_turn_confidence_threshold` to 1. This will stop the model from predicting end-of-turns and will only use silence-based detection.
If you are using your own form of turn detection (such as VAD or a custom turn detection model), you can send a `ForceEndpoint` event to the server to force the end of a turn and receive the final turn transcript.

```python
ws.send(json.dumps({"type": "ForceEndpoint"}))
```

### Important notes

- Silence-based detection can override model-based detection even with high EOT confidence thresholds
- Word finalization always takes precedence — endpointing won't occur until the last word is finalized
- We define end-of-turn detection as the process of detecting the end of sustained speech activity, often called end-pointing in the Voice Agents context

---
title: 'Streaming API: Message Sequence Breakdown'
hide-nav-links: true
description: The anatomy of an utterance from Partial to Final Transcript
---

# Message sequence breakdown

## Partial transcripts

```json
{
  "turn_order": 0,
  "turn_is_formatted": false,
  "end_of_turn": false,
  "transcript": "",
  "end_of_turn_confidence": 0.6819284558296204,
  "words": [
    {
      "start": 1440,
      "end": 1520,
      "text": "hi",
      "confidence": 0.9967870712280273,
      "word_is_final": false
    }
  ],
  "type": "Turn"
}
```

```json
{
  "turn_order": 0,
  "turn_is_formatted": false,
  "end_of_turn": false,
  "transcript": "hi my",
  "end_of_turn_confidence": 0.004442221485078335,
  "words": [
    {
      "start": 1440,
      "end": 1520,
      "text": "hi",
      "confidence": 0.9967870712280273,
      "word_is_final": true
    },
    {
      "start": 1600,
      "end": 1680,
      "text": "my",
      "confidence": 0.999546468257904,
      "word_is_final": true
    },
    {
      "start": 1600,
      "end": 1680,
      "text": "name",
      "confidence": 0.9597182273864746,
      "word_is_final": false
    }
  ],
  "type": "Turn"
}
```

```json
{
  "turn_order": 0,
  "turn_is_formatted": false,
  "end_of_turn": false,
  "transcript": "hi my name",
  "end_of_turn_confidence": 0.01579524204134941,
  "words": [
    {
      "start": 1440,
      "end": 1520,
      "text": "hi",
      "confidence": 0.9967870712280273,
      "word_is_final": true
    },
    {
      "start": 1600,
      "end": 1680,
      "text": "my",
      "confidence": 0.999546468257904,
      "word_is_final": true
    },
    {
      "start": 1600,
      "end": 1680,
      "text": "name",
      "confidence": 0.9597182273864746,
      "word_is_final": true
    },
    {
      "start": 1680,
      "end": 1760,
      "text": "is",
      "confidence": 0.8261497616767883,
      "word_is_final": false
    }
  ],
  "type": "Turn"
}
```

```json
{
  "turn_order": 0,
  "turn_is_formatted": false,
  "end_of_turn": false,
  "transcript": "hi my name is",
  "end_of_turn_confidence": 0.017141787335276604,
  "words": [
    {
      "start": 1440,
      "end": 1520,
      "text": "hi",
      "confidence": 0.9967870712280273,
      "word_is_final": true
    },
    {
      "start": 1600,
      "end": 1680,
      "text": "my",
      "confidence": 0.999546468257904,
      "word_is_final": true
    },
    {
      "start": 1600,
      "end": 1680,
      "text": "name",
      "confidence": 0.9597182273864746,
      "word_is_final": true
    },
    {
      "start": 1680,
      "end": 1760,
      "text": "is",
      "confidence": 0.8261497616767883,
      "word_is_final": true
    },
    {
      "start": 2320,
      "end": 2400,
      "text": "son",
      "confidence": 0.471368670463562,
      "word_is_final": false
    }
  ],
  "type": "Turn"
}
```

```json
{
  "turn_order": 0,
  "turn_is_formatted": false,
  "end_of_turn": false,
  "transcript": "hi my name is",
  "end_of_turn_confidence": 0.24533912539482117,
  "words": [
    {
      "start": 1440,
      "end": 1520,
      "text": "hi",
      "confidence": 0.9967870712280273,
      "word_is_final": true
    },
    {
      "start": 1600,
      "end": 1680,
      "text": "my",
      "confidence": 0.999546468257904,
      "word_is_final": true
    },
    {
      "start": 1600,
      "end": 1680,
      "text": "name",
      "confidence": 0.9597182273864746,
      "word_is_final": true
    },
    {
      "start": 1680,
      "end": 1760,
      "text": "is",
      "confidence": 0.8261497616767883,
      "word_is_final": true
    },
    {
      "start": 2320,
      "end": 2640,
      "text": "sonny",
      "confidence": 0.5737350583076477,
      "word_is_final": false
    }
  ],
  "type": "Turn"
}
```

```json
{
  "turn_order": 0,
  "turn_is_formatted": false,
  "end_of_turn": true,
  "transcript": "hi my name is sonny",
  "end_of_turn_confidence": 0.8095446228981018,
  "words": [
    {
      "start": 1440,
      "end": 1520,
      "text": "hi",
      "confidence": 0.9967870712280273,
      "word_is_final": true
    },
    {
      "start": 1600,
      "end": 1680,
      "text": "my",
      "confidence": 0.999546468257904,
      "word_is_final": true
    },
    {
      "start": 1600,
      "end": 1680,
      "text": "name",
      "confidence": 0.9597182273864746,
      "word_is_final": true
    },
    {
      "start": 1680,
      "end": 1760,
      "text": "is",
      "confidence": 0.8261497616767883,
      "word_is_final": true
    },
    {
      "start": 2320,
      "end": 3040,
      "text": "sonny",
      "confidence": 0.5737350583076477,
      "word_is_final": true
    }
  ],
  "type": "Turn"
}
```

```json
{
  "turn_order": 0,
  "turn_is_formatted": true,
  "end_of_turn": true,
  "transcript": "Hi, my name is Sonny.",
  "end_of_turn_confidence": 0.8095446228981018,
  "words": [
    {
      "start": 1440,
      "end": 1520,
      "text": "Hi,",
      "confidence": 0.9967870712280273,
      "word_is_final": true
    },
    {
      "start": 1600,
      "end": 1680,
      "text": "my",
      "confidence": 0.999546468257904,
      "word_is_final": true
    },
    {
      "start": 1600,
      "end": 1680,
      "text": "name",
      "confidence": 0.9597182273864746,
      "word_is_final": true
    },
    {
      "start": 1680,
      "end": 1760,
      "text": "is",
      "confidence": 0.8261497616767883,
      "word_is_final": true
    },
    {
      "start": 2320,
      "end": 3040,
      "text": "Sonny.",
      "confidence": 0.5737350583076477,
      "word_is_final": true
    }
  ],
  "type": "Turn"
}
```


---
title: Apply LLMs to Audio Files
subtitle: Learn how to leverage LLMs for speech using LeMUR.
hide-nav-links: true
description: Learn how to leverage LLMs for speech using LeMUR.
---

## Overview

A Large Language Model (LLM) is a machine learning model that uses natural language processing (NLP) to generate text. [LeMUR](https://assemblyai.com/docs/api-reference/lemur) is a framework that lets you apply LLMs to audio transcripts, for example to ask questions about a call, or to summarize a meeting.

By the end of this tutorial, you'll be able to use LeMUR to summarize an audio file.

Here's the full sample code for what you'll build in this tutorial:

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>

```python
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

transcriber = aai.Transcriber()

# You can use a local filepath:
# audio_file = "./example.mp3"

# Or use a publicly-accessible URL:
audio_file = (
  "https://assembly.ai/sports_injuries.mp3"
)
transcript = transcriber.transcribe(audio_file)

prompt = "Provide a brief summary of the transcript."

result = transcript.lemur.task(
  prompt, final_model=aai.LemurModel.claude4_sonnet_20250514
)

print(result.response)
```

</Tab>
<Tab language="python" title="Python">

```python
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
  "authorization": "<YOUR_API_KEY>"
}

# You can use a local filepath:
# with open("./my-audio.mp3", "rb") as f:
# response = requests.post(base_url + "/v2/upload",
# headers=headers,
# data=f)
# upload_url = response.json()["upload_url"]
# Or use a publicly-accessible URL:

upload_url = "https://assembly.ai/sports_injuries.mp3"

data = {
  "audio_url": upload_url
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

transcript_id = response.json()["id"]
polling_endpoint = base_url + f"/v2/transcript/{transcript_id}"

while True:
transcript = requests.get(polling_endpoint, headers=headers).json()

  if transcript["status"] == "completed":
    break

  elif transcript["status"] == "error":
    raise RuntimeError(f"Transcription failed: {transcript['error']}")

  else:
    time.sleep(3)

prompt = "Provide a brief summary of the transcript."

lemur_data = {
  "prompt": prompt,
  "transcript_ids": [transcript_id],
  "final_model": "anthropic/claude-sonnet-4-20250514",
}

result = requests.post(base_url + "/lemur/v3/generate/task", headers=headers, json=lemur_data)
print(result.json()["response"])
```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">

```javascript
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// You can use a local filepath:
// const audioFile = "./example.mp3"

// Or use a publicly-accessible URL:
const audioFile = "https://assembly.ai/sports_injuries.mp3";

const run = async () => {
  const transcript = await client.transcripts.transcribe({ audio: audioFile });

  const prompt = "Provide a brief summary of the transcript.";

  const { response } = await client.lemur.task({
    transcript_ids: [transcript.id],
    prompt,
    final_model: "anthropic/claude-sonnet-4-20250514",
  });

  console.log(response);
};

run();
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from "axios";
import fs from "fs-extra";

const base_url = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${base_url}/v2/upload`, audioData, {
  headers,
});

const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL of an audio or video file on the web
};

const response = await axios.post(base_url + "/v2/transcript", data, {
  headers,
});

const transcript_id = response.data.id;
const polling_endpoint = base_url + `/v2/transcript/${transcript_id}`;

while (true) {
  const transcript = (await axios.get(polling_endpoint, { headers })).data;

  if (transcript.status === "completed") {
    break;
  } else if (transcript.status === "error") {
    throw new Error(`Transcription failed: ${transcript.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}

const prompt = "Provide a brief summary of the transcript.";

const lemur_data = {
  prompt: prompt,
  transcript_ids: [transcript_id],
  final_model: "anthropic/claude-sonnet-4-20250514",
};

const result = await axios.post(
  base_url + "/lemur/v3/generate/task",
  lemur_data,
  { headers }
);
console.log(result.data.response);
```

</Tab>
<Tab language="csharp" title="C#">

```csharp
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Threading.Tasks;

public class Transcript
{
 public string Id { get; set; }
 public string Status { get; set; }
 public string Text { get; set; }

 [JsonPropertyName("language_code")]
 public string LanguageCode { get; set; }

 public string Error { get; set; }
}

public class LemurResponse
{
[JsonPropertyName("request_id")]
public string RequestId { get; set; }

public string Response { get; set; }

public Usage Usage { get; set; }
}

public class Usage
{
[JsonPropertyName("input_tokens")]
public int InputTokens { get; set; }

[JsonPropertyName("output_tokens")]
public int OutputTokens { get; set; }
}

private static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
{
  using (var fileStream = File.OpenRead(filePath))
  using (var fileContent = new StreamContent(fileStream))
  {
      fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

      using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent))
      {
          response.EnsureSuccessStatusCode();
          var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
          return jsonDoc.RootElement.GetProperty("upload_url").GetString();
      }
  }
}


private static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
{
 var data = new { audio_url = audioUrl };
 var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

 using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
 {
     response.EnsureSuccessStatusCode();
     return await response.Content.ReadFromJsonAsync<Transcript>();
 }
}

private static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
{
 var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

 while (true)
 {
     var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
     transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();
     switch (transcript.Status)
     {
         case "processing":
         case "queued":
             await Task.Delay(TimeSpan.FromSeconds(3));
             break;
         case "completed":
             return transcript;
         case "error":
             throw new Exception($"Transcription failed: {transcript.Error}");
         default:
             throw new Exception("This code shouldn't be reachable.");
     }
 }
}

private static async Task<LemurResponse> GenerateTaskAsync(string prompt, List<string> transcriptIds, HttpClient httpClient)
{
 var data = new
 {
     transcript_ids = transcriptIds,
     prompt,
     final_model = "anthropic/claude-sonnet-4-20250514"
 };

 var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

 using var response = await httpClient.PostAsync("https://api.assemblyai.com/lemur/v3/generate/task", content);
 response.EnsureSuccessStatusCode();
 return await response.Content.ReadFromJsonAsync<LemurResponse>();
}

using (var httpClient = new HttpClient())
{
 httpClient.DefaultRequestHeaders.Authorization =
     new AuthenticationHeaderValue("<YOUR_API_KEY>");

 const string prompt = "Provide a brief summary of the transcript.";

 var uploadUrl = await UploadFileAsync("/my_audio.mp3", httpClient);
 var transcript = await CreateTranscriptAsync(uploadUrl, httpClient); // You can also replace uploadUrl with an audio file URL
 transcript = await WaitForTranscriptToProcess(transcript, httpClient);

 var transcriptIds = new List<string> { transcript.Id };
 var lemurResponse = await GenerateTaskAsync(prompt, transcriptIds, httpClient);

 Console.WriteLine(lemurResponse.Response);
}
```

</Tab>
<Tab language="ruby" title="Ruby">

```ruby
require 'net/http'
require 'json'

base_url = "https://api.assemblyai.com"
headers = {
   "authorization" => "<YOUR_API_KEY>",
   "content-type" => "application/json"
}

path = "/my_audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

uri = URI("#{base_url}/v2/transcript")
request = Net::HTTP::Post.new(uri, headers)
request.body = { audio_url: upload_url }.to_json # You can also replace upload_url with an audio file URL

response = http.request(request)
transcript_id = JSON.parse(response.body)["id"]
polling_endpoint = "#{base_url}/v2/transcript/#{transcript_id}"

while true
   polling_uri = URI(polling_endpoint)
   polling_request = Net::HTTP::Get.new(polling_uri, headers)
   polling_response = http.request(polling_request)
   transcription_result = JSON.parse(polling_response.body)

   if transcription_result["status"] == "completed"
       break
   elsif transcription_result["status"] == "error"
       raise "Transcription failed: #{transcription_result["error"]}"
   else
       sleep(3)
   end
end

prompt = "Provide a brief summary of the transcript."

lemur_uri = URI("#{base_url}/lemur/v3/generate/task")
lemur_request = Net::HTTP::Post.new(lemur_uri, headers)
lemur_request.body = {
  final_model: "anthropic/claude-sonnet-4-20250514",
  prompt: prompt,
  transcript_ids: [transcript_id]
}.to_json

lemur_response = http.request(lemur_request)
lemur_result = JSON.parse(lemur_response.body)
puts lemur_result["response"]
```

</Tab>
<Tab language="php" title="PHP">

```php
<?php
$base_url = "https://api.assemblyai.com";
$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);
$path = "/my_audio.mp3";

$ch = curl_init($base_url . "/v2/upload");
curl_setopt_array($ch, [
    CURLOPT_POST => true,
    CURLOPT_POSTFIELDS => file_get_contents($path),
    CURLOPT_HTTPHEADER => $headers,
    CURLOPT_RETURNTRANSFER => true
]);

$response = curl_exec($ch);
$upload_url = json_decode($response, true)["upload_url"];
curl_close($ch);

$ch = curl_init($base_url . "/v2/transcript");
curl_setopt_array($ch, [
    CURLOPT_POST => true,
    CURLOPT_POSTFIELDS => json_encode(["audio_url" => $upload_url]), // You can also replace upload_url with an audio file URL
    CURLOPT_HTTPHEADER => $headers,
    CURLOPT_RETURNTRANSFER => true
]);

$response = curl_exec($ch);
$transcript_id = json_decode($response, true)['id'];
curl_close($ch);

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $ch = curl_init($polling_endpoint);
    curl_setopt_array($ch, [
        CURLOPT_HTTPHEADER => $headers,
        CURLOPT_RETURNTRANSFER => true
    ]);

    $transcription_result = json_decode(curl_exec($ch), true);
    curl_close($ch);

    if ($transcription_result['status'] === "completed") {
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    }
    sleep(3);
}

$prompt = 'Provide a brief summary of the transcript.'

$ch = curl_init($base_url . "/lemur/v3/generate/task");
curl_setopt_array($ch, [
    CURLOPT_RETURNTRANSFER => true,
    CURLOPT_POST => true,
    CURLOPT_HTTPHEADER => $headers,
    CURLOPT_POSTFIELDS => json_encode([
        'final_model' => 'anthropic/claude-sonnet-4-20250514',
        'prompt' => $prompt,
        'transcript_ids' => [$transcript_id]
    ])
]);

$response = curl_exec($ch);
$result = json_decode($response, true);
echo $result['response'];
curl_close($ch);
```

</Tab>
</Tabs>

If you run the code above, you'll see the following output:

```plain
The transcript describes several common sports injuries - runner's knee,
sprained ankle, meniscus tear, rotator cuff tear, and ACL tear. It provides
definitions, causes, and symptoms for each injury. The transcript seems to be
narrating sports footage and describing injuries as they occur to the athletes.
Overall, it provides an overview of these common sports injuries that can result
from overuse or sudden trauma during athletic activities
```

## Before you begin

To complete this tutorial, you need:

- [Python](https://www.python.org/), [Node](https://nodejs.org/en), [.NET](https://dotnet.microsoft.com/en-us/download), [Ruby](https://www.ruby-lang.org/en/documentation/installation/) or [PHP](https://www.php.net/) installed.
- An <a href="https://www.assemblyai.com/dashboard/signup" target="_blank">AssemblyAI account with a credit card set up</a>.
- Basic understanding of how to [Transcribe an audio file](/docs/getting-started/transcribe-an-audio-file).

## Step 1: Install prerequisites

<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

Install the package via pip:

```bash
pip install assemblyai
```

</Tab>

<Tab language="python" title="Python">
Install the package via pip:

```bash
pip install requests
```

</Tab>

<Tab language="javascript-sdk" title="JavaScript SDK">
Install the package via NPM:

```bash
npm install assemblyai
```

</Tab>

<Tab language="javascript" title="JavaScript">
Install the package via NPM:

```bash
npm install axios
```

</Tab>

<Tab language="csharp" title="C#">

No additional packages required - uses standard .NET libraries

</Tab>

<Tab language="ruby" title="Ruby">

No additional gems required - uses standard libraries

</Tab>

<Tab language="php" title="PHP">

No additional packages required - uses built-in cURL functions

</Tab>

</Tabs>

## Step 2: Transcribe an audio file

LeMUR uses one or more transcripts as input to generate text output. In this step, you'll transcribe an audio file that you can later use to create a prompt for.

For more information about transcribing audio, see [Transcribe an audio file](/docs/getting-started/transcribe-an-audio-file).

<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

```python
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

transcriber = aai.Transcriber()

# You can use a local filepath:
# audio_file = "./example.mp3"

# Or use a publicly-accessible URL:
audio_file = (
  "https://assembly.ai/sports_injuries.mp3"
)

transcript = transcriber.transcribe(audio_file)
```

</Tab>

<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {"authorization": "<YOUR_API_KEY>"}

# You can use a local filepath:
# with open("./my-audio.mp3", "rb") as f:
#     response = requests.post(base_url + "/v2/upload", headers=headers, data=f)
#     upload_url = response.json()["upload_url"]

# Or use a publicly-accessible URL:
upload_url = "https://assembly.ai/sports_injuries.mp3"

data = {"audio_url": upload_url}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

transcript_id = response.json()["id"]
polling_endpoint = base_url + f"/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()

    if transcript["status"] == "completed":
        break

    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")

    else:
        time.sleep(3)
```

</Tab>

<Tab language="javascript-sdk" title="JavaScript SDK">

```javascript
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// You can use a local filepath:
// const audioFile = "./example.mp3"

// Or use a publicly-accessible URL:
const audioUrl = "https://assembly.ai/sports_injuries.mp3";

const run = async () => {
  const transcript = await client.transcripts.transcribe({ audio: audioUrl });
};

run();
```

</Tab>

<Tab language="javascript" title="JavaScript">

```javascript
import axios from "axios";
import fs from "fs-extra";

const base_url = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL of an audio or video file on the web
};

const response = await axios.post(base_url + "/v2/transcript", data, {
  headers,
});

const transcript_id = response.data.id;
const polling_endpoint = base_url + `/v2/transcript/${transcript_id}`;

while (true) {
  const transcript = (await axios.get(polling_endpoint, { headers })).data;

  if (transcript.status === "completed") {
    break;
  } else if (transcript.status === "error") {
    throw new Error(`Transcription failed: ${transcript.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

</Tab>

<Tab language="csharp" title="C#">

```csharp
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Threading.Tasks;

public class Transcript
{
   public string Id { get; set; }
   public string Status { get; set; }
   public string Text { get; set; }
   [JsonPropertyName("language_code")]
   public string LanguageCode { get; set; }
   public string Error { get; set; }
}

private static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
{
    using (var fileStream = File.OpenRead(filePath))
    using (var fileContent = new StreamContent(fileStream))
    {
        fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");
        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent))
        {
            response.EnsureSuccessStatusCode();
            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }
}
private static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
{
   var data = new { audio_url = audioUrl };
   var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");
   using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
   {
       response.EnsureSuccessStatusCode();
       return await response.Content.ReadFromJsonAsync<Transcript>();
   }
}
private static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
{
   var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";
   while (true)
   {
       var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
       transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();
       switch (transcript.Status)
       {
           case "processing":
           case "queued":
               await Task.Delay(TimeSpan.FromSeconds(3));
               break;
           case "completed":
               return transcript;
           case "error":
               throw new Exception($"Transcription failed: {transcript.Error}");
           default:
               throw new Exception("This code shouldn't be reachable.");
       }
   }
}

using (var httpClient = new HttpClient())
{
   httpClient.DefaultRequestHeaders.Authorization =
       new AuthenticationHeaderValue("<YOUR_API_KEY>");

  var uploadUrl = await UploadFileAsync("/my_audio.mp3", httpClient);
   var transcript = await CreateTranscriptAsync(uploadUrl, httpClient); // You can also replace uploadUrl with an audio file URL
   transcript = await WaitForTranscriptToProcess(transcript, httpClient);

}
```

</Tab>
<Tab language="ruby" title="Ruby">

```ruby
require 'net/http'
require 'json'

base_url = "https://api.assemblyai.com"
headers = {
   "authorization" => "<YOUR_API_KEY>",
   "content-type" => "application/json"
}

path = "/my_audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

uri = URI("#{base_url}/v2/transcript")
request = Net::HTTP::Post.new(uri, headers)
request.body = { audio_url: upload_url }.to_json # You can also replace upload_url with an audio file URL

response = http.request(request)
transcript_id = JSON.parse(response.body)["id"]
polling_endpoint = "#{base_url}/v2/transcript/#{transcript_id}"

while true
   polling_uri = URI(polling_endpoint)
   polling_request = Net::HTTP::Get.new(polling_uri, headers)
   polling_response = http.request(polling_request)
   transcription_result = JSON.parse(polling_response.body)

   if transcription_result["status"] == "completed"
       break
   elsif transcription_result["status"] == "error"
       raise "Transcription failed: #{transcription_result["error"]}"
   else
       sleep(3)
   end
end
```

</Tab>

<Tab language="php" title="PHP">

```php
<?php
$base_url = "https://api.assemblyai.com";
$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);
$path = "/my_audio.mp3";

$ch = curl_init($base_url . "/v2/upload");
curl_setopt_array($ch, [
    CURLOPT_POST => true,
    CURLOPT_POSTFIELDS => file_get_contents($path),
    CURLOPT_HTTPHEADER => $headers,
    CURLOPT_RETURNTRANSFER => true
]);

$response = curl_exec($ch);
$upload_url = json_decode($response, true)["upload_url"];
curl_close($ch);

$ch = curl_init($base_url . "/v2/transcript");
curl_setopt_array($ch, [
    CURLOPT_POST => true,
    CURLOPT_POSTFIELDS => json_encode(["audio_url" => $upload_url]), // You can also replace upload_url with an audio file URL
    CURLOPT_HTTPHEADER => $headers,
    CURLOPT_RETURNTRANSFER => true
]);

$response = curl_exec($ch);
$transcript_id = json_decode($response, true)['id'];
curl_close($ch);

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $ch = curl_init($polling_endpoint);
    curl_setopt_array($ch, [
        CURLOPT_HTTPHEADER => $headers,
        CURLOPT_RETURNTRANSFER => true
    ]);

    $transcription_result = json_decode(curl_exec($ch), true);
    curl_close($ch);

    if ($transcription_result['status'] === "completed") {
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    }
    sleep(3);
}
```

</Tab>

</Tabs>

<Tip title="Use existing transcript">

If you've already transcribed an audio file you want to use, you can get an existing transcript using its ID. You can find the ID for previously transcribed audio files in the <a href="https://www.assemblyai.com/app/processing-queue" target="_blank">Processing queue</a>.

<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

```python
transcript = aai.Transcript.get_by_id("YOUR_TRANSCRIPT_ID")
```

</Tab>

<Tab language="python" title="Python">

```python
transcript = requests.get("https://api.assemblyai.com/v2/transcript/YOUR_TRANSCRIPT_ID", headers=headers).json()
```

</Tab>

<Tab language="javascript-sdk" title="JavaScript SDK">

```javascript
const transcript = await client.transcripts.get("YOUR_TRANSCRIPT_ID");
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
const transcript = (
  await axios.get(
    "https://api.assemblyai.com/v2/transcript/YOUR_TRANSCRIPT_ID",
    { headers }
  )
).data;
```

</Tab>

<Tab language="csharp" title="C#">

```csharp
var transcript = await httpClient.GetFromJsonAsync<Transcript>($"https://api.assemblyai.com/v2/transcript/YOUR_TRANSCRIPT_ID");
```

</Tab>
<Tab language="ruby" title="Ruby">

```ruby
transcript = JSON.parse(Net::HTTP.get(URI("#{base_url}/v2/transcript/YOUR_TRANSCRIPT_ID"), headers))
```

</Tab>

<Tab language="php" title="PHP">

```php
$transcription_result = json_decode(file_get_contents($base_url . "/v2/transcript/YOUR_TRANSCRIPT_ID", false, stream_context_create(['http' => ['header' => implode("\r\n", $headers)]])), true);
```

</Tab>

</Tabs>

</Tip>

## Step 3: Prompt LeMUR to generate text output

In this step, you'll create a [Custom task](https://assemblyai.com/docs/api-reference/lemur/task) with LeMUR and use the transcript you created in the previous step as input.

The input to a custom task is called a _prompt_. A prompt is a text string that provides LeMUR with instructions on how to generate the text output.

For more techniques on how to build prompts, see [Improving your prompt](/docs/lemur/improving-your-prompt).

<Steps>

<Step>

<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

Write a prompt with instructions on how LeMUR should generate the text output.

```python
prompt = "Provide a brief summary of the transcript."
```

</Tab>

<Tab language="python" title="Python">

Write a prompt with instructions on how LeMUR should generate the text output.

```python
prompt = "Provide a brief summary of the transcript."
```

</Tab>

<Tab language="javascript-sdk" title="JavaScript SDK">

Write a prompt with instructions on how LeMUR should generate the text output.

```javascript
const prompt = "Provide a brief summary of the transcript.";
```

</Tab>
<Tab language="javascript" title="JavaScript">

Write a prompt with instructions on how LeMUR should generate the text output.

```javascript
const prompt = "Provide a brief summary of the transcript.";
```

</Tab>

<Tab language="csharp" title="C#">

Write a prompt with instructions on how LeMUR should generate the text output.

```csharp
const string prompt = "Provide a brief summary of the transcript.";
```

</Tab>
<Tab language="ruby" title="Ruby">

Write a prompt with instructions on how LeMUR should generate the text output.

```ruby
prompt = 'Provide a brief summary of the transcript.'
```

</Tab>

<Tab language="php" title="PHP">

Write a prompt with instructions on how LeMUR should generate the text output.

```php
$prompt = 'Provide a brief summary of the transcript.'
```

</Tab>
</Tabs>

</Step>
<Step>

<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

Create a custom task with LeMUR, using the transcript and prompt as input. The final model defines the LLM to use to process the task. For available models to choose from, see [Change the model type](/docs/lemur/customize-parameters#change-the-model-type).

```python
result = transcript.lemur.task(
    prompt, final_model=aai.LemurModel.claude4_sonnet_20250514
)
```

</Tab>

<Tab language="python" title="Python">

Create a custom LeMUR task request, using the transcript and prompt as input. The final model defines the LLM to use to process the task. For available models to choose from, see [Change the model type](/docs/lemur/customize-parameters#change-the-model-type).

```python
lemur_data = {
  "prompt": prompt,
  "transcript_ids": [transcript_id],
  "final_model": "anthropic/claude-sonnet-4-20250514",
}

result = requests.post(base_url + "/lemur/v3/generate/task", headers=headers, json=lemur_data)
```

</Tab>

<Tab language="javascript-sdk" title="JavaScript SDK">

Create a custom task with LeMUR, using the transcript and prompt as input. The final model defines the LLM to use to process the task. For available models to choose from, see [Change the model type](/docs/lemur/customize-parameters#change-the-model-type).

```javascript
const { response } = await client.lemur.task({
  transcript_ids: [transcript.id],
  prompt,
  final_model: "anthropic/claude-sonnet-4-20250514",
});
```

</Tab>
<Tab language="javascript" title="JavaScript">

Create a custom LeMUR task request, using the transcript and prompt as input. The final model defines the LLM to use to process the task. For available models to choose from, see [Change the model type](/docs/lemur/customize-parameters#change-the-model-type).

```javascript
const lemur_data = {
  prompt: prompt,
  transcript_ids: [transcript_id],
  final_model: "anthropic/claude-sonnet-4-20250514",
};

const result = await axios.post(
  base_url + "/lemur/v3/generate/task",
  lemur_data,
  { headers }
);
```

</Tab>

<Tab language="csharp" title="C#">

Create a custom LeMUR task request, using the transcript and prompt as input. The final model defines the LLM to use to process the task. For available models to choose from, see [Change the model type](/docs/lemur/customize-parameters#change-the-model-type).

```csharp
public class LemurResponse
{
  [JsonPropertyName("request_id")]
  public string RequestId { get; set; }

  public string Response { get; set; }

  public Usage Usage { get; set; }
}

public class Usage
{
  [JsonPropertyName("input_tokens")]
  public int InputTokens { get; set; }

  [JsonPropertyName("output_tokens")]
  public int OutputTokens { get; set; }
}

private static async Task<LemurResponse> GenerateTaskAsync(string prompt, List<string> transcriptIds, HttpClient httpClient)
{
   var data = new
   {
       transcript_ids = transcriptIds,
       prompt,
       final_model = "anthropic/claude-sonnet-4-20250514"
   };
   var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

   using var response = await httpClient.PostAsync("https://api.assemblyai.com/lemur/v3/generate/task", content);
   response.EnsureSuccessStatusCode();
   return await response.Content.ReadFromJsonAsync<LemurResponse>();
}

var transcriptIds = new List<string> { transcript.Id };
var lemurResponse = await GenerateTaskAsync(prompt, transcriptIds, httpClient);
```

</Tab>

<Tab language="ruby" title="Ruby">

Create a custom LeMUR task request, using the transcript and prompt as input. The final model defines the LLM to use to process the task. For available models to choose from, see [Change the model type](/docs/lemur/customize-parameters#change-the-model-type).

```ruby
lemur_uri = URI("#{base_url}/lemur/v3/generate/task")
lemur_request = Net::HTTP::Post.new(lemur_uri, headers)
lemur_request.body = {
  final_model: "anthropic/claude-sonnet-4-20250514",
  prompt: prompt,
  transcript_ids: [transcript_id]
}.to_json

lemur_response = http.request(lemur_request)
lemur_result = JSON.parse(lemur_response.body)
```

</Tab>

<Tab language="php" title="PHP">

Create a custom LeMUR task request, using the transcript and prompt as input. The final model defines the LLM to use to process the task. For available models to choose from, see [Change the model type](/docs/lemur/customize-parameters#change-the-model-type).

```php
$ch = curl_init($base_url . "/lemur/v3/generate/task");
curl_setopt_array($ch, [
    CURLOPT_RETURNTRANSFER => true,
    CURLOPT_POST => true,
    CURLOPT_HTTPHEADER => $headers,
    CURLOPT_POSTFIELDS => json_encode([
        'final_model' => 'anthropic/claude-sonnet-4-20250514',
        'prompt' => $prompt,
        'transcript_ids' => [$transcript_id]
    ])
]);

$response = curl_exec($ch);
$result = json_decode($response, true);
```

</Tab>

</Tabs>
</Step>

<Step>

Print the result.

<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

```python
print(result.response)
```

</Tab>

<Tab language="python" title="Python">

```python
print(result.json()["response"])
```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">

```javascript
console.log(response);
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
console.log(result.data.response);
```

</Tab>

<Tab language="csharp" title="C#">

```csharp
Console.WriteLine(lemurResponse.Response);
```

</Tab>

<Tab language="ruby" title="Ruby">

```ruby
puts lemur_result["response"]
```

</Tab>

<Tab language="php" title="PHP">

```php
echo $result['response'];
curl_close($ch);
```

</Tab>

</Tabs>

The output will look something like this:

```
 The transcript describes several common sports injuries - runner's knee,
 sprained ankle, meniscus tear, rotator cuff tear, and ACL tear. It provides
 definitions, causes, and symptoms for each injury. The transcript seems to be
 narrating sports footage and describing injuries as they occur to the athletes.
 Overall, it provides an overview of these common sports injuries that can
 result from overuse or sudden trauma during athletic activities
```

</Step>

</Steps>

## Next steps

In this tutorial, you've learned how to generate LLM output based on your audio transcripts. The type of output depends on your prompt, so try exploring different prompts to see how they affect the output. Here's a few more prompts to try.

- "Provide an analysis of the transcript and offer areas to improve with exact quotes."
- "What's the main take-away from the transcript?"
- "Generate a set of action items from this transcript."

To learn more about how to apply LLMs to your transcripts, see the following resources:

- [Ask questions about your audio data using LeMUR](/docs/lemur/ask-questions)
- [Writing good prompts](/docs/lemur/improving-your-prompt)

## Need some help?

If you get stuck, or have any other questions, we'd love to help you out. Contact our support team at support@assemblyai.com or create a [support ticket](https://www.assemblyai.com/contact/support).

---
title: Summarize Your Audio Data
subtitle: >-
  In this guide, you'll learn how to use LeMUR to summarize your audio data with
  key takeaways.
description: Summarize your audio files with Large Language Models.
---

<Tip>

If you want a Quickstart, see [Apply LLMs to audio files](/docs/lemur/apply-llms-to-audio-files).

</Tip>

<Note title="Before you start">

To use LeMUR, you need an <a href="https://www.assemblyai.com/dashboard/signup" target="_blank">AssemblyAI account</a> with a credit card set up.

</Note>

## Basic summary example

If you want to send a custom prompt to the LLM, you can use the [LeMUR Task](https://assemblyai.com/docs/api-reference/lemur/task) endpoint to apply the model to your transcribed audio files.

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>
  
```python {14-15,17-20}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# Step 1: Transcribe an audio file.

# audio_file = "./local_file.mp3"

audio_file = "https://assembly.ai/sports_injuries.mp3"

transcriber = aai.Transcriber()
transcript = transcriber.transcribe(audio_file)

# Step 2: Define a summarization prompt.
prompt = "Provide a brief summary of the transcript."

# Step 3: Apply LeMUR.
result = transcript.lemur.task(
    prompt, final_model=aai.LemurModel.claude4_sonnet_20250514
)

print(result.response)
```

</Tab>
<Tab language="python" title="Python">

```python {40-41,43-50}
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
  "authorization": "<YOUR_API_KEY>"
}

# Step 1: Transcribe an audio file.
# with open("./my-audio.mp3", "rb") as f:
# response = requests.post(base_url + "/v2/upload",
#                         headers=headers,
#                         data=f)
# upload_url = response.json()["upload_url"]

upload_url = "https://assembly.ai/sports_injuries.mp3"

data = {
  "audio_url": upload_url
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

transcript_id = response.json()["id"]
polling_endpoint = base_url + f"/v2/transcript/{transcript_id}"

while True:
  transcript = requests.get(polling_endpoint, headers=headers).json()

  if transcript["status"] == "completed":
    break

  elif transcript["status"] == "error":
    raise RuntimeError(f"Transcription failed: {transcript['error']}")

  else:
    time.sleep(3)

# Step 2: Define a summarization prompt.
prompt = "Provide a brief summary of the transcript."

# Step 3: Apply LeMUR.
lemur_data = {
  "prompt": prompt,
  "transcript_ids": [transcript_id],
  "final_model": "anthropic/claude-sonnet-4-20250514",
}

result = requests.post(base_url + "/lemur/v3/generate/task", headers=headers, json=lemur_data)

print(result.json()["response"])
```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">
  
```javascript {13-14,16-21}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

const run = async () => {
  // Step 1: Transcribe an audio file.
  //const audioFile = './local_file.mp3'
  const audioFile = "https://assembly.ai/sports_injuries.mp3";
  const transcript = await client.transcripts.transcribe({ audio: audioFile });

  // Step 2: Define a summarization prompt.
  const prompt = "Provide a brief summary of the transcript.";

  // Step 3: Apply LeMUR.
  const { response } = await client.lemur.task({
    transcript_ids: [transcript.id],
    prompt,
    final_model: "anthropic/claude-sonnet-4-20250514",
  });

  console.log(response);
};

run();
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript {40-41,43-50}
import axios from "axios";
import fs from "fs-extra";

const base_url = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

// Step 1: Transcribe an audio file.
const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL of an audio or video file on the web
};

const response = await axios.post(base_url + "/v2/transcript", data, {
  headers,
});

const transcript_id = response.data.id;
const polling_endpoint = base_url + `/v2/transcript/${transcript_id}`;

while (true) {
  const transcript = (await axios.get(polling_endpoint, { headers })).data;

  if (transcript.status === "completed") {
    break;
  } else if (transcript.status === "error") {
    throw new Error(`Transcription failed: ${transcript.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}

// Step 2: Define a summarization prompt.
const prompt = "Provide a brief summary of the transcript.";

// Step 3: Apply LeMUR.
const lemur_data = {
  prompt: prompt,
  transcript_ids: [transcript_id],
  final_model: "anthropic/claude-sonnet-4-20250514",
};

const result = await axios.post(
  base_url + "/lemur/v3/generate/task",
  lemur_data,
  { headers }
);

console.log(result.data.response);
```

</Tab>
<Tab language="csharp" title="C#">
  
```csharp {120-121,123-125}
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Threading.Tasks;

public class Transcript
{
public string Id { get; set; }
public string Status { get; set; }
public string Text { get; set; }

[JsonPropertyName("language_code")]
public string LanguageCode { get; set; }

public string Error { get; set; }
}

public class LemurResponse
{
[JsonPropertyName("request_id")]
public string RequestId { get; set; }

public string Response { get; set; }

public Usage Usage { get; set; }
}

public class Usage
{
[JsonPropertyName("input_tokens")]
public int InputTokens { get; set; }

[JsonPropertyName("output_tokens")]
public int OutputTokens { get; set; }
}

private static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
{
using (var fileStream = File.OpenRead(filePath))
using (var fileContent = new StreamContent(fileStream))
{
fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent))
        {
            response.EnsureSuccessStatusCode();
            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }

}

private static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
{
var data = new { audio_url = audioUrl };
var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
{
response.EnsureSuccessStatusCode();
return await response.Content.ReadFromJsonAsync<Transcript>();
}
}

private static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
{
var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

while (true)
{
var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();
switch (transcript.Status)
{
case "processing":
case "queued":
await Task.Delay(TimeSpan.FromSeconds(3));
break;
case "completed":
return transcript;
case "error":
throw new Exception($"Transcription failed: {transcript.Error}");
default:
throw new Exception("This code shouldn't be reachable.");
}
}
}

private static async Task<LemurResponse> GenerateTaskAsync(string prompt, List<string> transcriptIds, HttpClient httpClient)
{
var data = new
{
transcript_ids = transcriptIds,
prompt,
final_model = "anthropic/claude-sonnet-4-20250514"
};

var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

using var response = await httpClient.PostAsync("https://api.assemblyai.com/lemur/v3/generate/task", content);
response.EnsureSuccessStatusCode();
return await response.Content.ReadFromJsonAsync<LemurResponse>();
}

using (var httpClient = new HttpClient())
{
httpClient.DefaultRequestHeaders.Authorization =
new AuthenticationHeaderValue("<YOUR_API_KEY>");

// Step 1: Transcribe an audio file.
var uploadUrl = await UploadFileAsync("/my_audio.mp3", httpClient);
var transcript = await CreateTranscriptAsync(uploadUrl, httpClient); // You can also replace uploadUrl with an audio file URL
transcript = await WaitForTranscriptToProcess(transcript, httpClient);

// Step 2: Define a summarization prompt.
const string prompt = "Provide a brief summary of the transcript.";

//Step 3: Apply LeMUR.
var transcriptIds = new List<string> { transcript.Id };
var lemurResponse = await GenerateTaskAsync(prompt, transcriptIds, httpClient);

Console.WriteLine(lemurResponse.Response);
}
```

</Tab>
<Tab language="ruby" title="Ruby">

```ruby {44-45,47-56}
require 'net/http'
require 'json'

base_url = "https://api.assemblyai.com"
headers = {
   "authorization" => "<YOUR_API_KEY>",
   "content-type" => "application/json"
}

# Step 1: Transcribe an audio file.
path = "/my_audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

uri = URI("#{base_url}/v2/transcript")
request = Net::HTTP::Post.new(uri, headers)
request.body = { audio_url: upload_url }.to_json # You can also replace upload_url with an audio file URL

response = http.request(request)
transcript_id = JSON.parse(response.body)["id"]
polling_endpoint = "#{base_url}/v2/transcript/#{transcript_id}"

while true
   polling_uri = URI(polling_endpoint)
   polling_request = Net::HTTP::Get.new(polling_uri, headers)
   polling_response = http.request(polling_request)
   transcription_result = JSON.parse(polling_response.body)

   if transcription_result["status"] == "completed"
       break
   elsif transcription_result["status"] == "error"
       raise "Transcription failed: #{transcription_result["error"]}"
   else
       sleep(3)
   end
end

# Step 2: Define a summarization prompt.
prompt = "Provide a brief summary of the transcript."

# Step 3: Apply LeMUR.
lemur_uri = URI("#{base_url}/lemur/v3/generate/task")
lemur_request = Net::HTTP::Post.new(lemur_uri, headers)
lemur_request.body = {
  final_model: "anthropic/claude-sonnet-4-20250514",
  prompt: prompt,
  transcript_ids: [transcript_id]
}.to_json

lemur_response = http.request(lemur_request)

lemur_result = JSON.parse(lemur_response.body)
puts lemur_result["response"]
```

</Tab>
<Tab language="php" title="PHP">

```php {54-55,57-70}
<?php
$base_url = "https://api.assemblyai.com";
$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);
$path = "/my_audio.mp3";

// Step 1: Transcribe an audio file.
$ch = curl_init($base_url . "/v2/upload");
curl_setopt_array($ch, [
    CURLOPT_POST => true,
    CURLOPT_POSTFIELDS => file_get_contents($path),
    CURLOPT_HTTPHEADER => $headers,
    CURLOPT_RETURNTRANSFER => true
]);

$response = curl_exec($ch);
$upload_url = json_decode($response, true)["upload_url"];
curl_close($ch);

$ch = curl_init($base_url . "/v2/transcript");
curl_setopt_array($ch, [
    CURLOPT_POST => true,
    CURLOPT_POSTFIELDS => json_encode(["audio_url" => $upload_url]), // You can also replace upload_url with an audio file URL
    CURLOPT_HTTPHEADER => $headers,
    CURLOPT_RETURNTRANSFER => true
]);

$response = curl_exec($ch);
$transcript_id = json_decode($response, true)['id'];
curl_close($ch);

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $ch = curl_init($polling_endpoint);
    curl_setopt_array($ch, [
        CURLOPT_HTTPHEADER => $headers,
        CURLOPT_RETURNTRANSFER => true
    ]);

    $transcription_result = json_decode(curl_exec($ch), true);
    curl_close($ch);

    if ($transcription_result['status'] === "completed") {
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    }
    sleep(3);
}

// Step 2: Define a summarization prompt.
$prompt = 'Provide a brief summary of the transcript.'

// Step 3: Apply LeMUR.
$ch = curl_init($base_url . "/lemur/v3/generate/task");
curl_setopt_array($ch, [
    CURLOPT_RETURNTRANSFER => true,
    CURLOPT_POST => true,
    CURLOPT_HTTPHEADER => $headers,
    CURLOPT_POSTFIELDS => json_encode([
        'final_model' => 'anthropic/claude-sonnet-4-20250514',
        'prompt' => $prompt,
        'transcript_ids' => [$transcript_id]
    ])
]);

$response = curl_exec($ch);
$result = json_decode($response, true);
echo $result['response'];
curl_close($ch);
```

</Tab>
</Tabs>

#### Example output

```plain
The transcript describes several common sports injuries - runner's knee,
sprained ankle, meniscus tear, rotator cuff tear, and ACL tear. It provides
definitions, causes, and symptoms for each injury. The transcript seems to be
narrating sports footage and describing injuries as they occur to the athletes.
Overall, it provides an overview of these common sports injuries that can result
from overuse or sudden trauma during athletic activities
```

## Summary with specialized endpoint

The [LeMUR Summary endpoint](https://www.assemblyai.com/docs/api-reference/lemur/summary) requires no prompt engineering and facilitates more deterministic and structured outputs. See the code examples below for more information on how to use this endpoint.

You can add additional context to provide information that is not explicitly referenced in the audio data, as well as specify an answer format. For this, use the optional parameters `context` and `answer_format`.

<Tabs groupId="language">
<Tab value="python-sdk" title="Python SDK" default>

```python {8-12}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

audio_url = "https://assembly.ai/meeting.mp4"
transcript = aai.Transcriber().transcribe(audio_url)

result = transcript.lemur.summarize(
    final_model=aai.LemurModel.claude4_sonnet_20250514,
    context="A GitLab meeting to discuss logistics",
    answer_format="TLDR"
)

print(result.response)
```

</Tab>
<Tab language="python" title="Python">

```python {33-40}
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
  "authorization": "<YOUR_API_KEY>"
}

upload_url = "https://assembly.ai/meeting.mp4"

data = {
  "audio_url": upload_url
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

transcript_id = response.json()["id"]
polling_endpoint = base_url + f"/v2/transcript/{transcript_id}"

while True:
  transcript = requests.get(polling_endpoint, headers=headers).json()

  if transcript["status"] == "completed":
    break

  elif transcript["status"] == "error":
    raise RuntimeError(f"Transcription failed: {transcript['error']}")

  else:
    time.sleep(3)

lemur_data = {
  "transcript_ids": [transcript_id],
  "final_model": "anthropic/claude-sonnet-4-20250514",
  "context": "A GitLab meeting to discuss logistics",
  "answer_format": "TLDR"
}

result = requests.post(base_url + "/lemur/v3/generate/summary", headers=headers, json=lemur_data)

print(result.json()["response"])
```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">
  
```javascript {12-17}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

const audioUrl = "https://assembly.ai/meeting.mp4";

const run = async () => {
  const transcript = await client.transcripts.transcribe({ audio: audioUrl });

  const { response } = await client.lemur.summary({
    transcript_ids: [transcript.id],
    final_model: "anthropic/claude-sonnet-4-20250514",
    context: "A GitLab meeting to discuss logistics",
    answer_format: "TLDR",
  });

  console.log(response);
};

run();
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript {33-40}
import axios from "axios";

const base_url = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const uploadUrl = "https://assembly.ai/meeting.mp4";

const data = {
  audio_url: uploadUrl,
};

const response = await axios.post(base_url + "/v2/transcript", data, {
  headers,
});

const transcript_id = response.data.id;
const polling_endpoint = base_url + `/v2/transcript/${transcript_id}`;

while (true) {
  const transcript = (await axios.get(polling_endpoint, { headers })).data;

  if (transcript.status === "completed") {
    break;
  } else if (transcript.status === "error") {
    throw new Error(`Transcription failed: ${transcript.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}

const lemur_data = {
  transcript_ids: [transcript_id],
  final_model: "anthropic/claude-sonnet-4-20250514",
  context: "A GitLab meeting to discuss logistics",
  answer_format: "TLDR",
};

const result = await axios.post(
  base_url + "/lemur/v3/generate/summary",
  lemur_data,
  { headers }
);

console.log(result.data.response);
```

</Tab>
<Tab language="csharp" title="C#">
  
```csharp {78-87, 105}
using System;
using System.Collections.Generic;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Program
{
public class Transcript
{
public string Id { get; set; }
public string Status { get; set; }
public string Text { get; set; }
[JsonPropertyName("language_code")]
public string LanguageCode { get; set; }
public string Error { get; set; }
}

    public class LemurResponse
    {
        [JsonPropertyName("request_id")]
        public string RequestId { get; set; }

        public string Response { get; set; }

        public Usage Usage { get; set; }
    }

    public class Usage
    {
        [JsonPropertyName("input_tokens")]
        public int InputTokens { get; set; }

        [JsonPropertyName("output_tokens")]
        public int OutputTokens { get; set; }
    }

    private static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new { audio_url = audioUrl };
        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");
        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    private static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";
        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();
            switch (transcript.Status)
            {
                case "processing":
                case "queued":
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("This code shouldn't be reachable.");
            }
        }
    }

    private static async Task<LemurResponse> GenerateSummaryAsync(List<string> transcriptIds, HttpClient httpClient)
    {
        var data = new
        {
            transcript_ids = transcriptIds,
            final_model = "anthropic/claude-sonnet-4-20250514",
            context = "A GitLab meeting to discuss logistics",
            answer_format = "TLDR"
        };
        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using var response = await httpClient.PostAsync("https://api.assemblyai.com/lemur/v3/generate/summary", content);
        response.EnsureSuccessStatusCode();
        return await response.Content.ReadFromJsonAsync<LemurResponse>();
    }

    public static async Task Main()
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Authorization =
                new AuthenticationHeaderValue("<YOUR_API_KEY>");

            string audioUrl = "https://assembly.ai/meeting.mp4";

            var transcript = await CreateTranscriptAsync(audioUrl, httpClient);
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            var transcriptIds = new List<string> { transcript.Id };
            var lemurResponse = await GenerateSummaryAsync(transcriptIds, httpClient);
            Console.WriteLine(lemurResponse.Response);
        }
    }

}
```

</Tab>
<Tab language="ruby" title="Ruby">

```ruby {35-45}
require 'net/http'
require 'json'

base_url = "https://api.assemblyai.com"
headers = {
   "authorization" => "<YOUR_API_KEY>",
   "content-type" => "application/json"
}

upload_url = "https://assembly.ai/meeting.mp4"

uri = URI("#{base_url}/v2/transcript")
request = Net::HTTP::Post.new(uri, headers)
request.body = { audio_url: upload_url }.to_json

response = http.request(request)
transcript_id = JSON.parse(response.body)["id"]
polling_endpoint = "#{base_url}/v2/transcript/#{transcript_id}"

while true
   polling_uri = URI(polling_endpoint)
   polling_request = Net::HTTP::Get.new(polling_uri, headers)
   polling_response = http.request(polling_request)
   transcription_result = JSON.parse(polling_response.body)

   if transcription_result["status"] == "completed"
       break
   elsif transcription_result["status"] == "error"
       raise "Transcription failed: #{transcription_result["error"]}"
   else
       sleep(3)
   end
end

lemur_uri = URI("#{base_url}/lemur/v3/generate/summary")
lemur_request = Net::HTTP::Post.new(lemur_uri, headers)
lemur_request.body = {
  final_model: "anthropic/claude-sonnet-4-20250514",
  transcript_ids: [transcript_id],
  context: "A GitLab meeting to discuss logistics",
  answer_format: "TLDR"

}.to_json

lemur_response = http.request(lemur_request)

lemur_result = JSON.parse(lemur_response.body)
puts lemur_result["response"]
```

</Tab>
<Tab language="php" title="PHP">

```php {42-53}
<?php
$base_url = "https://api.assemblyai.com";
$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$upload_url = "https://assembly.ai/meeting.mp4";

$ch = curl_init($base_url . "/v2/transcript");
curl_setopt_array($ch, [
    CURLOPT_POST => true,
    CURLOPT_POSTFIELDS => json_encode(["audio_url" => $upload_url]),
    CURLOPT_HTTPHEADER => $headers,
    CURLOPT_RETURNTRANSFER => true
]);

$response = curl_exec($ch);
$transcript_id = json_decode($response, true)['id'];
curl_close($ch);

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $ch = curl_init($polling_endpoint);
    curl_setopt_array($ch, [
        CURLOPT_HTTPHEADER => $headers,
        CURLOPT_RETURNTRANSFER => true
    ]);

    $transcription_result = json_decode(curl_exec($ch), true);
    curl_close($ch);

    if ($transcription_result['status'] === "completed") {
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    }
    sleep(3);
}

$ch = curl_init($base_url . "/lemur/v3/generate/summary");
curl_setopt_array($ch, [
    CURLOPT_RETURNTRANSFER => true,
    CURLOPT_POST => true,
    CURLOPT_HTTPHEADER => $headers,
    CURLOPT_POSTFIELDS => json_encode([
        'final_model' => 'anthropic/claude-sonnet-4-20250514',
        'transcript_ids' => [$transcript_id],
        'context' => 'A GitLab meeting to discuss logistics',
        'answer_format' => 'TLDR'
    ])
]);

$response = curl_exec($ch);
$result = json_decode($response, true);
echo $result['response'];
curl_close($ch);
```

</Tab>
</Tabs>

## Custom summary example (Advanced)

In this example, we'll run a custom LeMUR task with an advanced prompt to create custom summaries:

<Card
  icon="book"
  title="Cookbook: Custom summary with LeMUR Task"
  href="https://www.assemblyai.com/docs/guides/task-endpoint-custom-summary"
/>

## More summarization prompt examples

Try any of these prompts to get started:

| Use case                 | Example prompt                                                                       |
| ------------------------ | ------------------------------------------------------------------------------------ |
| Summaries                | <i>"Summarize key decisions and important points from the phone call transcript"</i> |
| Summarize audio segments | <i>"Summarize the key events of each chapter"</i>                                    |

For more use cases and prompt examples, see [LeMUR examples](/docs/lemur/examples).

## API reference

- [LeMUR Task endpoint](https://assemblyai.com/docs/api-reference/lemur/task)
- [LeMUR Summary endpoint](https://www.assemblyai.com/docs/api-reference/lemur/summary)

## Improve the results

To improve the results, see the following resources:

- Optimize your prompt with the [prompt engineering guide](/docs/lemur/improving-your-prompt).
- To alter the outcome, see [Change model and parameters](/docs/lemur/customize-parameters).


---
title: Ask Questions About Your Audio Data
description: Ask questions about your audio data with Large Language Models.
---

In this guide, you'll learn how to use LeMUR to ask questions and get answers about your audio data.

<Tip>

If you want a Quickstart, see [Apply LLMs to audio files](/docs/lemur/apply-llms-to-audio-files).

</Tip>

<Note title="Before you start">

To use LeMUR, you need an <a href="https://www.assemblyai.com/dashboard/signup" target="_blank"> AssemblyAI account </a> with a credit card set up.

</Note>

## Basic Q&A example

To ask a question about your audio data, define a prompt with your questions and call the [LeMUR Task](https://assemblyai.com/docs/api-reference/lemur/task) endpoint. The underlying transcript is used as additional context for the model.

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>
  
```python {14-15,17-19}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# Step 1: Transcribe an audio file.

# audio_file = "./local_file.mp3"

audio_file = "https://assembly.ai/sports_injuries.mp3"

transcriber = aai.Transcriber()
transcript = transcriber.transcribe(audio_file)

# Step 2: Define a prompt with your question(s).
prompt = "What is a runner's knee?"

# Step 3: Apply LeMUR.

result = transcript.lemur.task(prompt, final_model=aai.LemurModel.claude4_sonnet_20250514)

print(result.response)
```

</Tab>
<Tab language="python" title="Python">

```python {42-43,45-53}
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
  "authorization": "<YOUR_API_KEY>"
}

# Step 1: Transcribe an audio file.
# You can use a local filepath:
# with open("./my-audio.mp3", "rb") as f:
# response = requests.post(base_url + "/v2/upload",
#                         headers=headers,
#                         data=f)
# upload_url = response.json()["upload_url"]

# Or use a publicly-accessible URL:
upload_url = "https://assembly.ai/sports_injuries.mp3"

data = {
  "audio_url": upload_url
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

transcript_id = response.json()["id"]
polling_endpoint = base_url + f"/v2/transcript/{transcript_id}"

while True:
  transcript = requests.get(polling_endpoint, headers=headers).json()

  if transcript["status"] == "completed":
    break

  elif transcript["status"] == "error":
    raise RuntimeError(f"Transcription failed: {transcript['error']}")

  else:
    time.sleep(3)

# Step 2: Define a prompt with your question(s).
prompt = "What is a runner's knee?"

# Step 3: Apply LeMUR.
lemur_data = {
  "prompt": prompt,
  "transcript_ids": [transcript_id],
  "final_model": "anthropic/claude-sonnet-4-20250514",
}

result = requests.post(base_url + "/lemur/v3/generate/task", headers=headers, json=lemur_data)
print(result.json()["response"])
```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">

```javascript {13-14,16-21, 23}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

const run = async () => {
  // Step 1: Transcribe an audio file.
  //const audioFile = './local_file.mp3'
  const audioFile = "https://assembly.ai/sports_injuries.mp3";
  const transcript = await client.transcripts.transcribe({ audio: audioFile });

  // Step 2: Define a prompt with your question(s).
  const prompt = "What is a runner's knee?";

  // Step 3: Apply LeMUR.
  const { response } = await client.lemur.task({
    transcript_ids: [transcript.id],
    prompt,
    final_model: "anthropic/claude-sonnet-4-20250514",
  });

  console.log(response);
};

run();
```

</Tab>
<Tab language="javascript" title="JavaScript">
  
```javascript {44-45,47-52}
import axios from "axios";

const base_url = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

// Step 1: Transcribe an audio file.
// import fs from 'fs-extra';
// const path = './my-audio.mp3';
// const audioData = await fs.readFile(path)
// const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
// headers
// })
// const uploadUrl = uploadResponse.data.upload_url

// Or use a publicly-accessibly URL:
const uploadUrl = "https://assembly.ai/sports_injuries.mp3";

const data = {
  audio_url: uploadUrl,
};

const response = await axios.post(base_url + "/v2/transcript", data, {
  headers,
});

const transcript_id = response.data.id;
const polling_endpoint = base_url + `/v2/transcript/${transcript_id}`;

while (true) {
  const transcript = (await axios.get(polling_endpoint, { headers })).data;

  if (transcript.status === "completed") {
    break;
  } else if (transcript.status === "error") {
    throw new Error(`Transcription failed: ${transcript.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}

// Step 2: Define a prompt with your question(s).
const prompt = "What is a runner's knee?";

// Step 3: Apply LeMUR.
const lemur_data = {
  prompt: prompt,
  transcript_ids: [transcript_id],
  final_model: "anthropic/claude-sonnet-4-20250514",
};

const result = await axios.post(
  base_url + "/lemur/v3/generate/task",
  lemur_data,
  { headers }
);
console.log(result.data.response);
```
</Tab>

<Tab language="csharp" title="C#">

```csharp {122-123,125-127}
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Threading.Tasks;

public class Transcript
{
   public string Id { get; set; }
   public string Status { get; set; }
   public string Text { get; set; }

   [JsonPropertyName("language_code")]
   public string LanguageCode { get; set; }

   public string Error { get; set; }
}

public class LemurResponse
{
  [JsonPropertyName("request_id")]
  public string RequestId { get; set; }

  public string Response { get; set; }

  public Usage Usage { get; set; }
}

public class Usage
{
  [JsonPropertyName("input_tokens")]
  public int InputTokens { get; set; }

  [JsonPropertyName("output_tokens")]
  public int OutputTokens { get; set; }
}

private static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
{
    using (var fileStream = File.OpenRead(filePath))
    using (var fileContent = new StreamContent(fileStream))
    {
        fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent))
        {
            response.EnsureSuccessStatusCode();
            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }
}


private static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
{
   var data = new { audio_url = audioUrl };
   var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

   using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
   {
       response.EnsureSuccessStatusCode();
       return await response.Content.ReadFromJsonAsync<Transcript>();
   }
}

private static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
{
   var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

   while (true)
   {
       var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
       transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();
       switch (transcript.Status)
       {
           case "processing":
           case "queued":
               await Task.Delay(TimeSpan.FromSeconds(3));
               break;
           case "completed":
               return transcript;
           case "error":
               throw new Exception($"Transcription failed: {transcript.Error}");
           default:
               throw new Exception("This code shouldn't be reachable.");
       }
   }
}

private static async Task<LemurResponse> GenerateTaskAsync(string prompt, List<string> transcriptIds, HttpClient httpClient)
{
   var data = new
   {
       transcript_ids = transcriptIds,
       prompt,
       final_model = "anthropic/claude-sonnet-4-20250514"
   };

   var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

   using var response = await httpClient.PostAsync("https://api.assemblyai.com/lemur/v3/generate/task", content);
   response.EnsureSuccessStatusCode();
   return await response.Content.ReadFromJsonAsync<LemurResponse>();
}

using (var httpClient = new HttpClient())
{
   httpClient.DefaultRequestHeaders.Authorization =
       new AuthenticationHeaderValue("<YOUR_API_KEY>");

   // Step 1: Transcribe an audio file.
   // var uploadUrl = await UploadFileAsync("/my_audio.mp3", httpClient);
   // Or use a publicly-accessible URL.
   var uploadUrl = "https://assembly.ai/sports_injuries.mp3";
   var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);
   transcript = await WaitForTranscriptToProcess(transcript, httpClient);

   // Step 2: Define a prompt with your question(s).
   const string prompt = "What is a runner's knee?";

   // Step 3: Apply LeMUR.
   var transcriptIds = new List<string> { transcript.Id };
   var lemurResponse = await GenerateTaskAsync(prompt, transcriptIds, httpClient);

   Console.WriteLine(lemurResponse.Response);
}
```

</Tab>
<Tab language="ruby" title="Ruby">

```ruby {47-48,50-59}
require 'net/http'
require 'json'

base_url = "https://api.assemblyai.com"
headers = {
   "authorization" => "<YOUR_API_KEY>",
   "content-type" => "application/json"
}

# Step 1: Transcribe an audio file.
# path = "/my_audio.mp3"
# uri = URI("#{base_url}/v2/upload")
# request = Net::HTTP::Post.new(uri, headers)
# request.body = File.read(path)

# http = Net::HTTP.new(uri.host, uri.port)
# http.use_ssl = true
# upload_response = http.request(request)
# upload_url = JSON.parse(upload_response.body)["upload_url"]

# Or use a publicly-accessible URL:
upload_url = "https://assembly.ai/sports_injuries.mp3"

uri = URI("#{base_url}/v2/transcript")
request = Net::HTTP::Post.new(uri, headers)
request.body = { audio_url: upload_url }.to_json

response = http.request(request)
transcript_id = JSON.parse(response.body)["id"]
polling_endpoint = "#{base_url}/v2/transcript/#{transcript_id}"

while true
   polling_uri = URI(polling_endpoint)
   polling_request = Net::HTTP::Get.new(polling_uri, headers)
   polling_response = http.request(polling_request)
   transcription_result = JSON.parse(polling_response.body)

   if transcription_result["status"] == "completed"
       break
   elsif transcription_result["status"] == "error"
       raise "Transcription failed: #{transcription_result["error"]}"
   else
       sleep(3)
   end
end

# Step 2: Define a prompt with your question(s).
prompt = "What is a runner's knee?"

# Step 3: Apply LeMUR.
lemur_uri = URI("#{base_url}/lemur/v3/generate/task")
lemur_request = Net::HTTP::Post.new(lemur_uri, headers)
lemur_request.body = {
  final_model: "anthropic/claude-sonnet-4-20250514",
  prompt: prompt,
  transcript_ids: [transcript_id]
}.to_json

lemur_response = http.request(lemur_request)
lemur_result = JSON.parse(lemur_response.body)
puts lemur_result["response"]
```

</Tab>
<Tab language="php" title="PHP">

```php {57-58,60-73}
<?php
$base_url = "https://api.assemblyai.com";
$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

// Step 1: Transcribe an audio file.
// $path = "/my_audio.mp3";
// $ch = curl_init($base_url . "/v2/upload");
// curl_setopt_array($ch, [
//     CURLOPT_POST => true,
//     CURLOPT_POSTFIELDS => file_get_contents($path),
//     CURLOPT_HTTPHEADER => $headers,
//     CURLOPT_RETURNTRANSFER => true
// ]);

// $response = curl_exec($ch);
// $upload_url = json_decode($response, true)["upload_url"];
// curl_close($ch);

// Or use a publicly-accessible URL:
$upload_url = "https://assembly.ai/sports_injuries.mp3"

$ch = curl_init($base_url . "/v2/transcript");
curl_setopt_array($ch, [
    CURLOPT_POST => true,
    CURLOPT_POSTFIELDS => json_encode(["audio_url" => $upload_url]), // You can also replace upload_url with an audio file URL
    CURLOPT_HTTPHEADER => $headers,
    CURLOPT_RETURNTRANSFER => true
]);

$response = curl_exec($ch);
$transcript_id = json_decode($response, true)['id'];
curl_close($ch);

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $ch = curl_init($polling_endpoint);
    curl_setopt_array($ch, [
        CURLOPT_HTTPHEADER => $headers,
        CURLOPT_RETURNTRANSFER => true
    ]);

    $transcription_result = json_decode(curl_exec($ch), true);
    curl_close($ch);

    if ($transcription_result['status'] === "completed") {
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    }
    sleep(3);
}

// Step 2: Define a prompt with your question(s).
$prompt = "What is a runner's knee?";

// Step 3: Apply LeMUR.
$ch = curl_init($base_url . "/lemur/v3/generate/task");
curl_setopt_array($ch, [
    CURLOPT_RETURNTRANSFER => true,
    CURLOPT_POST => true,
    CURLOPT_HTTPHEADER => $headers,
    CURLOPT_POSTFIELDS => json_encode([
        'final_model' => 'anthropic/claude-sonnet-4-20250514',
        'prompt' => $prompt,
        'transcript_ids' => [$transcript_id]
    ])
]);

$response = curl_exec($ch);
$result = json_decode($response, true);
echo $result['response'];
curl_close($ch);
```

</Tab>
</Tabs>

#### Example output

```plain
Based on the transcript, runner's knee is a condition characterized
by pain behind or around the kneecap. It is caused by overuse,
muscle imbalance and inadequate stretching. Symptoms include pain
under or around the kneecap and pain when walking.
```

## Q&A with specialized endpoint

The [LeMUR Question & Answer function](https://www.assemblyai.com/docs/api-reference/lemur/question-answer) requires no prompt engineering and facilitates more deterministic and structured outputs. See the code examples below for more information on how to use this endpoint.

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>

```python {8-16,18-22}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

audio_url = "https://assembly.ai/meeting.mp4"
transcript = aai.Transcriber().transcribe(audio_url)

questions = [
    aai.LemurQuestion(
        question="What are the top level KPIs for engineering?",
        context="KPI stands for key performance indicator",
        answer_format="short sentence"),
    aai.LemurQuestion(
        question="How many days has it been since the data team has gotten updated metrics?",
        answer_options=["1", "2", "3", "4", "5", "6", "7", "more than 7"]),
]

result = transcript.lemur.question(
    final_model=aai.LemurModel.claude4_sonnet_20250514,
    questions,
    context="A GitLab meeting to discuss logistics"
)

for qa_response in result.response:
    print(f"Question: {qa_response.question}")
    print(f"Answer: {qa_response.answer}")
```

</Tab>
<Tab language="python" title="Python">

```python {33-41, 43-49}
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
  "authorization": "<YOUR_API_KEY>"
}

upload_url = "https://assembly.ai/meeting.mp4"

data = {
  "audio_url": upload_url
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

transcript_id = response.json()["id"]
polling_endpoint = base_url + f"/v2/transcript/{transcript_id}"

while True:
  transcript = requests.get(polling_endpoint, headers=headers).json()

  if transcript["status"] == "completed":
    break

  elif transcript["status"] == "error":
    raise RuntimeError(f"Transcription failed: {transcript['error']}")

  else:
    time.sleep(3)

questions = [{
  "question": "What are the top level KPIs for engineering?",
  "context": "KPI stands for key performance indicator",
  "answer_format": "short sentence"
},
{
  "question": "How many days has it been since the data team has gotten updated metrics?",
  "answer_options": ["1", "2", "3", "4", "5", "6", "7", "more than 7"]
}]

lemur_data = {
  "questions": questions,
  "transcript_ids": [transcript_id],
  "final_model": "anthropic/claude-sonnet-4-20250514",
}

result = requests.post(base_url + "/lemur/v3/generate/question-answer", headers=headers, json=lemur_data)

for qa_response in result.json()["response"]:
  print(f"Question: {qa_response['question']}")
  print(f"Answer: {qa_response['answer']}")
```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">

```javascript {12-23,25-30}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

const audioUrl = "https://assembly.ai/meeting.mp4";

const run = async () => {
  const transcript = await client.transcripts.transcribe({ audio: audioUrl });

  const questions = [
    {
      question: "What are the top level KPIs for engineering?",
      context: "KPI stands for key performance indicator",
      answer_format: "short sentence",
    },
    {
      question:
        "How many days has it been since the data team has gotten updated metrics?",
      answer_options: ["1", "2", "3", "4", "5", "6", "7", "more than 7"],
    },
  ];

  const { response: qas } = await client.lemur.questionAnswer({
    transcript_ids: [transcript.id],
    final_model: "anthropic/claude-sonnet-4-20250514",
    context: "A GitLab meeting to discuss logistics",
    questions: questions,
  });

  for (const { question, answer } of qas) {
    console.log("Question", question);
    console.log("Answer", answer);
  }
};

run();
```

```javascript {33-43,45-51}
import axios from "axios";

const base_url = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const uploadUrl = "https://assembly.ai/meeting.mp4";

const data = {
  audio_url: uploadUrl,
};

const response = await axios.post(base_url + "/v2/transcript", data, {
  headers,
});

const transcript_id = response.data.id;
const polling_endpoint = base_url + `/v2/transcript/${transcript_id}`;

while (true) {
  const transcript = (await axios.get(polling_endpoint, { headers })).data;

  if (transcript.status === "completed") {
    break;
  } else if (transcript.status === "error") {
    throw new Error(`Transcription failed: ${transcript.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}

const questions = [
  {
    question: "What are the top level KPIs for engineering?",
    context: "KPI stands for key performance indicator",
    answer_format: "short sentence",
  },
  {
    question:
      "How many days has it been since the data team has gotten updated metrics?",
    answer_options: ["1", "2", "3", "4", "5", "6", "7", "more than 7"],
  },
];

const lemur_data = {
  questions: questions,
  transcript_ids: [transcript_id],
  final_model: "anthropic/claude-sonnet-4-20250514",
};

const result = await axios.post(
  base_url + "/lemur/v3/generate/question-answer",
  lemur_data,
  { headers }
);

for (const qa_response of result.data.response) {
  console.log(`Question: ${qa_response.question}`);
  console.log(`Answer: ${qa_response.answer}`);
}
```

</Tab>
<Tab language="csharp" title="C#">

```csharp {107-120,126-127}
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Threading.Tasks;
using System.Collections.Generic;

public class Transcript
{
   public string Id { get; set; }
   public string Status { get; set; }
   public string Text { get; set; }

   [JsonPropertyName("language_code")]
   public string LanguageCode { get; set; }

   public string Error { get; set; }
}

public class LemurResponse
{
    [JsonPropertyName("request_id")]
    public string RequestId { get; set; }

    [JsonPropertyName("response")]
    public List<QuestionAnswer> Response { get; set; }

    public Usage Usage { get; set; }
}

public class QuestionAnswer
{
    public string Question { get; set; }

    public string Answer { get; set; }
}

public class Usage
{
  [JsonPropertyName("input_tokens")]
  public int InputTokens { get; set; }

  [JsonPropertyName("output_tokens")]
  public int OutputTokens { get; set; }
}

private static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
{
   var data = new { audio_url = audioUrl };
   var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

   using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
   {
       response.EnsureSuccessStatusCode();
       return await response.Content.ReadFromJsonAsync<Transcript>();
   }
}

private static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
{
   var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

   while (true)
   {
       var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
       transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();
       switch (transcript.Status)
       {
           case "processing":
           case "queued":
               await Task.Delay(TimeSpan.FromSeconds(3));
               break;
           case "completed":
               return transcript;
           case "error":
               throw new Exception($"Transcription failed: {transcript.Error}");
           default:
               throw new Exception("This code shouldn't be reachable.");
       }
   }
}

private static async Task<LemurResponse> GenerateQuestionAnswerAsync(List<object> questions, List<string> transcriptIds, HttpClient httpClient)
{
   var data = new
   {
       transcript_ids = transcriptIds,
       questions = questions,
       final_model = "anthropic/claude-sonnet-4-20250514"
   };

   var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

   using var response = await httpClient.PostAsync("https://api.assemblyai.com/lemur/v3/generate/question-answer", content);
   response.EnsureSuccessStatusCode();
   return await response.Content.ReadFromJsonAsync<LemurResponse>();
}

using (var httpClient = new HttpClient())
{
   httpClient.DefaultRequestHeaders.Authorization =
       new AuthenticationHeaderValue("<YOUR_API_KEY>");

   var questions = new List<object>
   {
       new
       {
           question = "What are the top level KPIs for engineering?",
           context = "KPI stands for key performance indicator",
           answer_format = "short sentence"
       },
       new
       {
           question = "How many days has it been since the data team has gotten updated metrics?",
           answer_options = new List<string> { "1", "2", "3", "4", "5", "6", "7", "more than 7" }
       }
   };

   var uploadUrl = "https://assembly.ai/meeting.mp4";
   var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);
   transcript = await WaitForTranscriptToProcess(transcript, httpClient);

   var transcriptIds = new List<string> { transcript.Id };
   var lemurResponse = await GenerateQuestionAnswerAsync(questions, transcriptIds, httpClient);

   foreach (var qa_response in lemurResponse.Response)
   {
       Console.WriteLine($"Question: {qa_response.Question}");
       Console.WriteLine($"Answer: {qa_response.Answer}");
   }
}
```

</Tab>
<Tab language="ruby" title="Ruby">

```ruby {35-45,47-55}
require 'net/http'
require 'json'

base_url = "https://api.assemblyai.com"
headers = {
   "authorization" => "<YOUR_API_KEY>",
   "content-type" => "application/json"
}

upload_url = "https://assembly.ai/meeting.mp4"

uri = URI("#{base_url}/v2/transcript")
request = Net::HTTP::Post.new(uri, headers)
request.body = { audio_url: upload_url }.to_json

response = http.request(request)
transcript_id = JSON.parse(response.body)["id"]
polling_endpoint = "#{base_url}/v2/transcript/#{transcript_id}"

while true
   polling_uri = URI(polling_endpoint)
   polling_request = Net::HTTP::Get.new(polling_uri, headers)
   polling_response = http.request(polling_request)
   transcription_result = JSON.parse(polling_response.body)

   if transcription_result["status"] == "completed"
       break
   elsif transcription_result["status"] == "error"
       raise "Transcription failed: #{transcription_result["error"]}"
   else
       sleep(3)
   end
end

questions = [
  {
    question: "What are the top level KPIs for engineering?",
    context: "KPI stands for key performance indicator",
    answer_format: "short sentence"
  },
  {
    question: "How many days has it been since the data team has gotten updated metrics?",
    answer_options: ["1", "2", "3", "4", "5", "6", "7", "more than 7"]
  }
]

lemur_uri = URI("#{base_url}/lemur/v3/generate/question-answer")
lemur_request = Net::HTTP::Post.new(lemur_uri, headers)
lemur_request.body = {
  final_model: "anthropic/claude-sonnet-4-20250514",
  questions: questions,
  transcript_ids: [transcript_id]
}.to_json

lemur_response = http.request(lemur_request)
lemur_result = JSON.parse(lemur_response.body)

lemur_result["response"].each do |qa_response|
  puts "Question: #{qa_response['question']}"
  puts "Answer: #{qa_response['answer']}"
end
```

</Tab>
<Tab language="php" title="PHP">

```php {42-64}
<?php
$base_url = "https://api.assemblyai.com";
$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$upload_url = "https://assembly.ai/meeting.mp4"

$ch = curl_init($base_url . "/v2/transcript");
curl_setopt_array($ch, [
    CURLOPT_POST => true,
    CURLOPT_POSTFIELDS => json_encode(["audio_url" => $upload_url]),
    CURLOPT_HTTPHEADER => $headers,
    CURLOPT_RETURNTRANSFER => true
]);

$response = curl_exec($ch);
$transcript_id = json_decode($response, true)['id'];
curl_close($ch);

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $ch = curl_init($polling_endpoint);
    curl_setopt_array($ch, [
        CURLOPT_HTTPHEADER => $headers,
        CURLOPT_RETURNTRANSFER => true
    ]);

    $transcription_result = json_decode(curl_exec($ch), true);
    curl_close($ch);

    if ($transcription_result['status'] === "completed") {
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    }
    sleep(3);
}

$questions = [
    [
        "question" => "What are the top level KPIs for engineering?",
        "context" => "KPI stands for key performance indicator",
        "answer_format" => "short sentence"
    ],
    [
        "question" => "How many days has it been since the data team has gotten updated metrics?",
        "answer_options" => ["1", "2", "3", "4", "5", "6", "7", "more than 7"]
    ]
];

$ch = curl_init($base_url . "/lemur/v3/generate/question-answer");
curl_setopt_array($ch, [
    CURLOPT_RETURNTRANSFER => true,
    CURLOPT_POST => true,
    CURLOPT_HTTPHEADER => $headers,
    CURLOPT_POSTFIELDS => json_encode([
        'final_model' => 'anthropic/claude-sonnet-4-20250514',
        'questions' => $questions,
        'transcript_ids' => [$transcript_id]
    ])
]);

$response = curl_exec($ch);
$result = json_decode($response, true);
curl_close($ch);

foreach ($result['response'] as $qa_response) {
    echo "Question: " . $qa_response['question'] . PHP_EOL;
    echo "Answer: " . $qa_response['answer'] . PHP_EOL;
}
```

</Tab>
</Tabs>

## Custom Q&A example (Advanced)

This example shows how you can run a custom LeMUR task with an advanced prompt to create custom Q&A responses:

<Card
  icon="book"
  title="Cookbook: Custom Q&A with LeMUR Task"
  href="https://www.assemblyai.com/docs/guides/task-endpoint-structured-QA"
/>

## More Q&A prompt examples

Try any of these prompts to get started:

| Use case               | Example prompt                                                            |
| ---------------------- | ------------------------------------------------------------------------- |
| Question and answer    | <i>"Identify any patterns or trends based on the transcript"</i>          |
| Closed-ended questions | <i>"Did the customer express a positive sentiment in the phone call?"</i> |
| Sentiment analysis     | <i>"What was the emotional sentiment of the phone call?"</i>              |

For more use cases and prompt examples, see [LeMUR examples](/docs/lemur/examples).

## API reference

- [LeMUR Task endpoint](https://assemblyai.com/docs/api-reference/lemur/task)
- [LeMUR Question & Answer endpoint](https://www.assemblyai.com/docs/api-reference/lemur/question-answer)

## Improve the results

To improve the results, see the following resources:

- Optimize your prompt with the [prompt engineering guide](/docs/lemur/improving-your-prompt).
- To alter the outcome, see [Change model and parameters](/docs/lemur/customize-parameters).


---
title: Create Custom LLM Prompts
subtitle: Learn about different use cases for LeMUR with these examples.
description: Learn about different use cases for LeMUR with these examples.
---

<Tip>

If you want a Quickstart, see [Apply LLMs to audio files](/docs/lemur/apply-llms-to-audio-files).

</Tip>

<Note title="Before you start">

To use LeMUR, you need an <a href="https://www.assemblyai.com/dashboard/signup" target="_blank"> AssemblyAI account </a> with a credit card set up.

</Note>

## Custom prompt example

If you want to send a custom prompt to the LLM, you can use the [LeMUR Task](https://assemblyai.com/docs/api-reference/lemur/task) and apply the model to your transcribed audio files.

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>

```python {13-14,16-19}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# Step 1: Transcribe an audio file.
# audio_file = "./local_file.mp4"
# Or use a publicly-accessible URL:
audio_file = "https://assembly.ai/call.mp4"

transcriber = aai.Transcriber()
transcript = transcriber.transcribe(audio_file)

# Step 2: Define your prompt.
prompt = "What was the emotional sentiment of the phone call?"

# Step 3: Apply LeMUR.
result = transcript.lemur.task(
    prompt, final_model=aai.LemurModel.claude4_sonnet_20250514
)

print(result.response)
```

</Tab>
<Tab language="python" title="Python">

```python {42-43,45-52}
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
  "authorization": "<YOUR_API_KEY>"
}

# Step 1: Transcribe an audio file.
# You can use a local filepath:
# with open("./my-audio.mp3", "rb") as f:
# response = requests.post(base_url + "/v2/upload",
#                         headers=headers,
#                         data=f)
# upload_url = response.json()["upload_url"]

# Or use a publicly-accessible URL:
upload_url = "https://assembly.ai/call.mp4"

data = {
  "audio_url": upload_url
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

transcript_id = response.json()["id"]
polling_endpoint = base_url + f"/v2/transcript/{transcript_id}"

while True:
  transcript = requests.get(polling_endpoint, headers=headers).json()

  if transcript["status"] == "completed":
    break

  elif transcript["status"] == "error":
    raise RuntimeError(f"Transcription failed: {transcript['error']}")

  else:
    time.sleep(3)

# Step 2: Define your prompt.
prompt = "What was the emotional sentiment of the phone call?"

# Step 3: Apply LeMUR.
lemur_data = {
  "prompt": prompt,
  "transcript_ids": [transcript_id],
  "final_model": "anthropic/claude-sonnet-4-20250514",
}

result = requests.post(base_url + "/lemur/v3/generate/task", headers=headers, json=lemur_data)
print(result.json()["response"])
```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">

```javascript {14-15,17-22}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

const run = async () => {
  // Step 1: Transcribe an audio file.
  // const audioFile = './local_file.mp4'
  // Or use a publicly-accessible URL:
  const audioFile = "https://assembly.ai/call.mp4";
  const transcript = await client.transcripts.transcribe({ audio: audioFile });

  // Step 2: Define your prompt.
  const prompt = "What was the emotional sentiment of the phone call?";

  // Step 3: Apply LeMUR.
  const { response } = await client.lemur.task({
    transcript_ids: [transcript.id],
    prompt,
    final_model: "anthropic/claude-sonnet-4-20250514",
  });

  console.log(response);
};

run();
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript {44-45,47-52}
import axios from "axios";

const base_url = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

// Step 1: Transcribe an audio file.
// import fs from 'fs-extra';
// const path = './my-audio.mp3';
// const audioData = await fs.readFile(path)
// const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
// headers
// })
// const uploadUrl = uploadResponse.data.upload_url

// Or use a publicly-accessibly URL:
const uploadUrl = "https://assembly.ai/call.mp4";

const data = {
  audio_url: uploadUrl,
};

const response = await axios.post(base_url + "/v2/transcript", data, {
  headers,
});

const transcript_id = response.data.id;
const polling_endpoint = base_url + `/v2/transcript/${transcript_id}`;

while (true) {
  const transcript = (await axios.get(polling_endpoint, { headers })).data;

  if (transcript.status === "completed") {
    break;
  } else if (transcript.status === "error") {
    throw new Error(`Transcription failed: ${transcript.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}

// Step 2: Define your prompt.
const prompt = "What was the emotional sentiment of the phone call?";

// Step 3: Apply LeMUR.
const lemur_data = {
  prompt: prompt,
  transcript_ids: [transcript_id],
  final_model: "anthropic/claude-sonnet-4-20250514",
};

const result = await axios.post(
  base_url + "/lemur/v3/generate/task",
  lemur_data,
  { headers }
);
console.log(result.data.response);
```

</Tab>
<Tab language="csharp" title="C#">

```csharp {122-123,125-127}
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Threading.Tasks;

public class Transcript
{
   public string Id { get; set; }
   public string Status { get; set; }
   public string Text { get; set; }

   [JsonPropertyName("language_code")]
   public string LanguageCode { get; set; }

   public string Error { get; set; }
}

public class LemurResponse
{
  [JsonPropertyName("request_id")]
  public string RequestId { get; set; }

  public string Response { get; set; }

  public Usage Usage { get; set; }
}

public class Usage
{
  [JsonPropertyName("input_tokens")]
  public int InputTokens { get; set; }

  [JsonPropertyName("output_tokens")]
  public int OutputTokens { get; set; }
}

private static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
{
    using (var fileStream = File.OpenRead(filePath))
    using (var fileContent = new StreamContent(fileStream))
    {
        fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent))
        {
            response.EnsureSuccessStatusCode();
            var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
            return jsonDoc.RootElement.GetProperty("upload_url").GetString();
        }
    }
}


private static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
{
   var data = new { audio_url = audioUrl };
   var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

   using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
   {
       response.EnsureSuccessStatusCode();
       return await response.Content.ReadFromJsonAsync<Transcript>();
   }
}

private static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
{
   var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

   while (true)
   {
       var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
       transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();
       switch (transcript.Status)
       {
           case "processing":
           case "queued":
               await Task.Delay(TimeSpan.FromSeconds(3));
               break;
           case "completed":
               return transcript;
           case "error":
               throw new Exception($"Transcription failed: {transcript.Error}");
           default:
               throw new Exception("This code shouldn't be reachable.");
       }
   }
}

private static async Task<LemurResponse> GenerateTaskAsync(string prompt, List<string> transcriptIds, HttpClient httpClient)
{
   var data = new
   {
       transcript_ids = transcriptIds,
       prompt,
       final_model = "anthropic/claude-sonnet-4-20250514"
   };

   var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

   using var response = await httpClient.PostAsync("https://api.assemblyai.com/lemur/v3/generate/task", content);
   response.EnsureSuccessStatusCode();
   return await response.Content.ReadFromJsonAsync<LemurResponse>();
}

using (var httpClient = new HttpClient())
{
   httpClient.DefaultRequestHeaders.Authorization =
       new AuthenticationHeaderValue("<YOUR_API_KEY>");

   // Step 1: Transcribe an audio file.
   // var uploadUrl = await UploadFileAsync("/my_audio.mp3", httpClient);
   // Or use a publicly-accessible URL.
   var uploadUrl = "https://assembly.ai/call.mp4";
   var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);
   transcript = await WaitForTranscriptToProcess(transcript, httpClient);

   // Step 2: Define your prompt.
   const string prompt = "What was the emotional sentiment of the phone call?";

   // Step 3: Apply LeMUR.
   var transcriptIds = new List<string> { transcript.Id };
   var lemurResponse = await GenerateTaskAsync(prompt, transcriptIds, httpClient);

   Console.WriteLine(lemurResponse.Response);
}
```

</Tab>
<Tab language="ruby" title="Ruby">

```ruby {47-48,50-59}
require 'net/http'
require 'json'

base_url = "https://api.assemblyai.com"
headers = {
   "authorization" => "<YOUR_API_KEY>",
   "content-type" => "application/json"
}

# Step 1: Transcribe an audio file.
# path = "/my_audio.mp3"
# uri = URI("#{base_url}/v2/upload")
# request = Net::HTTP::Post.new(uri, headers)
# request.body = File.read(path)

# http = Net::HTTP.new(uri.host, uri.port)
# http.use_ssl = true
# upload_response = http.request(request)
# upload_url = JSON.parse(upload_response.body)["upload_url"]

# Or use a publicly-accessible URL:
upload_url = "https://assembly.ai/call.mp4"

uri = URI("#{base_url}/v2/transcript")
request = Net::HTTP::Post.new(uri, headers)
request.body = { audio_url: upload_url }.to_json

response = http.request(request)
transcript_id = JSON.parse(response.body)["id"]
polling_endpoint = "#{base_url}/v2/transcript/#{transcript_id}"

while true
   polling_uri = URI(polling_endpoint)
   polling_request = Net::HTTP::Get.new(polling_uri, headers)
   polling_response = http.request(polling_request)
   transcription_result = JSON.parse(polling_response.body)

   if transcription_result["status"] == "completed"
       break
   elsif transcription_result["status"] == "error"
       raise "Transcription failed: #{transcription_result["error"]}"
   else
       sleep(3)
   end
end

# Step 2: Define your prompt.
prompt = "What was the emotional sentiment of the phone call?"

# Step 3: Apply LeMUR.
lemur_uri = URI("#{base_url}/lemur/v3/generate/task")
lemur_request = Net::HTTP::Post.new(lemur_uri, headers)
lemur_request.body = {
  final_model: "anthropic/claude-sonnet-4-20250514",
  prompt: prompt,
  transcript_ids: [transcript_id]
}.to_json

lemur_response = http.request(lemur_request)
lemur_result = JSON.parse(lemur_response.body)
puts lemur_result["response"]
```

</Tab>
<Tab language="php" title="PHP">

```php {57-58,60-73}
<?php
$base_url = "https://api.assemblyai.com";
$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

// Step 1: Transcribe an audio file.
// $path = "/my_audio.mp3";
// $ch = curl_init($base_url . "/v2/upload");
// curl_setopt_array($ch, [
//     CURLOPT_POST => true,
//     CURLOPT_POSTFIELDS => file_get_contents($path),
//     CURLOPT_HTTPHEADER => $headers,
//     CURLOPT_RETURNTRANSFER => true
// ]);

// $response = curl_exec($ch);
// $upload_url = json_decode($response, true)["upload_url"];
// curl_close($ch);

// Or use a publicly-accessible URL:
$upload_url = "https://assembly.ai/call.mp4"

$ch = curl_init($base_url . "/v2/transcript");
curl_setopt_array($ch, [
    CURLOPT_POST => true,
    CURLOPT_POSTFIELDS => json_encode(["audio_url" => $upload_url]), // You can also replace upload_url with an audio file URL
    CURLOPT_HTTPHEADER => $headers,
    CURLOPT_RETURNTRANSFER => true
]);

$response = curl_exec($ch);
$transcript_id = json_decode($response, true)['id'];
curl_close($ch);

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $ch = curl_init($polling_endpoint);
    curl_setopt_array($ch, [
        CURLOPT_HTTPHEADER => $headers,
        CURLOPT_RETURNTRANSFER => true
    ]);

    $transcription_result = json_decode(curl_exec($ch), true);
    curl_close($ch);

    if ($transcription_result['status'] === "completed") {
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    }
    sleep(3);
}

// Step 2: Define your prompt.
$prompt = 'What was the emotional sentiment of the phone call?'

// Step 3: Apply LeMUR.
$ch = curl_init($base_url . "/lemur/v3/generate/task");
curl_setopt_array($ch, [
    CURLOPT_RETURNTRANSFER => true,
    CURLOPT_POST => true,
    CURLOPT_HTTPHEADER => $headers,
    CURLOPT_POSTFIELDS => json_encode([
        'final_model' => 'anthropic/claude-sonnet-4-20250514',
        'prompt' => $prompt,
        'transcript_ids' => [$transcript_id]
    ])
]);

$response = curl_exec($ch);
$result = json_decode($response, true);
echo $result['response'];
curl_close($ch);
```

  </Tab>
</Tabs>

## Popular Use Cases

<CardGroup cols={3}>
  <Card
    title="Speaker Identification"
    href="/docs/guides/speaker-identification"
  >
    _Assign Speaker Names With LeMUR_
  </Card>
  <Card
    title="Custom Topic Tags"
    href="/docs/guides/custom-topic-tags"
  >
    _Label content with custom topic tags using LeMUR_
  </Card>
  <Card
    title="Boost Transcription Accuracy"
    href="/docs/guides/custom-vocab-lemur"
  >
    _Boost Custom Vocabulary List with LeMUR_
  </Card>
  <Card
    title="Sentiment Analysis"
    href="/docs/guides/call-sentiment-analysis"
  >
    _Analyze the sentiment of a phone call with LeMUR_
  </Card>
  <Card
    title="SOAP Note Generation"
    href="/docs/guides/soap-note-generation"
  >
    _Generate SOAP notes with LeMUR_
  </Card>
  <Card
    title="Action Items"
    href="/docs/guides/generate-meeting-action-items-with-lemur"
  >
    _Generate action items with LeMUR_
  </Card>
</CardGroup>

## Ideas to get you started

| Use case                         | Example prompt                                                                                    |
| -------------------------------- | ------------------------------------------------------------------------------------------------- |
| Question & Answer                | <i>"Identify any patterns or trends based on the transcript"</i>                                  |
| Quote or Citation                | <i>"List the timestamp X topic was discussed, provide specific citations"</i>                     |
| Closed-ended questions           | <i>"Did the customer express a positive sentiment in the phone call?"</i>                         |
| Sentiment analysis               | <i>"What was the emotional sentiment of the phone call?"</i>                                      |
| Summaries                        | <i>"Summarize key decisions and important points from the phone call transcript"</i>              |
| Summarize audio segments         | <i>"Summarize the key events of each chapter"</i>                                                 |
| Generate titles and descriptions | <i>"Generate an attention-grabbing YouTube title based on the video transcript"</i>               |
| Generate tags                    | <i>"Generate keywords that can be used to describe the key themes of the conversation"</i>        |
| Action items                     | <i>"What action items were assigned to each participant?"</i>                                     |
| Generate content                 | <i>"Generate a blog post with key information presented in bullet points from the transcript"</i> |
| Paraphrasing                     | <i>"Rephrase X segment from the transcript in a different way"</i>                                |

You can find more ideas and code examples in our [Cookbooks](/docs/guides).


---
title: Change Model and Parameters
subtitle: Learn how you can customize LeMUR parameters to alter the outcome.
description: Analyze your audio files with Large Language Models.
---

## Change the model type

LeMUR features the following LLMs:

- Claude 4 Sonnet
- Claude 4 Opus
- Claude 3.7 Sonnet
- Claude 3.5 Sonnet
- Claude 3.5 Haiku
- Claude 3 Opus
- Claude 3 Haiku
- Claude 3 Sonnet (_Legacy - Sunsetting on 7/21/25_)

You can switch the model by specifying the `final_model` parameter.

<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>

```python {3}
result = transcript.lemur.task(
    prompt,
    final_model=aai.LemurModel.claude4_sonnet_20250514
)
```
| Model                                                    | SDK Parameter                              | Description                                                                                                                                              | Region Availability |
| -------------------------------------------------------- | ------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------- |
| **Claude 4 Sonnet**                                      | `aai.LemurModel.claude_sonnet_4_20250514`  | A model with enhanced reasoning and improved performance for everyday tasks while maintaining exceptional speed and cost-effectiveness.                  | US & EU             |
| **Claude 4 Opus**                                        | `aai.LemurModel.claude_opus_4_20250514`    | Anthropic's most capable model yet, offering superior performance on complex reasoning tasks, advanced creative work, and sophisticated problem-solving. | US only             |
| **Claude 3.7 Sonnet**                                    | `aai.LemurModel.claude3_7_sonnet_20250219` | A advanced model featuring enhanced reasoning capabilities. Strong at complex reasoning tasks.                                                           | US & EU             |
| **Claude 3.5 Sonnet**                                    | `aai.LemurModel.claude3_5_sonnet`          | A mid-tier upgrade balancing power and performance. This uses Anthropic's Claude 3.5 Sonnet model version `claude-3-5-sonnet-20240620`.                  | US & EU             |
| **Claude 3.5 Haiku**                                     | `aai.LemurModel.claude3_5_haiku_20241022`  | The fastest model in the family, optimized for quick responses while maintaining good reasoning.                                                         | US & EU             |
| **Claude 3.0 Opus**                                      | `aai.LemurModel.claude3_opus`              | The most powerful legacy Claude 3 model, excels at complex writing and analysis.                                                                         | US only             |
| **Claude 3.0 Haiku**                                     | `aai.LemurModel.claude3_haiku`             | An entry-level, fast legacy model for everyday tasks.                                                                                                    | US only             |
| **Claude 3.0 Sonnet** (_Legacy - Sunsetting on 7/21/25_) | `aai.LemurModel.claude3_sonnet`            | A legacy mid-tier model balancing power and speed.     

  </Tab>
  <Tab language="python" title="Python">

```python {4}
data = {
  "prompt": prompt,
  "transcript_ids": [transcript_id],
  "final_model": "anthropic/claude-sonnet-4-20250514"
}

result = requests.post("https://api.assemblyai.com/lemur/v3/generate/task", headers=headers, json=data)
```

| Model                                                    | API Parameter                          | Description                                                                                                                                              | Region Availability |
| -------------------------------------------------------- | -------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------- |
| **Claude 4 Sonnet**                                      | `anthropic/claude-sonnet-4-20250514`   | A model with enhanced reasoning and improved performance for everyday tasks while maintaining exceptional speed and cost-effectiveness.                  | US & EU             |
| **Claude 4 Opus**                                        | `anthropic/claude-opus-4-20250514`     | Anthropic's most capable model yet, offering superior performance on complex reasoning tasks, advanced creative work, and sophisticated problem-solving. | US only             |
| **Claude 3.7 Sonnet**                                    | `anthropic/claude-3-7-sonnet-20250219` | The newest and most advanced model featuring enhanced reasoning capabilities. Strong at complex reasoning tasks.                                         | US & EU             |
| **Claude 3.5 Sonnet**                                    | `anthropic/claude-3-5-sonnet`          | A mid-tier upgrade balancing power and performance. This uses Anthropic's Claude 3.5 Sonnet model version `claude-3-5-sonnet-20240620`.                  | US & EU             |
| **Claude 3.5 Haiku**                                     | `anthropic/claude-3-5-haiku-20241022`  | The fastest model in the family, optimized for quick responses while maintaining good reasoning.                                                         | US & EU             |
| **Claude 3.0 Opus**                                      | `anthropic/claude-3-opus`              | The most powerful legacy Claude 3 model, excels at complex writing and analysis.                                                                         | US only             |
| **Claude 3.0 Haiku**                                     | `anthropic/claude-3-haiku`             | An entry-level, fast legacy model for everyday tasks.                                                                                                    | US only             |
| **Claude 3.0 Sonnet** (_Legacy - Sunsetting on 7/21/25_) | `anthropic/claude-3-sonnet`            | A legacy mid-tier model balancing power and speed.                                                                                                       | US only             |

  </Tab>
  <Tab language="javascript-sdk" title="JavaScript SDK">

```javascript {4}
const { response } = await client.lemur.task({
  transcript_ids: [transcript.id],
  prompt,
  final_model: "anthropic/claude-sonnet-4-20250514",
});
```

| Model                                                    | SDK Parameter                          | Description                                                                                                                                              | Region Availability |
| -------------------------------------------------------- | -------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------- |
| **Claude 4 Sonnet**                                      | `anthropic/claude-sonnet-4-20250514`   | A model with enhanced reasoning and improved performance for everyday tasks while maintaining exceptional speed and cost-effectiveness.                  | US & EU             |
| **Claude 4 Opus**                                        | `anthropic/claude-opus-4-20250514`     | Anthropic's most capable model yet, offering superior performance on complex reasoning tasks, advanced creative work, and sophisticated problem-solving. | US only             |
| **Claude 3.7 Sonnet**                                    | `anthropic/claude-3-7-sonnet-20250219` | The newest and most advanced model featuring enhanced reasoning capabilities. Strong at complex reasoning tasks.                                         | US & EU             |
| **Claude 3.5 Sonnet**                                    | `anthropic/claude-3-5-sonnet`          | A mid-tier upgrade balancing power and performance. This uses Anthropic's Claude 3.5 Sonnet model version `claude-3-5-sonnet-20240620`.                  | US & EU             |
| **Claude 3.5 Haiku**                                     | `anthropic/claude-3-5-haiku-20241022`  | The fastest model in the family, optimized for quick responses while maintaining good reasoning.                                                         | US & EU             |
| **Claude 3.0 Opus**                                      | `anthropic/claude-3-opus`              | The most powerful legacy Claude 3 model, excels at complex writing and analysis.                                                                         | US only             |
| **Claude 3.0 Haiku**                                     | `anthropic/claude-3-haiku`             | An entry-level, fast legacy model for everyday tasks.                                                                                                    | US only             |
| **Claude 3.0 Sonnet** (_Legacy - Sunsetting on 7/21/25_) | `anthropic/claude-3-sonnet`            | A legacy mid-tier model balancing power and speed.                                                                                                       | US only             |

  </Tab>
  <Tab language="javascript" title="JavaScript">

```javascript {4}
const data = {
  prompt: prompt,
  transcript_ids: [transcript_id],
  final_model: "anthropic/claude-sonnet-4-20250514",
};

const result = await axios.post(
  "https://api.assemblyai.com/lemur/v3/generate/task",
  data,
  { headers }
);
```

| Model                                                    | API Parameter                          | Description                                                                                                                                              | Region Availability |
| -------------------------------------------------------- | -------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------- |
| **Claude 4 Sonnet**                                      | `anthropic/claude-sonnet-4-20250514`   | A model with enhanced reasoning and improved performance for everyday tasks while maintaining exceptional speed and cost-effectiveness.                  | US & EU             |
| **Claude 4 Opus**                                        | `anthropic/claude-opus-4-20250514`     | Anthropic's most capable model yet, offering superior performance on complex reasoning tasks, advanced creative work, and sophisticated problem-solving. | US only             |
| **Claude 3.7 Sonnet**                                    | `anthropic/claude-3-7-sonnet-20250219` | The newest and most advanced model featuring enhanced reasoning capabilities. Strong at complex reasoning tasks.                                         | US & EU             |
| **Claude 3.5 Sonnet**                                    | `anthropic/claude-3-5-sonnet`          | A mid-tier upgrade balancing power and performance. This uses Anthropic's Claude 3.5 Sonnet model version `claude-3-5-sonnet-20240620`.                  | US & EU             |
| **Claude 3.5 Haiku**                                     | `anthropic/claude-3-5-haiku-20241022`  | The fastest model in the family, optimized for quick responses while maintaining good reasoning.                                                         | US & EU             |
| **Claude 3.0 Opus**                                      | `anthropic/claude-3-opus`              | The most powerful legacy Claude 3 model, excels at complex writing and analysis.                                                                         | US only             |
| **Claude 3.0 Haiku**                                     | `anthropic/claude-3-haiku`             | An entry-level, fast legacy model for everyday tasks.                                                                                                    | US only             |
| **Claude 3.0 Sonnet** (_Legacy - Sunsetting on 7/21/25_) | `anthropic/claude-3-sonnet`            | A legacy mid-tier model balancing power and speed.                                                                                                       | US only             |

  </Tab>
  <Tab language="csharp" title="C#">

```csharp {5}
var data = new
{
  transcript_ids = transcriptIds,
  prompt = prompt,
  final_model = "anthropic/claude-sonnet-4-20250514"
};

var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");
using var response = await httpClient.PostAsync("https://api.assemblyai.com/lemur/v3/generate/task", content);
```

| Model                                                    | API Parameter                          | Description                                                                                                                                              | Region Availability |
| -------------------------------------------------------- | -------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------- |
| **Claude 4 Sonnet**                                      | `anthropic/claude-sonnet-4-20250514`   | A model with enhanced reasoning and improved performance for everyday tasks while maintaining exceptional speed and cost-effectiveness.                  | US & EU             |
| **Claude 4 Opus**                                        | `anthropic/claude-opus-4-20250514`     | Anthropic's most capable model yet, offering superior performance on complex reasoning tasks, advanced creative work, and sophisticated problem-solving. | US only             |
| **Claude 3.7 Sonnet**                                    | `anthropic/claude-3-7-sonnet-20250219` | The newest and most advanced model featuring enhanced reasoning capabilities. Strong at complex reasoning tasks.                                         | US & EU             |
| **Claude 3.5 Sonnet**                                    | `anthropic/claude-3-5-sonnet`          | A mid-tier upgrade balancing power and performance. This uses Anthropic's Claude 3.5 Sonnet model version `claude-3-5-sonnet-20240620`.                  | US & EU             |
| **Claude 3.5 Haiku**                                     | `anthropic/claude-3-5-haiku-20241022`  | The fastest model in the family, optimized for quick responses while maintaining good reasoning.                                                         | US & EU             |
| **Claude 3.0 Opus**                                      | `anthropic/claude-3-opus`              | The most powerful legacy Claude 3 model, excels at complex writing and analysis.                                                                         | US only             |
| **Claude 3.0 Haiku**                                     | `anthropic/claude-3-haiku`             | An entry-level, fast legacy model for everyday tasks.                                                                                                    | US only             |
| **Claude 3.0 Sonnet** (_Legacy - Sunsetting on 7/21/25_) | `anthropic/claude-3-sonnet`            | A legacy mid-tier model balancing power and speed.                                                                                                       | US only             |

  </Tab>

  <Tab language="ruby" title="Ruby">

```ruby {3}
request = Net::HTTP::Post.new("https://api.assemblyai.com/lemur/v3/generate/task", headers)
request.body = {
  final_model: "anthropic/claude-sonnet-4-20250514",
  prompt: prompt,
  transcript_ids: [transcript_id]
}.to_json

response = http.request(lemur_request)
```

| Model                                                    | API Parameter                          | Description                                                                                                                                              | Region Availability |
| -------------------------------------------------------- | -------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------- |
| **Claude 4 Sonnet**                                      | `anthropic/claude-sonnet-4-20250514`   | A model with enhanced reasoning and improved performance for everyday tasks while maintaining exceptional speed and cost-effectiveness.                  | US & EU             |
| **Claude 4 Opus**                                        | `anthropic/claude-opus-4-20250514`     | Anthropic's most capable model yet, offering superior performance on complex reasoning tasks, advanced creative work, and sophisticated problem-solving. | US only             |
| **Claude 3.7 Sonnet**                                    | `anthropic/claude-3-7-sonnet-20250219` | The newest and most advanced model featuring enhanced reasoning capabilities. Strong at complex reasoning tasks.                                         | US & EU             |
| **Claude 3.5 Sonnet**                                    | `anthropic/claude-3-5-sonnet`          | A mid-tier upgrade balancing power and performance. This uses Anthropic's Claude 3.5 Sonnet model version `claude-3-5-sonnet-20240620`.                  | US & EU             |
| **Claude 3.5 Haiku**                                     | `anthropic/claude-3-5-haiku-20241022`  | The fastest model in the family, optimized for quick responses while maintaining good reasoning.                                                         | US & EU             |
| **Claude 3.0 Opus**                                      | `anthropic/claude-3-opus`              | The most powerful legacy Claude 3 model, excels at complex writing and analysis.                                                                         | US only             |
| **Claude 3.0 Haiku**                                     | `anthropic/claude-3-haiku`             | An entry-level, fast legacy model for everyday tasks.                                                                                                    | US only             |
| **Claude 3.0 Sonnet** (_Legacy - Sunsetting on 7/21/25_) | `anthropic/claude-3-sonnet`            | A legacy mid-tier model balancing power and speed.                                                                                                       | US only             |

  </Tab>

  <Tab language="php" title="PHP">

```php {7}
$ch = curl_init("https://api.assemblyai.com/lemur/v3/generate/task");
curl_setopt_array($ch, [
    CURLOPT_RETURNTRANSFER => true,
    CURLOPT_POST => true,
    CURLOPT_HTTPHEADER => $headers,
    CURLOPT_POSTFIELDS => json_encode([
        'final_model' => 'anthropic/claude-sonnet-4-20250514',
        'prompt' => $prompt,
        'transcript_ids' => [$transcript_id]
    ])
]);

$response = curl_exec($ch);
```

| Model                                                    | API Parameter                          | Description                                                                                                                                              | Region Availability |
| -------------------------------------------------------- | -------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------- |
| **Claude 4 Sonnet**                                      | `anthropic/claude-sonnet-4-20250514`   | A model with enhanced reasoning and improved performance for everyday tasks while maintaining exceptional speed and cost-effectiveness.                  | US & EU             |
| **Claude 4 Opus**                                        | `anthropic/claude-opus-4-20250514`     | Anthropic's most capable model yet, offering superior performance on complex reasoning tasks, advanced creative work, and sophisticated problem-solving. | US only             |
| **Claude 3.7 Sonnet**                                    | `anthropic/claude-3-7-sonnet-20250219` | The newest and most advanced model featuring enhanced reasoning capabilities. Strong at complex reasoning tasks.                                         | US & EU             |
| **Claude 3.5 Sonnet**                                    | `anthropic/claude-3-5-sonnet`          | A mid-tier upgrade balancing power and performance. This uses Anthropic's Claude 3.5 Sonnet model version `claude-3-5-sonnet-20240620`.                  | US & EU             |
| **Claude 3.5 Haiku**                                     | `anthropic/claude-3-5-haiku-20241022`  | The fastest model in the family, optimized for quick responses while maintaining good reasoning.                                                         | US & EU             |
| **Claude 3.0 Opus**                                      | `anthropic/claude-3-opus`              | The most powerful legacy Claude 3 model, excels at complex writing and analysis.                                                                         | US only             |
| **Claude 3.0 Haiku**                                     | `anthropic/claude-3-haiku`             | An entry-level, fast legacy model for everyday tasks.                                                                                                    | US only             |
| **Claude 3.0 Sonnet** (_Legacy - Sunsetting on 7/21/25_) | `anthropic/claude-3-sonnet`            | A legacy mid-tier model balancing power and speed.                                                                                                       | US only             |

  </Tab>
</Tabs>

You can find more information on pricing for each model <a href="https://www.assemblyai.com/pricing" target="_blank">here</a>.

## Change the maximum output size

You can change the maximum output size in tokens by specifying the `max_output_size` parameter.

| Model                                                    | API Parameter                          | Max Tokens Allowed |
| -------------------------------------------------------- | -------------------------------------- | ------------------ |
| **Claude 4 Sonnet**                                      | `anthropic/claude-sonnet-4-20250514`   | 64000              |
| **Claude 4 Opus**                                        | `anthropic/claude-opus-4-20250514`     | 64000              |
| **Claude 3.7 Sonnet**                                    | `anthropic/claude-3-7-sonnet-20250219` | 64000              |
| **Claude 3.5 Sonnet**                                    | `anthropic/claude-3-5-sonnet`          | 4000               |
| **Claude 3.5 Haiku**                                     | `anthropic/claude-3-5-haiku-20241022`  | 8192               |
| **Claude 3.0 Opus**                                      | `anthropic/claude-3-opus`              | 4000               |
| **Claude 3.0 Haiku**                                     | `anthropic/claude-3-haiku`             | 4000               |
| **Claude 3.0 Sonnet** (_Legacy - Sunsetting on 7/21/25_) | `anthropic/claude-3-sonnet`            | 4000               |

<br />
<br />

<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>
  
```python {4}
result = transcript.lemur.task(
    prompt,
    final_model,
    max_output_size=1000
)
```

  </Tab>

  <Tab language="python" title="Python">

```python {5}
data = {
  "prompt": prompt,
  "transcript_ids": [transcript_id],
  "final_model": final_model,
  "max_output_size": 1000
}

result = requests.post("https://api.assemblyai.com/lemur/v3/generate/task", headers=headers, json=data)
```

  </Tab>
  <Tab language="javascript-sdk" title="JavaScript SDK">

```javascript {5}
const { response } = await client.lemur.task({
  prompt,
  transcript_ids: [transcript.id],
  final_model,
  max_output_size: 1000,
});
```

  </Tab>
  <Tab language="javascript" title="JavaScript">

```javascript {5}
const data = {
  transcript_ids: [transcript_id],
  prompt: prompt,
  final_model: final_model,
  max_output_size: 1000,
};

const result = await axios.post(
  "https://api.assemblyai.com/lemur/v3/generate/task",
  data,
  { headers }
);
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp {6}
var data = new
{
  transcript_ids = transcriptIds,
  prompt = prompt,
  final_model = final_model,
  max_output_size = 1000
};

var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");
using var response = await httpClient.PostAsync("https://api.assemblyai.com/lemur/v3/generate/task", content);
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby {6}
request = Net::HTTP::Post.new("https://api.assemblyai.com/lemur/v3/generate/task", headers)
request.body = {
  transcript_ids: [transcript_id],
  prompt: prompt,
  final_model: final_model,
  max_output_size: 1000
}.to_json

response = http.request(lemur_request)
```

  </Tab>
  <Tab language="php" title="PHP">

```php {10}
$ch = curl_init("https://api.assemblyai.com/lemur/v3/generate/task");
curl_setopt_array($ch, [
    CURLOPT_RETURNTRANSFER => true,
    CURLOPT_POST => true,
    CURLOPT_HTTPHEADER => $headers,
    CURLOPT_POSTFIELDS => json_encode([
        'transcript_ids' => [$transcript_id]
        'prompt' => $prompt,
        'final_model' => $final_model,
        'max_output_size' => 1000
    ])
]);

$response = curl_exec($ch);
```

  </Tab>
</Tabs>

## Change the temperature

You can change the temperature by specifying the `temperature` parameter, ranging from 0.0 to 1.0.

Higher values result in answers that are more creative, lower values are more conservative.

<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>

```python {4}
result = transcript.lemur.task(
    prompt,
    final_model,
    temperature=0.7
)
```

  </Tab>

  <Tab language="python" title="Python">

```python {5}
data = {
  "prompt": prompt,
  "transcript_ids": [transcript_id],
  "final_model": final_model,
  "temperature": 0.7
}

result = requests.post("https://api.assemblyai.com/lemur/v3/generate/task", headers=headers, json=data)
```

  </Tab>

  <Tab language="javascript-sdk" title="JavaScript SDK">

```javascript {5}
const { response } = await client.lemur.task({
  prompt,
  transcript_ids: [transcript.id],
  final_model,
  temperature: 0.7,
});
```

  </Tab>
  <Tab language="javascript" title="JavaScript">

```javascript {5}
const data = {
  transcript_ids: [transcript_id],
  prompt: prompt,
  final_model: final_model,
  temperature: 0.7,
};

const result = await axios.post(
  "https://api.assemblyai.com/lemur/v3/generate/task",
  data,
  { headers }
);
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp {6}
var data = new
{
  transcript_ids = transcriptIds,
  prompt,
  final_model = final_model,
  temperature = 0.7
};

var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");
using var response = await httpClient.PostAsync("https://api.assemblyai.com/lemur/v3/generate/task", content);
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby {6}
request = Net::HTTP::Post.new("https://api.assemblyai.com/lemur/v3/generate/task", headers)
request.body = {
  transcript_ids: [transcript_id],
  prompt: prompt,
  final_model: final_model,
  temperature: 0.7
}.to_json

response = http.request(lemur_request)

```

  </Tab>
  <Tab language="php" title="PHP">

```php {10}
$ch = curl_init("https://api.assemblyai.com/lemur/v3/generate/task");
curl_setopt_array($ch, [
    CURLOPT_RETURNTRANSFER => true,
    CURLOPT_POST => true,
    CURLOPT_HTTPHEADER => $headers,
    CURLOPT_POSTFIELDS => json_encode([
        'transcript_ids' => [$transcript_id]
        'prompt' => $prompt,
        'final_model' => $final_model,
        'temperature' => 0.7
    ])
]);

$response = curl_exec($ch);
```

  </Tab>
</Tabs>

## Send customized input

You can submit custom text inputs to LeMUR without transcript IDs. This allows you to customize the input, for example, you could include the speaker labels for the LLM.

To submit custom text input, use the `input_text` parameter

<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>

```python {8}
text_with_speaker_labels = ""
for utt in transcript.utterances:
    text_with_speaker_labels += f"Speaker {utt.speaker}:\n{utt.text}\n"

result = aai.Lemur().task(
    prompt,
    final_model,
    input_text=text_with_speaker_labels
)
```

  </Tab>

  <Tab language="python" title="Python">

```python {8}
text_with_speaker_labels = ""
for utt in transcript["utterances"]:
  text_with_speaker_labels += f"Speaker {utt["speaker"]}:\n{utt["text"]}\n"

data = {
  "prompt": prompt,
  "final_model": final_model,
  "input_text": text_with_speaker_labels
}

result = requests.post("https://api.assemblyai.com/lemur/v3/generate/task", headers=headers, json=data)
```

  </Tab>

  <Tab language="javascript-sdk" title="JavaScript SDK">

```javascript {9}
let textWithSpeakerLabels = "";
for (const utt of transcript.utterances) {
  textWithSpeakerLabels += `Speaker ${utt.speaker}:\n${utt.text}\n`;
}

const { response } = await client.lemur.task({
  prompt: prompt,
  final_model: final_model,
  input_text: textWithSpeakerLabels,
});
```

  </Tab>
  <Tab language="javascript" title="JavaScript">

```javascript {9}
let textWithSpeakerLabels = "";
for (const utt of transcript.utterances) {
  textWithSpeakerLabels += `Speaker ${utt.speaker}:\n${utt.text}\n`;
}

const data = {
  prompt: prompt,
  final_model: final_model,
  input_text: textWithSpeakerLabels,
};

const result = await axios.post(
  "https://api.assemblyai.com/lemur/v3/generate/task",
  data,
  { headers }
);
```

  </Tab>
  
  <Tab language="csharp" title="C#">

```csharp {11}
string textWithSpeakerLabels = "";
foreach (var utt in transcript.utterances)
{
  textWithSpeakerLabels += $"Speaker {utt.speaker}:\n{utt.text}\n";
}

var data = new
{
  prompt,
  final_model = final_model,
  input_text = textWithSpeakerLabels
};

var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");
using var response = await httpClient.PostAsync("https://api.assemblyai.com/lemur/v3/generate/task", content);
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby {10}
text_with_speaker_labels = ""
transcript["utterances"].each do |utt|
  text_with_speaker_labels += "Speaker #{utt.speaker}:\n#{utt.text}\n"
end

request = Net::HTTP::Post.new("https://api.assemblyai.com/lemur/v3/generate/task", headers)
request.body = {
  prompt: prompt,
  final_model: final_model,
  input_text: text_with_speaker_labels
}.to_json

response = http.request(lemur_request)
```

  </Tab>

  <Tab language="php" title="PHP">

```php {14}
$text_with_speaker_labels = "";
foreach ($transcript['utterances'] as $utt) {
  $text_with_speaker_labels .= "Speaker {$utt['speaker']}:\n{$utt['text']}\n";
}

$ch = curl_init("https://api.assemblyai.com/lemur/v3/generate/task");
curl_setopt_array($ch, [
    CURLOPT_RETURNTRANSFER => true,
    CURLOPT_POST => true,
    CURLOPT_HTTPHEADER => $headers,
    CURLOPT_POSTFIELDS => json_encode([
        'prompt' => $prompt,
        'final_model' => $final_model,
        'input_text' => $text_with_speaker_labels
    ])
]);

$response = curl_exec($ch);
```

  </Tab>
</Tabs>

<Note>
The total input limit for the `prompt`, `context`, and `input_text` parameters combined is 200,000 tokens. This reflects the maximum context window supported by the model. Be sure to account for all three fields when structuring your request.
</Note>

## Submit multiple transcripts

LeMUR can easily ingest multiple transcripts in a single API call.

You can submit up to 100 hours of audio.

<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>

```python {1-7}
transcript_group = transcriber.transcribe_group(
    [
        "https://example.org/customer1.mp3",
        "https://example.org/customer2.mp3",
        "https://example.org/customer3.mp3",
    ],
)

# Or use existing transcripts:
# transcript_group = aai.TranscriptGroup.get_by_ids([id1, id2, id3])

result = transcript_group.lemur.task(
  prompt="Provide a summary of these customer calls."
)
```

  </Tab>

  <Tab language="python" title="Python">

```python {3}
data = {
  "prompt": prompt,
  "transcript_ids": [id1, id2, id3],
  "final_model": final_model,
}

result = requests.post("https://api.assemblyai.com/lemur/v3/generate/task", headers=headers, json=data)
```

  </Tab>
  <Tab language="javascript-sdk" title="JavaScript SDK">

```javascript {2}
const { response } = await client.lemur.task({
  transcript_ids: [id1, id2, id3],
  prompt: "Provide a summary of these customer calls.",
});
```

  </Tab>
  <Tab language="javascript" title="JavaScript">

```javascript {2}
const data = {
  transcript_ids: [id1, id2, id3],
  prompt: prompt,
  final_model: final_model,
};

const result = await axios.post(
  "https://api.assemblyai.com/lemur/v3/generate/task",
  data,
  { headers }
);
```

  </Tab>

  <Tab language="csharp" title="C#">

```csharp {3}
var data = new
{
  transcript_ids = new List<string> { id1, id2, id3 },
  prompt = prompt,
  final_model = final_model,
};

var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");
using var response = await httpClient.PostAsync("https://api.assemblyai.com/lemur/v3/generate/task", content);
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby {3}
request = Net::HTTP::Post.new("https://api.assemblyai.com/lemur/v3/generate/task", headers)
request.body = {
  transcript_ids: [id1, id2, id3],
  prompt: prompt,
  final_model: final_model
}.to_json

response = http.request(request)
```

  </Tab>
  <Tab language="php" title="PHP">

```php {7}
$ch = curl_init("https://api.assemblyai.com/lemur/v3/generate/task");
curl_setopt_array($ch, [
    CURLOPT_RETURNTRANSFER => true,
    CURLOPT_POST => true,
    CURLOPT_HTTPHEADER => $headers,
    CURLOPT_POSTFIELDS => json_encode([
        'transcript_ids' => [$id1, $id2, $id3],
        'prompt' => $prompt,
        'final_model' => $final_model
    ])
]);

$response = curl_exec($ch);
```

  </Tab>
</Tabs>

## Delete data

You can delete the data for a previously submitted LeMUR request.

Response data from the LLM, as well as any context provided in the original request will be removed.

<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>

```python {3}
result = transcript.lemur.task(prompt)

deletion_response = aai.Lemur.purge_request_data(result.request_id)
```

  </Tab>
  <Tab language="python" title="Python">

```python {5}
# First get the request_id from a previous LeMUR task response
request_id = result.json()["request_id"]

delete_url = f"https://api.assemblyai.com/lemur/v3/{request_id}"
deletion_response = requests.delete(delete_url, headers=headers)
```

  </Tab>
  <Tab language="javascript-sdk" title="JavaScript SDK">

```javascript {6}
const { response, request_id } = await client.lemur.task({
  transcript_ids: [transcript.id],
  prompt,
});

const deletionResponse = await client.lemur.purgeRequestData(request_id);
```

  </Tab>
  <Tab language="javascript" title="JavaScript">

```javascript {5}
// First get the request_id from a previous LeMUR task response
const request_id = result.data.request_id;

const delete_url = `https://api.assemblyai.com/lemur/v3/${request_id}`;
const deletion_response = await axios.delete(delete_url, { headers });
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp {5}
// First get the request_id from a previous LeMUR task response
string request_id = lemurResponse.RequestId;

string delete_url = $"https://api.assemblyai.com/lemur/v3/{request_id}";
using var deletion_response = await httpClient.DeleteAsync(delete_url);
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby {6}
# First get the request_id from a previous LeMUR task response
request_id = lemur_result["request_id"]

delete_uri = URI("#{base_url}/lemur/v3/#{request_id}")
delete_request = Net::HTTP::Delete.new(delete_uri, headers)
deletion_response = http.request(delete_request)
```

  </Tab>
  <Tab language="php" title="PHP">

```php {12}
// First get the request_id from a previous LeMUR task response
$request_id = $result['request_id'];

$delete_url = "https://api.assemblyai.com/lemur/v3/{$request_id}";
$ch = curl_init($delete_url);
curl_setopt_array($ch, [
    CURLOPT_RETURNTRANSFER => true,
    CURLOPT_CUSTOMREQUEST => "DELETE",
    CURLOPT_HTTPHEADER => $headers
]);

$deletion_response = curl_exec($ch);
curl_close($ch);
```

  </Tab>
</Tabs>

## API reference

You can find detailed information about all LeMUR API endpoints and parameters in the [LeMUR API reference](https://assemblyai.com/docs/api-reference/lemur).


---
title: Prompt Engineering
description: Learn how to improve your prompts.
---

Learn how to get better results from LeMUR by using prompting techniques to optimize your prompt. This page walks you through the parts of a prompt and gives you tips on how to use them to improve your prompts.

<Info title="Large language models">
  If you want to learn about LLMs first, see our blog post [Introduction to
  Large Language
  Models](https://www.assemblyai.com/blog/introduction-large-language-models-generative-ai/).
</Info>

<Tip title="Prompt engineering">

Writing good prompts is both an art and a science that has given rise to an entire field called _prompt engineering_.

If you want to learn more, see our crash course on prompt engineering:

<iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/aOm75o2Z5-o?si=bDWFpSPQ0PLpW6kK"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  allowfullscreen
></iframe>

</Tip>

## Anatomy of a prompt

Through a learning process called instruction fine-tuning, modern LLMs are now capable of performing specific tasks based on explicit instructions, also called _prompts_.

A prompt is a piece of text that guides the LLM to generate a response.

It can be as simple as a single sentence:

```
Provide a summary of the meeting.
```

However, if you want to improve your response, you can include additional information to help the LLM generate more desirable results.

A prompt can be broken down into three parts:

- **Instruction:** The instruction, or question, that you want the LLM to help you with.
- **Context:** Additional information that help the LLM understand the instruction.
- **Format:** The format of the response that you want the LLM to generate.

At a minimum, a prompt should include an instruction. However, by specifying context and the desired format, you can greatly improve the quality of the response.

If you include at least one of context and format, use the following format:

```plain
<YOUR_INSTRUCTION>

Context:
<YOUR_CONTEXT>

Answer Format:
<YOUR_FORMAT>
```

Replace `<YOUR_INSTRUCTION>`, `<YOUR_CONTEXT>`, and `<YOUR_FORMAT>` with your own text.

In the following sections, you'll learn about each part in more detail.

## Instruction

A good instruction is clear and concise. It should tell the LLM exactly what you want it to generate. It should also be short enough that the LLM can understand it.

### Prefer instructions over questions

Questions can often be ambiguous and may lead to unexpected results. Instead, use instructions starting with an imperative verb to tell the LLM what you want.

- **Recommended:** `List the key features of the product.`
- **Not recommended:** `What are the features of the product?`

### Avoid compound instructions

Focus on one task at a time. Including multiple instructions in a single prompt may confuse the LLM and lead to less accurate results.

- **Recommended:** `Identify action items from the meeting.`
- **Not recommended:** `Identify action items from the meeting and list highlights from each speaker.`

## Context

Add a `Context:` section to your prompt to provide additional information to the LLM. This can help the LLM generate more accurate results.

<Note title="Transcription text">
  LeMUR adds the transcription text as context to your prompt by default. You
  don't need to include it in your prompt.
</Note>

### Provide definitions

You can use context to define any terms that the LLM may not understand.

```plain
Identify action items from the meeting.

Context:
Action items are tasks for the participants to complete after the meeting.
```

### Provide examples

Rather than telling the LLM what to generate, you can show it examples of the output you want to generate.

```plain
Identify action items from the meeting.

Context:
Action item examples from other meetings:
- Schedule a follow-up meeting with the client to address their concerns.
- Review the proposal and provide your feedback by the end of the week.
- Complete the data analysis and share the results with the team.
```

## Format

You can tell LeMUR how the format should look like by adding a `Answer Format:` section to your prompt.

```plain
Provide a summary of the podcast.

Context:
This is an episode of the Lex Fridman podcast with guest Sam Altman, the CEO of OpenAI.

Answer Format:
catchy title, no longer than 10 words
```

You can also use the answer format to generate structured data, such as JSON or Markdown.

**JSON:**

```plain
Identify action items from the meeting.

Answer Format:
{
  "assignee": <assignee>,
  "action_item": <action item>,
  "due_date": <due_date>
}
```

**Markdown:**

```plain
Provide a summary of the meeting.

Context:
The AssemblyAI marketing team is meeting to discuss a new marketing campaign.

Answer Format:
**<topic header>**
<topic summary>
```

### Use tags to define placeholders

Use tags to insert dynamic content into your prompt. A tag is a description of a piece of information, surrounded by angle brackets, `<...>`.

In the format section of your prompt, place the tag where you want to insert the value.

```plain
Answer Format:
<OVERALL SUMMARY>

Number of participants: <NUMBER OF PARTICIPANTS>
```

### Remove the preamble

Sometimes the LLM generates a response that includes an introduction to the result, also known as a _preamble_. For example:

```
Here's a summary of the meeting:

...
```

To remove the preamble, you can ask the LLM to leave it out:

```
Provide a summary of the meeting. Do not provide a preamble.
```

## Learn more

If you want to see more examples of prompts, see the [LeMUR Examples](/docs/lemur/examples).

For more tips on prompt engineering, see [Awesome Prompt Engineering](https://github.com/promptslab/Awesome-Prompt-Engineering) on GitHub.


    ---
title: Summarization
description: Generate a single summary of your entire audio file
---

import { LanguageTable } from "../../assets/components/LanguagesTable";

<Accordion title="Supported languages">
  <LanguageTable
    languages={[
      { name: "Global English", code: "en" },
      { name: "Australian English", code: "en_au" },
      { name: "British English", code: "en_uk" },
      { name: "US English", code: "en_us" },
    ]}
    columns={2}
  />
  <br />
</Accordion>

Distill important information by summarizing your audio files.

The Summarization model generates a summary of the resulting transcript. You can control the style and format of the summary using [Summary models](#summary-models) and [Summary types](#summary-types).

<Warning title="Summarization and Auto Chapters">
  You can only enable one of the Summarization and [Auto
  Chapters](/docs/audio-intelligence/auto-chapters) models in the same
  transcription.
</Warning>

## Quickstart

<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>
  
  Enable Summarization by setting `summarization` to `True` in the transcription config. Use `summary_model` and `summary_type` to change the summary format.

If you specify one of `summary_model` or `summary_type`, then you must specify the other.

The following example returns an informative summary in a bulleted list.

```python {9-11}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(
  summarization=True,
  summary_model=aai.SummarizationModel.informative,
  summary_type=aai.SummarizationType.bullets
)

transcript = aai.Transcriber().transcribe(audio_file, config)

print(f"Transcript ID: ", transcript.id)
print(transcript.summary)
```

  </Tab>
  <Tab language="python" title="Python" default>
  
  Enable Summarization by setting `summarization` to `True` in the JSON payload. Use `summary_model` and `summary_type` to change the summary format.

If you specify one of `summary_model` or `summary_type`, then you must specify the other.

The following example returns an informative summary in a bulleted list.

```python {19-21}
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/v2/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "summarization": True,
    "summary_model": "informative",
    "summary_type": "bullets"
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    print(f"Transcript ID: ", transcript_id)
    print(transcription_result['summary'])
    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)

```

  </Tab>
  <Tab language="javascript-sdk" title="JavaScript SDK">
  
  Enable Summarization by setting `summarization` to `true` in the transcription config. Use `summary_model` and `summary_type` to change the summary format.

If you specify one of `summary_model` or `summary_type`, then you must specify the other.

The following example returns an informative summary in a bulleted list.

```javascript {12-14}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  summarization: true,
  summary_model: "informative",
  summary_type: "bullets",
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  console.log("Transcript ID: ", transcript.id);
  console.log(transcript.summary);
};

run();
```

  </Tab>
  <Tab language="javascript" title="JavaScript">
  
  Enable Summarization by setting `summarization` to `true` in the JSON payload. Use `summary_model` and `summary_type` to change the summary format.

If you specify one of `summary_model` or `summary_type`, then you must specify the other.

The following example returns an informative summary in a bulleted list.

```javascript {19-21}
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  summarization: true,
  summary_model: "informative",
  summary_type: "bullets",
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log("Transcript ID: ", transcriptionResult.id);
    console.log(transcriptionResult.summary);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  Enable Summarization by setting `summarization` to `true` in the JSON payload. Use `summary_model` and `summary_type` to change the summary format.

If you specify one of `summary_model` and `summary_type`, then you must specify the other.

The following example returns an informative summary in a bulleted list.

<Info>
  Most of these libraries are included by default, but on .NET Framework and
  Mono you need to reference the System.Net.Http library and install the
  [System.Net.Http.Json NuGet
  package](https://www.nuget.org/packages/System.Net.Http.Json).
</Info>

```csharp {52-54}
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Threading.Tasks;
using System.Text.Json.Serialization;

class Program
{
    static async Task Main(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Authorization =
                new AuthenticationHeaderValue("<YOUR_API_KEY>");

            var uploadUrl = await UploadFileAsync("./my-audio.mp3", httpClient);

            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);

            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            // Print the transcript ID and summary
            Console.WriteLine($"Transcript ID: {transcript.Id}");
            Console.WriteLine(transcript.Summary);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var fileContent = new StreamContent(fileStream))
        {
            fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent))
            {
                response.EnsureSuccessStatusCode();
                var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
                return jsonDoc.RootElement.GetProperty("upload_url").GetString();
            }
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new {
            audio_url = audioUrl,
            summarization = true,
            summary_model = "informative",
            summary_type = "bullets"
        };

        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            if (transcript.Status == "completed")
            {
                return transcript;
            }
            else if (transcript.Status == "error")
            {
                throw new Exception($"Transcription failed: {transcript.Error}");
            }
            else
            {
                await Task.Delay(TimeSpan.FromSeconds(3));
            }
        }
    }

    public class Transcript
    {
        [JsonPropertyName("id")]
        public string Id { get; set; }

        [JsonPropertyName("status")]
        public string Status { get; set; }

        [JsonPropertyName("text")]
        public string Text { get; set; }

        [JsonPropertyName("summary")]
        public string Summary { get; set; }

        [JsonPropertyName("error")]
        public string Error { get; set; }
    }
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  Enable Summarization by setting `summarization` to `true` in the JSON payload. Use `summary_model` and `summary_type` to change the summary format.

If you specify one of `summary_model` or `summary_type`, then you must specify the other.

The following example returns an informative summary in a bulleted list.

```ruby {23-25}
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "/my_audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
    "summarization" => true,
    "summary_model" => "informative",
    "summary_type" => "bullets"
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts "Transcription text: #{transcription_result['summary']}"
    break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    sleep(3)
  end
end
```

  </Tab>
  <Tab language="php" title="PHP">
  
  Enable Summarization by setting `summarization` to `true` in the JSON payload. Use `summary_model` and `summary_type` to change the summary format.

If you specify one of `summary_model` or `summary_type`, then you must specify the other.

The following example returns an informative summary in a bulleted list.

```php {30-32}
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./local_file.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "summarization" => true,
    "summary_model" => "informative",
    "summary_type" => "bullets"
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        echo $transcription_result['summary'];
        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

  </Tab>
</Tabs>

### Example output

```plain
- Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines from Maine to Maryland to Minnesota are gray and smoggy. In some places, the air quality warnings include the warning to stay inside.
- Air pollution levels in Baltimore are considered unhealthy. Exposure to high levels can lead to a host of health problems. With climate change, we are seeing more wildfires. Will we be seeing more of these kinds of wide ranging air quality consequences?
```

<Tip title="Custom Summaries Using LeMUR">
  If you want more control of the output format, see how to generate a [Custom
  summary using LeMUR](/docs/lemur/summarize-audio).
</Tip>

## API reference

### Request

```bash {6-8}
curl https://api.assemblyai.com/v2/transcript \
--header "Authorization: <YOUR_API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "audio_url": "YOUR_AUDIO_URL",
  "summarization": true,
  "summary_model": "informative",
  "summary_type": "bullets"
}'
```

| Key             | Type    | Description                                                    |
| --------------- | ------- | -------------------------------------------------------------- |
| `summarization` | boolean | Enable Summarization.                                          |
| `summary_type`  | string  | [Summary type](#summary-types) to use for the transcription.   |
| `summary_model` | string  | [Summary model](#summary-models) to use for the transcription. |

### Response

<Json
  json={{
    summary:
      "- Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines from Maine to Maryland to Minnesota are gray and smoggy. In some places, the air quality warnings include the warning to stay inside.\n- Air pollution levels in Baltimore are considered unhealthy. Exposure to high levels can lead to a host of health problems. With climate change, we are seeing more wildfires. Will we be seeing more of these kinds of wide ranging air quality consequences?",
  }}
/>


| Key       | Type   | Description                  |
| --------- | ------ | ---------------------------- |
| `summary` | string | A summary of the audio file. |

The response also includes the request parameters used to generate the transcript.

### Summary types

The summary type determines both the length and the format of the summary, for example as a bulleted list or a paragraph.

| Value               | Description                                                           | Example                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| ------------------- | --------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `bullets` (default) | A bulleted summary with the most important points.                    | - The human brain has nearly tripled in mass in two million years. <br /> - One of the main reasons that our brain got so big is because it got a new part, called the frontal lobe.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| `bullets_verbose`   | A longer bullet point list summarizing the entire transcription text. | Dan Gilbert is a psychologist and a happiness expert. His talk is recorded live at Ted conference. He explains why the human brain has nearly tripled in size in 2 million years. He also explains the difference between winning the lottery and becoming a paraplegic.\n- In 1994, Pete Best said he's happier than he would have been with the Beatles. In the free choice paradigm, monet prints are ranked from the one they like the most to the one that they don't. People prefer the third one over the fourth one because it's a little better.\n- People synthesize happiness when they change their affective. Hedonic aesthetic to make up your mind and change your mind is the friend of natural happiness. But it's the enemy of synthetic happiness. The psychological immune system works best when we are stuck. This is the difference between dating and marriage. People don't know this about themselves and it can work to their disadvantage.\n- In a photography course at Harvard, 66% of students choose not to take the course where they have the opportunity to change their mind. Adam Smith said that some things are better than others. Dan Gilbert recorded at Ted, 2004 in Monterey, California, 2004. |
| `gist`              | A few words summarizing the entire transcription text.                | A big brain                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| `headline`          | A single sentence summarizing the entire transcription text.          | The human brain has nearly tripled in mass in two million years.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| `paragraph`         | A single paragraph summarizing the entire transcription text.         | The human brain has nearly tripled in mass in two million years. It went from the one-and-a-quarter-pound brain of our ancestor, habilis, to the almost three-pound meatloaf everybody here has between their ears.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |

### Summary models

The summary model determines the style and tone of the summary.

| Value                   | When to use                                                                                | Supported summary type                                   | Required parameters                                                              |
| ----------------------- | ------------------------------------------------------------------------------------------ | -------------------------------------------------------- | -------------------------------------------------------------------------------- |
| `informative` (default) | Best for files with a single speaker, such as presentations or lectures.                   | `bullets`, `bullets_verbose`, `headline`, or `paragraph` | `punctuate` and `format_text` set to `true`                                      |
| `conversational`        | Best for any 2-person conversation, such as customer/agent or interview/interviewee calls. | `bullets`, `bullets_verbose`, `headline`, or `paragraph` | `punctuate`, `format_text`, and `speaker_labels` or `multichannel` set to `true` |
| `catchy`                | Best for creating video, podcast, or media titles.                                         | `headline`, or `gist`                                    | `punctuate` and `format_text` set to `true`                                      |

## Frequently asked questions

<Accordion title="How long does it take to generate a summarization output?" theme="dark" iconColor="white" >
  
The inference speed of the Summary model depends on the desired output length. However, a single batch can be processed in less than 1 second.

  </Accordion>

<Accordion title="Can I extract individual words and their corresponding speaker labels with Summarization?" theme="dark" iconColor="white" >
  
No. Summarization only generates a single abstractive summary of the entire audio file, and doesn't provide word-level information or speaker labels. If you need word-level information, consider using [Word-level timestamps](/docs/speech-to-text/pre-recorded-audio/word-level-timestamps) and [Speaker Diarization](/docs/speech-to-text/speaker-diarization) instead.

  </Accordion>


---
title: Content Moderation
description: Detect sensitive content in your audio files
---

import { LanguageTable } from "../../assets/components/LanguagesTable";

<Accordion title="Supported languages">
  <LanguageTable
    languages={[
      { name: "Global English", code: "en" },
      { name: "Australian English", code: "en_au" },
      { name: "British English", code: "en_uk" },
      { name: "US English", code: "en_us" },
      { name: "Spanish", code: "es" },
      { name: "French", code: "fr" },
      { name: "German", code: "de" },
      { name: "Italian", code: "it" },
      { name: "Portuguese", code: "pt" },
    ]}
    columns={2}
  />
  <br />
</Accordion>

The Content Moderation model lets you detect inappropriate content in audio files to ensure that your content is safe for all audiences.

The model pinpoints sensitive discussions in spoken data and their severity.

## Quickstart

<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>
  
  Enable Content Moderation by setting `content_safety` to `True` in the transcription config.

```python {8}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(content_safety=True)

transcript = aai.Transcriber().transcribe(audio_file, config)

print(f"Transcript ID:", transcript.id)

for result in transcript.content_safety.results:
    print(result.text)
    print(f"Timestamp: {result.timestamp.start} - {result.timestamp.end}")

    # Get category, confidence, and severity.
    for label in result.labels:
        print(f"{label.label} - {label.confidence} - {label.severity}")  # content safety category

# Get the confidence of the most common labels in relation to the entire audio file.
for label, confidence in transcript.content_safety.summary.items():
    print(f"{confidence * 100}% confident that the audio contains {label}")

# Get the overall severity of the most common labels in relation to the entire audio file.
for label, severity_confidence in transcript.content_safety.severity_score_summary.items():
    print(f"{severity_confidence.low * 100}% confident that the audio contains low-severity {label}")
    print(f"{severity_confidence.medium * 100}% confident that the audio contains medium-severity {label}")
    print(f"{severity_confidence.high * 100}% confident that the audio contains high-severity {label}")
```

  </Tab>
  <Tab language="python" title="Python" default>
  
  Enable Content Moderation by setting `content_safety` to `True` in the JSON payload.

```python {19}
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./local_file.mp3", "rb") as f:
    response = requests.post(base_url + "/v2/upload",
                            headers=headers,
                            data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "content_safety": True
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

print(f"Transcript ID:", transcript_id)

while True:
    transcription_result = requests.get(polling_endpoint, headers=headers).json()

    if transcription_result['status'] == 'completed':

        for result in transcription_result['content_safety_labels']['results']:
            print(result['text'])
            print(f"Timestamp: {result['timestamp']['start']} - {result['timestamp']['end']}")

            # Get category, confidence, and severity.
            for label in result['labels']:
                print(f"{label['label']} - {label['confidence']} - {label['severity']}")  # content safety category

        # Get the confidence of the most common labels in relation to the entire audio file.
        for label, confidence in transcription_result['content_safety_labels']['summary'].items():
            print(f"{confidence * 100}% confident that the audio contains {label}")

        # Get the overall severity of the most common labels in relation to the entire audio file.
        for label, severity_confidence in transcription_result['content_safety_labels']['severity_score_summary'].items():
            print(f"{severity_confidence['low'] * 100}% confident that the audio contains low-severity {label}")
            print(f"{severity_confidence['medium'] * 100}% confident that the audio contains medium-severity {label}")
            print(f"{severity_confidence['high'] * 100}% confident that the audio contains high-severity {label}")
        break
    elif transcription_result['status'] == 'error':
        raise RuntimeError(f"Transcription failed: {transcription_result['error']}")
    else:
        time.sleep(3)
```

  </Tab>
  <Tab language="javascript-sdk" title="JavaScript SDK">
  
  Enable Content Moderation by setting `content_safety` to `true` in the transcription config.

```javascript {12}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  content_safety: true,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);
  console.log(`Transcript ID: ${transcript.id}`);
  const contentSafetyLabels = transcript.content_safety_labels;

  // Get the parts of the transcript which were flagged as sensitive
  for (const result of contentSafetyLabels.results) {
    console.log(result.text);
    console.log(
      `Timestamp: ${result.timestamp.start} - ${result.timestamp.end}`
    );

    // Get category, confidence, and severity
    for (const label of result.labels) {
      console.log(`${label.label} - ${label.confidence} - ${label.severity}`);
    }
  }

  // Get the confidence of the most common labels in relation to the entire audio file
  for (const [label, confidence] of Object.entries(
    contentSafetyLabels.summary
  )) {
    console.log(
      `${confidence * 100}% confident that the audio contains ${label}`
    );
  }

  // Get the overall severity of the most common labels in relation to the entire audio file
  for (const [label, severity_confidence] of Object.entries(
    contentSafetyLabels.severity_score_summary
  )) {
    console.log(
      `${severity_confidence.low * 100}% confident that the audio contains low-severity ${label}`
    );
    console.log(
      `${severity_confidence.medium * 100}% confident that the audio contains medium-severity ${label}`
    );
    console.log(
      `${severity_confidence.high * 100}% confident that the audio contains high-severity ${label}`
    );
  }
};
run();
```

  </Tab>
  <Tab language="javascript" title="JavaScript">
  
  Enable Content Moderation by setting `content_safety` to `true` in the JSON payload.

```javascript {19}
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  content_safety: true,
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
console.log("Transcript ID: ", transcriptId);

const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    const contentSafetyLabels = transcriptionResult.content_safety_labels;
    for (const result of contentSafetyLabels.results) {
      console.log(result.text);
      console.log(
        `Timestamp: ${result.timestamp.start} - ${result.timestamp.end}`
      );

      // Get category, confidence, and severity.
      for (const label of result.labels) {
        console.log(`${label.label} - ${label.confidence} - ${label.severity}`); // content safety category
      }
    }
    // Get the confidence of the most common labels in relation to the entire audio file
    for (const [label, confidence] of Object.entries(
      contentSafetyLabels.summary
    )) {
      console.log(
        `${confidence * 100}% confident that the audio contains ${label}`
      );
    }
    // Get the confidence of the most common labels in relation to the entire audio file.
    for (const [label, severity_confidence] of Object.entries(
      contentSafetyLabels.severity_score_summary
    )) {
      console.log(
        `${severity_confidence.low * 100}% confident that the audio contains low-severity ${label}`
      );
      console.log(
        `${severity_confidence.medium * 100}% confident that the audio contains medium-severity ${label}`
      );
      console.log(
        `${severity_confidence.high * 100}% confident that the audio contains high-severity ${label}`
      );
    }

    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  Enable Content Moderation by setting `content_safety` to `true` in the JSON payload.

<Info>
  Most of these libraries are included by default, but on .NET Framework and
  Mono you need to reference the System.Net.Http library and install the
  [System.Net.Http.Json NuGet
  package](https://www.nuget.org/packages/System.Net.Http.Json).
</Info>

```csharp {53}
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Threading.Tasks;
using System.Text.Json.Serialization;
using System.Collections.Generic;

class Program
{
    static async Task Main(string[] args)
    {
        string baseUrl = "https://api.assemblyai.com";

        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Authorization =
                new AuthenticationHeaderValue("<YOUR_API_KEY>");

            string uploadUrl = await UploadFileAsync("./local_file.mp3", httpClient, baseUrl);

            var transcript = await CreateTranscriptWithContentSafetyAsync(uploadUrl, httpClient, baseUrl);

            Console.WriteLine($"Transcript ID: {transcript.Id}");
            transcript = await WaitForTranscriptToProcessAndAnalyzeContentSafety(transcript, httpClient, baseUrl);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient, string baseUrl)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var fileContent = new StreamContent(fileStream))
        {
            fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            using (var response = await httpClient.PostAsync($"{baseUrl}/v2/upload", fileContent))
            {
                response.EnsureSuccessStatusCode();
                var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
                return jsonDoc.RootElement.GetProperty("upload_url").GetString();
            }
        }
    }

    static async Task<Transcript> CreateTranscriptWithContentSafetyAsync(string audioUrl, HttpClient httpClient, string baseUrl)
    {
        var data = new
        {
            audio_url = audioUrl,
            content_safety = true
        };

        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync($"{baseUrl}/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcessAndAnalyzeContentSafety(Transcript transcript, HttpClient httpClient, string baseUrl)
    {
        string pollingEndpoint = $"{baseUrl}/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "completed":
                    // Process content safety results
                    if (transcript.ContentSafetyLabels != null)
                    {
                        if (transcript.ContentSafetyLabels.Results != null)
                        {
                            foreach (var result in transcript.ContentSafetyLabels.Results)
                            {
                                Console.WriteLine(result.Text);
                                Console.WriteLine($"Timestamp: {result.Timestamp.Start} - {result.Timestamp.End}");

                                // Get category, confidence, and severity
                                foreach (var label in result.Labels)
                                {
                                    Console.WriteLine($"{label.LabelName} - {label.Confidence} - {label.Severity}");
                                }
                            }
                        }

                        // Get the confidence of the most common labels
                        if (transcript.ContentSafetyLabels.Summary != null)
                        {
                            foreach (var label in transcript.ContentSafetyLabels.Summary)
                            {
                                Console.WriteLine($"{label.Value * 100}% confident that the audio contains {label.Key}");
                            }
                        }

                        // Get the overall severity
                        if (transcript.ContentSafetyLabels.SeverityScoreSummary != null)
                        {
                            foreach (var label in transcript.ContentSafetyLabels.SeverityScoreSummary)
                            {
                                Console.WriteLine($"{label.Value.Low * 100}% confident that the audio contains low-severity {label.Key}");
                                Console.WriteLine($"{label.Value.Medium * 100}% confident that the audio contains medium-severity {label.Key}");
                                Console.WriteLine($"{label.Value.High * 100}% confident that the audio contains high-severity {label.Key}");
                            }
                        }
                    }

                    return transcript;

                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");

                default:
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
            }
        }
    }

    public class Transcript
    {
        [JsonPropertyName("id")]
        public string Id { get; set; }

        [JsonPropertyName("status")]
        public string Status { get; set; }

        [JsonPropertyName("text")]
        public string Text { get; set; }

        [JsonPropertyName("content_safety_labels")]
        public ContentSafetyLabels ContentSafetyLabels { get; set; }

        [JsonPropertyName("error")]
        public string Error { get; set; }
    }

    public class ContentSafetyLabels
    {
        [JsonPropertyName("status")]
        public string Status { get; set; }

        [JsonPropertyName("results")]
        public List<ContentSafetyResult> Results { get; set; }

        [JsonPropertyName("summary")]
        public Dictionary<string, double> Summary { get; set; }

        [JsonPropertyName("severity_score_summary")]
        public Dictionary<string, SeverityScore> SeverityScoreSummary { get; set; }
    }

    public class ContentSafetyResult
    {
        [JsonPropertyName("text")]
        public string Text { get; set; }

        [JsonPropertyName("timestamp")]
        public TimeRange Timestamp { get; set; }

        [JsonPropertyName("labels")]
        public List<Label> Labels { get; set; }

        [JsonPropertyName("sentences_idx_start")]
        public int SentencesIdxStart { get; set; }

        [JsonPropertyName("sentences_idx_end")]
        public int SentencesIdxEnd { get; set; }
    }

    public class TimeRange
    {
        [JsonPropertyName("start")]
        public int Start { get; set; }

        [JsonPropertyName("end")]
        public int End { get; set; }
    }

    public class Label
    {
        [JsonPropertyName("label")]
        public string LabelName { get; set; }

        [JsonPropertyName("confidence")]
        public double Confidence { get; set; }

        [JsonPropertyName("severity")]
        public double Severity { get; set; }
    }

    public class SeverityScore
    {
        [JsonPropertyName("low")]
        public double Low { get; set; }

        [JsonPropertyName("medium")]
        public double Medium { get; set; }

        [JsonPropertyName("high")]
        public double High { get; set; }
    }
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  Enable Content Moderation by setting `content_safety` to `true` in the JSON payload.

```ruby {23}
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "/my_audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
    "content_safety" => true,
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'

    transcription_result['content_safety_labels']['results'].each do |result|
      puts result['text']
      puts "Timestamp: #{result['timestamp']['start']} - #{result['timestamp']['end']}"

      # Get category, confidence, and severity.
      result['labels'].each do |label|
        puts "#{label['label']} - #{label['confidence']} - #{label['severity']}"  # content safety category
      end
    end
    # Get the confidence of the most common labels in relation to the entire audio file.
    transcription_result['content_safety_labels']['summary'].each do |label, confidence|
      puts "#{confidence * 100}% confident that the audio contains #{label}"
    end
    # Get the overall severity of the most common labels in relation to the entire audio file.
    transcription_result['content_safety_labels']['severity_score_summary'].each do |label, severity_confidence|
      puts "#{severity_confidence['low'] * 100}% confident that the audio contains low-severity #{label}"
      puts "#{severity_confidence['medium'] * 100}% confident that the audio contains medium-severity #{label}"
      puts "#{severity_confidence['high'] * 100}% confident that the audio contains high-severity #{label}"
    end
  break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    sleep(3)
  end
end
```

  </Tab>
  <Tab language="php" title="PHP">
  
  Enable Content Moderation by setting `content_safety` to `true` in the JSON payload.

```php {30}
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./local_file.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "content_safety" => true,
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        $content_safety_labels = $transcription_result['content_safety_labels'];

        foreach ($content_safety_labels['results'] as $result) {
          echo $result['text'] . "\n";
          echo "Timestamp: {$result['timestamp']['start']} - {$result['timestamp']['end']}\n";

          // Get category, confidence, and severity.
          foreach ($result['labels'] as $label) {
            echo "{$label['label']} - {$label['confidence']} - {$label['severity']}\n"; // content safety category
          }
        }
        // Get the confidence of the most common labels in relation to the entire audio file.
        foreach ($transcription_result['content_safety_labels']['summary'] as $label => $confidence) {
          echo round($confidence * 100, 2) . "% confident that the audio contains $label\n";
        }
        // Get the confidence of the most common labels in relation to the entire audio file.
        foreach ($content_safety_labels['severity_score_summary'] as $label => $severity_confidence) {
          echo ($severity_confidence['low'] * 100) . "% confident that the audio contains low-severity {$label}\n";
          echo ($severity_confidence['medium'] * 100) . "% confident that the audio contains medium-severity {$label}\n";
          echo ($severity_confidence['high'] * 100) . "% confident that the audio contains high-severity {$label}\n";
        }
        break;
    }  else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

  </Tab>
</Tabs>

### Example output

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines...
Timestamp: 250 - 28920
disasters - 0.8141 - 0.4014

So what is it about the conditions right now that have caused this round of wildfires to...
Timestamp: 29290 - 56190
disasters - 0.9217 - 0.5665

So what is it in this haze that makes it harmful? And I'm assuming it is...
Timestamp: 56340 - 88034
health_issues - 0.9358 - 0.8906

...

99.42% confident that the audio contains disasters
92.70% confident that the audio contains health_issues

57.43% confident that the audio contains low-severity disasters
42.56% confident that the audio contains mid-severity disasters
0.0% confident that the audio contains high-severity disasters
23.57% confident that the audio contains low-severity health_issues
30.22% confident that the audio contains mid-severity health_issues
46.19% confident that the audio contains high-severity health_issues
```

## Adjust the confidence threshold

The confidence threshold determines how likely something is to be flagged as inappropriate content. A threshold of 50% (which is the default) means any label with a confidence score of 50% or greater is flagged.

<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>
  
  To adjust the confidence threshold for your transcription, include `content_safety_confidence` in the transcription config.

```python {4}
# Setting the content safety confidence threshold to 60%.
config = aai.TranscriptionConfig(
  content_safety=True,
  content_safety_confidence=60
)
```

  </Tab>
  <Tab language="python" title="Python" default>
  
  To adjust the confidence threshold for your transcription, include `content_safety_confidence` in the JSON payload.

```python {5}
# Setting the content safety confidence threshold to 60%.
data = {
    "audio_url": upload_url,
    "content_safety": True,
    "content_safety_confidence": 60
}
```

  </Tab>
  <Tab language="javascript-sdk" title="JavaScript SDK">
  
  To adjust the confidence threshold for your transcription, include `content_safety_confidence` in the transcription config.

```javascript {5}
// Setting the content safety confidence threshold to 60%.
const params = {
  audio: audioUrl,
  content_safety: true,
  content_safety_confidence: 60,
};
```

  </Tab>
    <Tab language="javascript" title="JavaScript">
  
  To adjust the confidence threshold for your transcription, include `content_safety_confidence` in the JSON payload.

```javascript {5}
// Setting the content safety confidence threshold to 60%.
const data = {
  audio_url: uploadUrl,
  content_safety: true,
  content_safety_confidence: 60,
};
```

  </Tab>
  <Tab language="csharp" title="C#">
  
 To adjust the confidence threshold for your transcription, include `content_safety_confidence` in the JSON payload.

```csharp {6}
// Setting the content safety confidence threshold to 60%.
        var data = new
        {
            audio_url = audioUrl,
            content_safety = true
            content_safety_confidence = 60
        };
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
 To adjust the confidence threshold for your transcription, include `content_safety_confidence` in the JSON payload.

```ruby {5}
# Setting the content safety confidence threshold to 60%.
data = {
    "audio_url" => upload_url,
    "content_safety" => true,
    "content_safety_confidence" => 60
}
```

  </Tab>
  <Tab language="php" title="PHP">
  
 To adjust the confidence threshold for your transcription, include `content_safety_confidence` in the JSON payload.

```php {5}
// Setting the content safety confidence threshold to 60%.
$data = array(
    "audio_url" => $upload_url,
    "content_safety" => true,
    "content_safety_confidence" => 60
);
```

  </Tab>
</Tabs>

## API reference

### Request

```bash {6-7}
curl https://api.assemblyai.com/v2/transcript \
--header "Authorization: <YOUR_API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "audio_url": "YOUR_AUDIO_URL",
  "content_safety": true,
  "content_safety_confidence": 60
}'
```

| Key                         | Type    | Description                                                                         |
| --------------------------- | ------- | ----------------------------------------------------------------------------------- |
| `content_safety`            | boolean | Enable Content Moderation.                                                          |
| `content_safety_confidence` | integer | The confidence threshold for content moderation. Values must be between 25 and 100. |

### Response

<Json
  json={{
    content_safety_labels: {
      status: "success",
      results: [
        {
          text: "Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines from Maine to Maryland to Minnesota are gray and smoggy. And in some places, the air quality warnings include the warning to stay inside. We wanted to better understand what's happening here and why, so we called Peter de Carlo, an associate professor in the Department of Environmental Health and Engineering at Johns Hopkins University Varsity. Good morning, professor. Good morning.",
          labels: [
            {
              label: "disasters",
              confidence: 0.8142836093902588,
              severity: 0.4093044400215149,
            },
          ],
          sentences_idx_start: 0,
          sentences_idx_end: 5,
          timestamp: {
            start: 250,
            end: 28840,
          },
        },
        {
          text: "What is it about the conditions right now that have caused this round of wildfires to affect so many people so far away? Well, there's a couple of things. The season has been pretty dry already. And then the fact that we're getting hit in the US. Is because there's a couple of weather systems that are essentially channeling the smoke from those Canadian wildfires through Pennsylvania into the Mid Atlantic and the Northeast and kind of just dropping the smoke there.",
          labels: [
            {
              label: "disasters",
              confidence: 0.9228760004043579,
              severity: 0.5578508377075195,
            },
          ],
          sentences_idx_start: 6,
          sentences_idx_end: 10,
          timestamp: {
            start: 29610,
            end: 56142,
          },
        },
        {
          text: "So what is it in this haze that makes it harmful? And I'm assuming it is harmful. It is. The levels outside right now in Baltimore are considered unhealthy. And most of that is due to what's called particulate matter, which are tiny particles, microscopic smaller than the width of your hair that can get into your lungs and impact your respiratory system, your cardiovascular system, and even your neurological your brain. What makes this particularly harmful? Is it the volume of particulant?",
          labels: [
            {
              label: "health_issues",
              confidence: 0.9303749203681946,
              severity: 0.878470242023468,
            },
          ],
          sentences_idx_start: 11,
          sentences_idx_end: 17,
          timestamp: {
            start: 56276,
            end: 88034,
          },
        },
        {
          text: "And so the concentrations of these particles in the air are just much, much higher than we typically see. And exposure to those high levels can lead to a host of health problems. And who is most vulnerable? I noticed that in New York City, for example, they're canceling outdoor activities. And so here it is in the early days of summer, and they have to keep all the kids inside. So who tends to be vulnerable in a situation like this? It's the youngest.",
          labels: [
            {
              label: "health_issues",
              confidence: 0.8302478790283203,
              severity: 0.4810393154621124,
            },
          ],
          sentences_idx_start: 23,
          sentences_idx_end: 29,
          timestamp: {
            start: 113354,
            end: 138754,
          },
        },
        {
          text: "So children, obviously, whose bodies are still developing. The elderly, who are their bodies are more in decline and they're more susceptible to the health impacts of breathing, the poor air quality. And then people who have preexisting health conditions, people with respiratory conditions or heart conditions can be triggered by high levels of air pollution. Could this get worse? That's a good question. In some areas, it's much worse than others. And it just depends on kind of where the smoke is concentrated.",
          labels: [
            {
              label: "health_issues",
              confidence: 0.9725411534309387,
              severity: 0.6577644348144531,
            },
          ],
          sentences_idx_start: 30,
          sentences_idx_end: 36,
          timestamp: {
            start: 138802,
            end: 170370,
          },
        },
        {
          text: "I think New York has some of the higher concentrations right now, but that's going to change as that air moves away from the New York area. But over the course of the next few days, we will see different areas being hit at different times with the highest concentrations. I was going to ask you about more fires start burning. I don't expect the concentrations to go up too much higher.",
          labels: [
            {
              label: "disasters",
              confidence: 0.6661975979804993,
              severity: 0.12955275177955627,
            },
          ],
          sentences_idx_start: 37,
          sentences_idx_end: 40,
          timestamp: {
            start: 170950,
            end: 189030,
          },
        },
        {
          text: "I was going to ask you how and you started to answer this, but how much longer could this last? Or forgive me if I'm asking you to speculate, but what do you think? Well, I think the fires are going to burn for a little bit longer, but the key for us in the US. Is the weather system changing. And so right now, it's kind of the weather systems that are pulling that air into our mid Atlantic and Northeast region.",
          labels: [
            {
              label: "disasters",
              confidence: 0.6248577833175659,
              severity: 0.02552894689142704,
            },
          ],
          sentences_idx_start: 41,
          sentences_idx_end: 45,
          timestamp: {
            start: 189100,
            end: 211082,
          },
        },
        {
          text: "As those weather systems change and shift, we'll see that smoke going elsewhere and not impact us in this region as much. And so I think that's going to be the defining factor. And I think the next couple of days we're going to see a shift in that weather pattern and start to push the smoke away from where we are. And finally, with the impacts of climate change, we are seeing more wildfires.",
          labels: [
            {
              label: "disasters",
              confidence: 0.8657896518707275,
              severity: 0.005704181734472513,
            },
          ],
          sentences_idx_start: 46,
          sentences_idx_end: 49,
          timestamp: {
            start: 211146,
            end: 232354,
          },
        },
        {
          text: "Will we be seeing more of these kinds of wide ranging air quality consequences or circumstances? I mean, that is one of the predictions for climate change. Looking into the future, the fire season is starting earlier and lasting longer, and we're seeing more frequent fires. So, yeah, this is probably something that we'll be seeing more frequently. This tends to be much more of an issue in the Western US. So the eastern US. Getting hit right now is a little bit new.",
          labels: [
            {
              label: "disasters",
              confidence: 0.8090482354164124,
              severity: 0.005799858830869198,
            },
          ],
          sentences_idx_start: 50,
          sentences_idx_end: 56,
          timestamp: {
            start: 232482,
            end: 261760,
          },
        },
      ],
      summary: {
        disasters: 0.9940800441842205,
        health_issues: 0.9216489289040967,
      },
      severity_score_summary: {
        disasters: {
          low: 0.5733263024656846,
          medium: 0.42667369753431533,
          high: 0.0,
        },
        health_issues: {
          low: 0.22863814977924785,
          medium: 0.45014154926938227,
          high: 0.32122030095136983,
        },
      },
    },
  }}
/>


| Key                                                                      | Type   | Description                                                                                                                 |
| ------------------------------------------------------------------------ | ------ | --------------------------------------------------------------------------------------------------------------------------- |
| `content_safety_labels`                                                  | object | An object containing all results of the Content Moderation model.                                                           |
| `content_safety_labels.status`                                           | string | Is either `success`, or `unavailable` in the rare case that the Content Moderation model failed.                            |
| `content_safety_labels.results`                                          | array  | An array of objects, one for each section in the audio file, that the Content Moderation file flagged.                      |
| `content_safety_labels.results[i].text`                                  | string | The transcript of the i-th section flagged by the Content Moderation model.                                                 |
| `content_safety_labels.results[i].labels`                                | array  | An array of objects, one per sensitive topic, that was detected in the i-th section.                                        |
| `content_safety_labels.results[i].labels[j].label`                       | string | The label of the sensitive topic.                                                                                           |
| `content_safety_labels.results[i].labels[j].confidence`                  | number | The confidence score for the j-th topic being discussed in the i-th section, from 0 to 1.                                   |
| `content_safety_labels.results[i].labels[j].severity`                    | number | How severely the j-th topic is discussed in the i-th section, from 0 to 1.                                                  |
| `content_safety_labels.results[i].sentences_idx_start`                   | number | The sentence index at which the i-th section begins.                                                                        |
| `content_safety_labels.results[i].sentences_idx_end`                     | number | The sentence index at which the i-th section ends.                                                                          |
| `content_safety_labels.results[i].timestamp`                             | object | Timestamp information for the i-th section.                                                                                 |
| `content_safety_labels.results[i].timestamp.start`                       | number | The time, in milliseconds, at which the i-th section begins.                                                                |
| `content_safety_labels.results[i].timestamp.end`                         | number | The time, in milliseconds, at which the i-th section ends.                                                                  |
| `content_safety_labels.summary`                                          | object | A summary of the Content Moderation confidence results for the entire audio file.                                           |
| `content_safety_labels.summary.topic`                                    | number | A confidence score for the presence of the sensitive topic "topic" across the entire audio file.                            |
| `content_safety_labels.severity_score_summary`                           | object | A summary of the Content Moderation severity results for the entire audio file.                                             |
| `content_safety_labels.severity_score_summary.topic.[low, medium, high]` | number | A distribution across the values "low", "medium", and "high" for the severity of the presence of "topic" in the audio file. |

The response also includes the request parameters used to generate the transcript.

## Supported labels

| Label                   | Description                                                                                                                                                                                                      | Model output              | Severity |
| ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------- | -------- |
| Accidents               | Any man-made incident that happens unexpectedly and results in damage, injury, or death.                                                                                                                         | `accidents`               | Yes      |
| Alcohol                 | Content that discusses any alcoholic beverage or its consumption.                                                                                                                                                | `alcohol`                 | Yes      |
| Company Financials      | Content that discusses any sensitive company financial information.                                                                                                                                              | `financials`              | No       |
| Crime Violence          | Content that discusses any type of criminal activity or extreme violence that is criminal in nature.                                                                                                             | `crime_violence`          | Yes      |
| Drugs                   | Content that discusses illegal drugs or their usage.                                                                                                                                                             | `drugs`                   | Yes      |
| Gambling                | Includes gambling on casino-based games such as poker, slots, etc. as well as sports betting.                                                                                                                    | `gambling`                | Yes      |
| Hate Speech             | Content that's a direct attack against people or groups based on their sexual orientation, gender identity, race, religion, ethnicity, national origin, disability, etc.                                         | `hate_speech`             | Yes      |
| Health Issues           | Content that discusses any medical or health-related problems.                                                                                                                                                   | `health_issues`           | Yes      |
| Manga                   | Mangas are comics or graphic novels originating from Japan with some of the more popular series being "Pokemon", "Naruto", "Dragon Ball Z", "One Punch Man", and "Sailor Moon".                                  | `manga`                   | No       |
| Marijuana               | This category includes content that discusses marijuana or its usage.                                                                                                                                            | `marijuana`               | Yes      |
| Natural Disasters       | Phenomena that happens infrequently and results in damage, injury, or death. Such as hurricanes, tornadoes, earthquakes, volcano eruptions, and firestorms.                                                      | `disasters`               | Yes      |
| Negative News           | News content with a negative sentiment which typically occur in the third person as an unbiased recapping of events.                                                                                             | `negative_news`           | No       |
| NSFW (Adult Content)    | Content considered "Not Safe for Work" and consists of content that a viewer would not want to be heard/seen in a public environment.                                                                            | `nsfw`                    | No       |
| Pornography             | Content that discusses any sexual content or material.                                                                                                                                                           | `pornography`             | Yes      |
| Profanity               | Any profanity or cursing.                                                                                                                                                                                        | `profanity`               | Yes      |
| Sensitive Social Issues | This category includes content that may be considered insensitive, irresponsible, or harmful to certain groups based on their beliefs, political affiliation, sexual orientation, or gender identity.            | `sensitive_social_issues` | No       |
| Terrorism               | Includes terrorist acts as well as terrorist groups. Examples include bombings, mass shootings, and ISIS. Note that many texts corresponding to this topic may also be classified into the crime violence topic. | `terrorism`               | Yes      |
| Tobacco                 | Text that discusses tobacco and tobacco usage, including e-cigarettes, nicotine, vaping, and general discussions about smoking.                                                                                  | `tobacco`                 | Yes      |
| Weapons                 | Text that discusses any type of weapon including guns, ammunition, shooting, knives, missiles, torpedoes, etc.                                                                                                   | `weapons`                 | Yes      |

## Frequently asked questions

<Accordion title="Why is the Content Moderation model not detecting sensitive content in my audio file?" theme="dark" iconColor="white" >
  
There could be a few reasons for this. First, make sure that the audio file contains speech, and not just background noise or music. Additionally, the model may not have been trained on the specific type of sensitive content you're looking for. If you believe the model should be able to detect the content but it's not, you can reach out to AssemblyAI's support team for assistance.

  </Accordion>

<Accordion title="Why is the Content Moderation model flagging content that isn't actually sensitive?" theme="dark" iconColor="white" >
  
The model may occasionally flag content as sensitive that isn't actually problematic. This can happen if the model isn't trained on the specific context or nuances of the language being used. In these cases, you can manually review the flagged content and determine if it's actually sensitive or not. If you believe the model is consistently flagging content incorrectly, you can contact AssemblyAI's support team to report the issue.

  </Accordion>

<Accordion title="How do I know which specific parts of the audio file contain sensitive content?" theme="dark" iconColor="white" >
  
The Content Moderation model provides segment-level results that pinpoint where in the audio the sensitive content was discussed, as well as the degree to which it was discussed. You can access this information in the results key of the API response. Each result in the list contains a text key that shows the sensitive content, and a labels key that shows the detected sensitive topics along with their confidence and severity scores.

  </Accordion>

<Accordion title="Can the Content Moderation model be used in real-time applications?" theme="dark" iconColor="white" >
  
The model is designed to process batches of segments in significantly less than 1 second, making it suitable for real-time applications. However, keep in mind that the actual processing time depends on the length of the audio file and the number of segments it's divided into. Additionally, the model may occasionally require additional time to process particularly complex or long segments.

  </Accordion>

<Accordion title="Why am I receiving an error message when using the Content Moderation model?" theme="dark" iconColor="white" >
  
If you receive an error message, it may be due to an issue with your request format or parameters. Double-check that your request includes the correct `audio_url` parameter. If you continue to experience issues, you can reach out to AssemblyAI's support team for assistance.

  </Accordion>


---
title: Sentiment Analysis
description: Detect the sentiment of speech in your audio
---

import { LanguageTable } from "../../assets/components/LanguagesTable";

<Accordion title="Supported languages">
  <LanguageTable
    languages={[
      { name: "Global English", code: "en" },
      { name: "Australian English", code: "en_au" },
      { name: "British English", code: "en_uk" },
      { name: "US English", code: "en_us" },
    ]}
    columns={2}
  />
  <br />
</Accordion>

The Sentiment Analysis model detects the sentiment of each spoken sentence in the transcript text. Use Sentiment Analysis to get a detailed analysis of the positive, negative, or neutral sentiment conveyed in the audio, along with a confidence score for each result.

## Quickstart

<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>
  
  Enable Sentiment Analysis by setting `sentiment_analysis` to `True` in the transcription config.

```python {8}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(sentiment_analysis=True)

transcript = aai.Transcriber().transcribe(audio_file, config)
print(f"Transcript ID:", transcript.id)

for sentiment_result in transcript.sentiment_analysis:
    print(sentiment_result.text)
    print(sentiment_result.sentiment)  # POSITIVE, NEUTRAL, or NEGATIVE
    print(sentiment_result.confidence)
    print(f"Timestamp: {sentiment_result.start} - {sentiment_result.end}")
```

  </Tab>
  <Tab language="python" title="Python" default>
  
  Enable Sentiment Analysis by setting `sentiment_analysis` to `True` in the JSON payload.

```python {19}
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./local_file.mp3", "rb") as f:
    response = requests.post(base_url + "/v2/upload",
                            headers=headers,
                            data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "sentiment_analysis": True
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

print(f"Transcript ID:", transcript_id)

while True:
    transcription_result = requests.get(polling_endpoint, headers=headers).json()

    if transcription_result['status'] == 'completed':
      for sentiment_result in transcription_result['sentiment_analysis_results']:
        print(sentiment_result['text'])
        print(sentiment_result['sentiment'])  # POSITIVE, NEUTRAL, or NEGATIVE
        print(sentiment_result['confidence'])
        print(f"Timestamp: {sentiment_result['start']} - {sentiment_result['end']}")
      break
    elif transcription_result['status'] == 'error':
        raise RuntimeError(f"Transcription failed: {transcription_result['error']}")
    else:
        time.sleep(3)
```

  </Tab>
  <Tab language="javascript-sdk" title="JavaScript SDK">
  
  Enable Sentiment Analysis by setting `sentiment_analysis` to `true` in the transcription config.

```javascript {12}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  sentiment_analysis: true,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);
  console.log("Transcript ID: ", transcript.id);

  for (const result of transcript.sentiment_analysis_results) {
    console.log(result.text);
    console.log(result.sentiment); // POSITIVE, NEUTRAL, or NEGATIVE
    console.log(result.confidence);
    console.log(`Timestamp: ${result.start} - ${result.end}`);
  }
};
run();
```

  </Tab>
  <Tab language="javascript" title="JavaScript">
  
  Enable Sentiment Analysis by setting `sentiment_analysis` to `true` in the JSON payload.

```javascript {19}
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  sentiment_analysis: true,
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
console.log("Transcript ID: ", transcriptId);

const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    for (const sentimentResult of transcriptionResult.sentiment_analysis_results) {
      console.log(sentimentResult.text);
      console.log(sentimentResult.sentiment); // POSITIVE, NEUTRAL, or NEGATIVE
      console.log(sentimentResult.confidence);
      console.log(
        `Timestamp: ${sentimentResult.start} - ${sentimentResult.end}`
      );
    }
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  Enable Sentiment Analysis by setting `sentiment_analysis` to `true` in the JSON payload.

<Info>
  Most of these libraries are included by default, but on .NET Framework and
  Mono you need to reference the System.Net.Http library and install the
  [System.Net.Http.Json NuGet
  package](https://www.nuget.org/packages/System.Net.Http.Json).
</Info>

```csharp {53}
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Threading.Tasks;
using System.Text.Json.Serialization;
using System.Collections.Generic;

class Program
{
    static async Task Main(string[] args)
    {
        string baseUrl = "https://api.assemblyai.com";

        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Authorization =
                new AuthenticationHeaderValue("<YOUR_API_KEY>");

            string uploadUrl = await UploadFileAsync("./local_file.mp3", httpClient, baseUrl);

            var transcript = await CreateTranscriptWithSentimentAnalysisAsync(uploadUrl, httpClient, baseUrl);

            Console.WriteLine($"Transcript ID: {transcript.Id}");
            transcript = await WaitForTranscriptToProcessAndAnalyzeSentiment(transcript, httpClient, baseUrl);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient, string baseUrl)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var fileContent = new StreamContent(fileStream))
        {
            fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            using (var response = await httpClient.PostAsync($"{baseUrl}/v2/upload", fileContent))
            {
                response.EnsureSuccessStatusCode();
                var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
                return jsonDoc.RootElement.GetProperty("upload_url").GetString();
            }
        }
    }

    static async Task<Transcript> CreateTranscriptWithSentimentAnalysisAsync(string audioUrl, HttpClient httpClient, string baseUrl)
    {
        var data = new
        {
            audio_url = audioUrl,
            sentiment_analysis = true
        };

        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync($"{baseUrl}/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcessAndAnalyzeSentiment(Transcript transcript, HttpClient httpClient, string baseUrl)
    {
        string pollingEndpoint = $"{baseUrl}/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "completed":
                    // Process sentiment analysis results
                    if (transcript.SentimentAnalysisResults != null)
                    {
                        foreach (var result in transcript.SentimentAnalysisResults)
                        {
                            Console.WriteLine(result.Text);
                            Console.WriteLine(result.Sentiment);  // POSITIVE, NEUTRAL, or NEGATIVE
                            Console.WriteLine(result.Confidence);
                            Console.WriteLine($"Timestamp: {result.Start} - {result.End}");
                        }
                    }

                    return transcript;

                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");

                default:
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
            }
        }
    }

    public class Transcript
    {
        [JsonPropertyName("id")]
        public string Id { get; set; }

        [JsonPropertyName("status")]
        public string Status { get; set; }

        [JsonPropertyName("text")]
        public string Text { get; set; }

        [JsonPropertyName("sentiment_analysis_results")]
        public List<SentimentAnalysisResult> SentimentAnalysisResults { get; set; }

        [JsonPropertyName("error")]
        public string Error { get; set; }
    }

    public class SentimentAnalysisResult
    {
        [JsonPropertyName("text")]
        public string Text { get; set; }

        [JsonPropertyName("sentiment")]
        public string Sentiment { get; set; }

        [JsonPropertyName("confidence")]
        public double Confidence { get; set; }

        [JsonPropertyName("start")]
        public int Start { get; set; }

        [JsonPropertyName("end")]
        public int End { get; set; }
    }
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  Enable Sentiment Analysis by setting `sentiment_analysis` to `true` in the JSON payload.

```ruby {22}
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI.parse("#{base_url}/v2/upload")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = File.read(path)
response = http.request(request)
upload_url = JSON.parse(response.body)["upload_url"]

data = {
  "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
  "sentiment_analysis" => true,
}
uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'

    transcription_result['sentiment_analysis_results'].each do |sentiment_result|
      puts sentiment_result['text']
      puts sentiment_result['sentiment'] # POSITIVE, NEUTRAL, or NEGATIVE
      puts sentiment_result['confidence']
      puts "Timestamp: #{sentiment_result['start']} - #{sentiment_result['end']}"
    end
  break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    sleep(3)
  end
end
```

  </Tab>
  <Tab language="php" title="PHP">
  
  Enable Sentiment Analysis by setting `sentiment_analysis` to `true` in the JSON payload.

```php {30}
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "sentiment_analysis" => true,
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {

        foreach ($transcription_result['sentiment_analysis_results'] as $sentiment_result) {
            echo $sentiment_result['text'] . "\n";
            echo $sentiment_result['sentiment'] . "\n"; # POSITIVE, NEUTRAL, or NEGATIVE
            echo $sentiment_result['confidence'] . "\n";
            echo "Timestamp: {$sentiment_result['start']} - {$sentiment_result['end']}" . "\n";
        }
        break;
    }  else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}

```

  </Tab>
</Tabs>

### Example output

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US.
NEGATIVE
0.8181032538414001
Timestamp: 250 - 6350
...
```

<Tip title="Sentiment Analysis Using LeMUR">
  Check out this cookbook [LeMUR for Customer Call Sentiment
  Analysis](/docs/guides/call-sentiment-analysis)
  for an example of how to leverage LeMUR's QA feature for sentiment analysis.
</Tip>

## Add speaker labels to sentiments

<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speaker_labels` in the transcription config.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```python {3}
config = aai.TranscriptionConfig(
  sentiment_analysis=True,
  speaker_labels=True
)
# ...
for sentiment_result in transcript.sentiment_analysis:
  print(sentiment_result.speaker)
```

  </Tab>
    <Tab language="python" title="Python" default>
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speaker_labels` in the JSON payload.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```python {4}
data = {
    "audio_url": upload_url,
    "sentiment_analysis": True,
    "speaker_labels": True
}
# ...
      for sentiment_result in transcription_result['sentiment_analysis_results']:
        print(sentiment_result['speaker'])
      break
```

  </Tab>
  <Tab language="javascript-sdk" title="JavaScript SDK">
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speaker_labels` in the transcription config.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```javascript {4}
const params = {
  audio: audioUrl,
  sentiment_analysis: true,
  speaker_labels: true,
};
// ...
for (const result of transcript.sentiment_analysis_results) {
  console.log(result.speaker);
}
```

  </Tab>
    <Tab language="javascript" title="JavaScript" default>
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speaker_labels` in the JSON payload.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```javascript {4}
const data = {
  audio_url: uploadUrl,
  sentiment_analysis: true,
  speaker_labels: true
}
// ...
    for sentimentResult of transcriptionResult['sentiment_analysis_results']:
      console.log(sentimentResult['speaker'])
    break
```

  </Tab>
    <Tab language="csharp" title="C#" default>
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speaker_labels` in the JSON payload.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```csharp {5,12,33-34}
  var data = new
  {
      audio_url = audioUrl,
      sentiment_analysis = true,
      speaker_labels = true
  };

// ...

  foreach (var result in transcript.SentimentAnalysisResults)
  {
      Console.WriteLine($"Speaker: {result.Speaker}");
  }

// ...
    public class SentimentAnalysisResult
    {
        [JsonPropertyName("text")]
        public string Text { get; set; }

        [JsonPropertyName("sentiment")]
        public string Sentiment { get; set; }

        [JsonPropertyName("confidence")]
        public double Confidence { get; set; }

        [JsonPropertyName("start")]
        public int Start { get; set; }

        [JsonPropertyName("end")]
        public int End { get; set; }

        [JsonPropertyName("speaker")]
        public string Speaker { get; set; }
    }
```

  </Tab>
  <Tab language="ruby" title="Ruby" default>
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speaker_labels` in the JSON payload.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```ruby {4}
data = {
  "audio_url" => upload_url,
  "sentiment_analysis" => true,
  "speaker_labels" => true
}
# ...
    transcription_result['sentiment_analysis_results'].each do |sentiment_result|
      puts sentiment_result['speaker']
    end
```

  </Tab>
    <Tab language="php" title="PHP" default>
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speaker_labels` in the JSON payload.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```php {4}
$data = array(
    "audio_url" => $upload_url,
    "sentiment_analysis" => true,
    "speaker_labels" => true
);
# ...
    foreach ($transcription_result['sentiment_analysis_results'] as $sentiment_result) {
        echo $sentiment_result['speaker'] . "\n";
    }
```

  </Tab>
</Tabs>

## API reference

### Request

```bash {6}
curl https://api.assemblyai.com/v2/transcript \
--header "Authorization: <YOUR_API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "audio_url": "YOUR_AUDIO_URL",
  "sentiment_analysis": true
}'
```

| Key                  | Type    | Description                |
| -------------------- | ------- | -------------------------- |
| `sentiment_analysis` | boolean | Enable Sentiment Analysis. |

### Response

<Json
  json={{
    sentiment_analysis_results: [
      {
        text: "Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US.",
        start: 250,
        end: 6382,
        sentiment: "NEGATIVE",
        confidence: 0.8181034922599792,
        speaker: null,
      },
      {
        text: "Skylines from Maine to Maryland to Minnesota are gray and smoggy.",
        start: 6516,
        end: 11050,
        sentiment: "NEGATIVE",
        confidence: 0.5900681018829346,
        speaker: null,
      },
      {
        text: "And in some places, the air quality warnings include the warning to stay inside.",
        start: 11130,
        end: 15646,
        sentiment: "NEUTRAL",
        confidence: 0.5371454358100891,
        speaker: null,
      },
      {
        text: "We wanted to better understand what's happening here and why, so we called Peter de Carlo, an associate professor in the Department of Environmental Health and Engineering at Johns Hopkins University Varsity.",
        start: 15828,
        end: 25490,
        sentiment: "NEUTRAL",
        confidence: 0.7929256558418274,
        speaker: null,
      },
      {
        text: "Good morning, professor.",
        start: 25570,
        end: 26950,
        sentiment: "POSITIVE",
        confidence: 0.8253473043441772,
        speaker: null,
      },
      {
        text: "Good morning.",
        start: 27850,
        end: 28840,
        sentiment: "POSITIVE",
        confidence: 0.7193593382835388,
        speaker: null,
      },
      {
        text: "What is it about the conditions right now that have caused this round of wildfires to affect so many people so far away?",
        start: 29610,
        end: 37400,
        sentiment: "NEGATIVE",
        confidence: 0.7902835607528687,
        speaker: null,
      },
      {
        text: "Well, there's a couple of things.",
        start: 38970,
        end: 40694,
        sentiment: "NEUTRAL",
        confidence: 0.7348891496658325,
        speaker: null,
      },
      {
        text: "The season has been pretty dry already.",
        start: 40892,
        end: 42922,
        sentiment: "NEGATIVE",
        confidence: 0.9268015623092651,
        speaker: null,
      },
      {
        text: "And then the fact that we're getting hit in the US.",
        start: 43056,
        end: 45898,
        sentiment: "NEGATIVE",
        confidence: 0.7737327814102173,
        speaker: null,
      },
      {
        text: "Is because there's a couple of weather systems that are essentially channeling the smoke from those Canadian wildfires through Pennsylvania into the Mid Atlantic and the Northeast and kind of just dropping the smoke there.",
        start: 46064,
        end: 56142,
        sentiment: "NEUTRAL",
        confidence: 0.6657092571258545,
        speaker: null,
      },
      {
        text: "So what is it in this haze that makes it harmful?",
        start: 56276,
        end: 59274,
        sentiment: "NEGATIVE",
        confidence: 0.8751137852668762,
        speaker: null,
      },
      {
        text: "And I'm assuming it is harmful.",
        start: 59322,
        end: 61070,
        sentiment: "NEGATIVE",
        confidence: 0.8000661730766296,
        speaker: null,
      },
      {
        text: "It is.",
        start: 62290,
        end: 63150,
        sentiment: "NEUTRAL",
        confidence: 0.6054744720458984,
        speaker: null,
      },
      {
        text: "The levels outside right now in Baltimore are considered unhealthy.",
        start: 63300,
        end: 67010,
        sentiment: "NEGATIVE",
        confidence: 0.9444372653961182,
        speaker: null,
      },
      {
        text: "And most of that is due to what's called particulate matter, which are tiny particles, microscopic smaller than the width of your hair that can get into your lungs and impact your respiratory system, your cardiovascular system, and even your neurological your brain.",
        start: 67750,
        end: 82950,
        sentiment: "NEGATIVE",
        confidence: 0.7631250619888306,
        speaker: null,
      },
      {
        text: "What makes this particularly harmful?",
        start: 83450,
        end: 85554,
        sentiment: "NEGATIVE",
        confidence: 0.932969331741333,
        speaker: null,
      },
      {
        text: "Is it the volume of particulant?",
        start: 85602,
        end: 88034,
        sentiment: "NEUTRAL",
        confidence: 0.8304142951965332,
        speaker: null,
      },
      {
        text: "Is it something in particular?",
        start: 88082,
        end: 89302,
        sentiment: "NEUTRAL",
        confidence: 0.8175228238105774,
        speaker: null,
      },
      {
        text: "What is it exactly?",
        start: 89436,
        end: 90278,
        sentiment: "NEUTRAL",
        confidence: 0.815334141254425,
        speaker: null,
      },
      {
        text: "Can you just drill down on that a little bit more?",
        start: 90364,
        end: 92540,
        sentiment: "NEUTRAL",
        confidence: 0.8712159991264343,
        speaker: null,
      },
      {
        text: "Yeah.",
        start: 93390,
        end: 93802,
        sentiment: "NEUTRAL",
        confidence: 0.5491448640823364,
        speaker: null,
      },
      {
        text: "So the concentration of particulate matter I was looking at some of the monitors that we have was reaching levels of what are, in science, big 150 micrograms per meter cubed, which is more than ten times what the annual average should be and about four times higher than what you're supposed to have on a 24 hours average.",
        start: 93856,
        end: 113258,
        sentiment: "NEUTRAL",
        confidence: 0.6115278601646423,
        speaker: null,
      },
      {
        text: "And so the concentrations of these particles in the air are just much, much higher than we typically see.",
        start: 113354,
        end: 119650,
        sentiment: "NEUTRAL",
        confidence: 0.5178466439247131,
        speaker: null,
      },
      {
        text: "And exposure to those high levels can lead to a host of health problems.",
        start: 119720,
        end: 123314,
        sentiment: "NEGATIVE",
        confidence: 0.939525306224823,
        speaker: null,
      },
      {
        text: "And who is most vulnerable?",
        start: 123432,
        end: 124942,
        sentiment: "NEUTRAL",
        confidence: 0.5373413562774658,
        speaker: null,
      },
      {
        text: "I noticed that in New York City, for example, they're canceling outdoor activities.",
        start: 125006,
        end: 128702,
        sentiment: "NEGATIVE",
        confidence: 0.7793571352958679,
        speaker: null,
      },
      {
        text: "And so here it is in the early days of summer, and they have to keep all the kids inside.",
        start: 128766,
        end: 132870,
        sentiment: "NEUTRAL",
        confidence: 0.4853213131427765,
        speaker: null,
      },
      {
        text: "So who tends to be vulnerable in a situation like this?",
        start: 132940,
        end: 136440,
        sentiment: "NEUTRAL",
        confidence: 0.5655253529548645,
        speaker: null,
      },
      {
        text: "It's the youngest.",
        start: 137370,
        end: 138754,
        sentiment: "NEUTRAL",
        confidence: 0.7843366265296936,
        speaker: null,
      },
      {
        text: "So children, obviously, whose bodies are still developing.",
        start: 138802,
        end: 142514,
        sentiment: "NEUTRAL",
        confidence: 0.765259325504303,
        speaker: null,
      },
      {
        text: "The elderly, who are their bodies are more in decline and they're more susceptible to the health impacts of breathing, the poor air quality.",
        start: 142562,
        end: 149660,
        sentiment: "NEGATIVE",
        confidence: 0.9401319026947021,
        speaker: null,
      },
      {
        text: "And then people who have preexisting health conditions, people with respiratory conditions or heart conditions can be triggered by high levels of air pollution.",
        start: 150530,
        end: 156910,
        sentiment: "NEGATIVE",
        confidence: 0.8857417702674866,
        speaker: null,
      },
      {
        text: "Could this get worse?",
        start: 157410,
        end: 158910,
        sentiment: "NEGATIVE",
        confidence: 0.9435272216796875,
        speaker: null,
      },
      {
        text: "That's a good question.",
        start: 162050,
        end: 163440,
        sentiment: "POSITIVE",
        confidence: 0.8283342719078064,
        speaker: null,
      },
      {
        text: "In some areas, it's much worse than others.",
        start: 165010,
        end: 166942,
        sentiment: "NEGATIVE",
        confidence: 0.896091103553772,
        speaker: null,
      },
      {
        text: "And it just depends on kind of where the smoke is concentrated.",
        start: 166996,
        end: 170370,
        sentiment: "NEUTRAL",
        confidence: 0.7206501960754395,
        speaker: null,
      },
      {
        text: "I think New York has some of the higher concentrations right now, but that's going to change as that air moves away from the New York area.",
        start: 170950,
        end: 176930,
        sentiment: "NEUTRAL",
        confidence: 0.758122980594635,
        speaker: null,
      },
      {
        text: "But over the course of the next few days, we will see different areas being hit at different times with the highest concentrations.",
        start: 177080,
        end: 183666,
        sentiment: "NEUTRAL",
        confidence: 0.7974407076835632,
        speaker: null,
      },
      {
        text: "I was going to ask you about more fires start burning.",
        start: 183778,
        end: 185634,
        sentiment: "NEUTRAL",
        confidence: 0.611624538898468,
        speaker: null,
      },
      {
        text: "I don't expect the concentrations to go up too much higher.",
        start: 185682,
        end: 189030,
        sentiment: "NEUTRAL",
        confidence: 0.5720192790031433,
        speaker: null,
      },
      {
        text: "I was going to ask you how and you started to answer this, but how much longer could this last?",
        start: 189100,
        end: 193242,
        sentiment: "NEUTRAL",
        confidence: 0.546766459941864,
        speaker: null,
      },
      {
        text: "Or forgive me if I'm asking you to speculate, but what do you think?",
        start: 193296,
        end: 196540,
        sentiment: "NEUTRAL",
        confidence: 0.7195860743522644,
        speaker: null,
      },
      {
        text: "Well, I think the fires are going to burn for a little bit longer, but the key for us in the US.",
        start: 198030,
        end: 202202,
        sentiment: "NEUTRAL",
        confidence: 0.5855423808097839,
        speaker: null,
      },
      {
        text: "Is the weather system changing.",
        start: 202256,
        end: 203754,
        sentiment: "NEUTRAL",
        confidence: 0.8448712229728699,
        speaker: null,
      },
      {
        text: "And so right now, it's kind of the weather systems that are pulling that air into our mid Atlantic and Northeast region.",
        start: 203802,
        end: 211082,
        sentiment: "NEUTRAL",
        confidence: 0.8283596038818359,
        speaker: null,
      },
      {
        text: "As those weather systems change and shift, we'll see that smoke going elsewhere and not impact us in this region as much.",
        start: 211146,
        end: 219122,
        sentiment: "NEUTRAL",
        confidence: 0.6655184030532837,
        speaker: null,
      },
      {
        text: "And so I think that's going to be the defining factor.",
        start: 219176,
        end: 221006,
        sentiment: "NEUTRAL",
        confidence: 0.6444751024246216,
        speaker: null,
      },
      {
        text: "And I think the next couple of days we're going to see a shift in that weather pattern and start to push the smoke away from where we are.",
        start: 221038,
        end: 227638,
        sentiment: "NEUTRAL",
        confidence: 0.8290640711784363,
        speaker: null,
      },
      {
        text: "And finally, with the impacts of climate change, we are seeing more wildfires.",
        start: 227724,
        end: 232354,
        sentiment: "NEGATIVE",
        confidence: 0.6964414715766907,
        speaker: null,
      },
      {
        text: "Will we be seeing more of these kinds of wide ranging air quality consequences or circumstances?",
        start: 232482,
        end: 240330,
        sentiment: "NEUTRAL",
        confidence: 0.5142849087715149,
        speaker: null,
      },
      {
        text: "I mean, that is one of the predictions for climate change.",
        start: 241310,
        end: 245162,
        sentiment: "NEUTRAL",
        confidence: 0.6701292395591736,
        speaker: null,
      },
      {
        text: "Looking into the future, the fire season is starting earlier and lasting longer, and we're seeing more frequent fires.",
        start: 245216,
        end: 251286,
        sentiment: "NEUTRAL",
        confidence: 0.475503146648407,
        speaker: null,
      },
      {
        text: "So, yeah, this is probably something that we'll be seeing more frequently.",
        start: 251318,
        end: 255578,
        sentiment: "NEUTRAL",
        confidence: 0.6273220181465149,
        speaker: null,
      },
      {
        text: "This tends to be much more of an issue in the Western US.",
        start: 255674,
        end: 258046,
        sentiment: "NEUTRAL",
        confidence: 0.5828160643577576,
        speaker: null,
      },
      {
        text: "So the eastern US.",
        start: 258148,
        end: 259054,
        sentiment: "NEUTRAL",
        confidence: 0.7606309056282043,
        speaker: null,
      },
      {
        text: "Getting hit right now is a little bit new.",
        start: 259092,
        end: 261760,
        sentiment: "NEUTRAL",
        confidence: 0.5369289517402649,
        speaker: null,
      },
      {
        text: "But yeah, I think with climate change moving forward, this is something that is going to happen more frequently.",
        start: 262130,
        end: 267770,
        sentiment: "NEUTRAL",
        confidence: 0.6625354886054993,
        speaker: null,
      },
      {
        text: "That's Peter De Carlo, associate professor in the Department of Environmental Health and Engineering at Johns Hopkins University.",
        start: 267930,
        end: 274850,
        sentiment: "NEUTRAL",
        confidence: 0.90962153673172,
        speaker: null,
      },
      {
        text: "Sergeant Carlo, thanks so much for joining us and sharing this expertise with us.",
        start: 274970,
        end: 278600,
        sentiment: "POSITIVE",
        confidence: 0.9753028154373169,
        speaker: null,
      },
      {
        text: "Thank you for having me.",
        start: 279370,
        end: 280340,
        sentiment: "POSITIVE",
        confidence: 0.9709058403968811,
        speaker: null,
      },
    ],
  }}
/>


| Key                                        | Type           | Description                                                                                                                |
| ------------------------------------------ | -------------- | -------------------------------------------------------------------------------------------------------------------------- |
| `sentiment_analysis_results`               | array          | A temporal sequence of Sentiment Analysis results for the audio file, one element for each sentence in the file.           |
| `sentiment_analysis_results[i].text`       | string         | The transcript of the i-th sentence.                                                                                       |
| `sentiment_analysis_results[i].start`      | number         | The starting time, in milliseconds, of the i-th sentence.                                                                  |
| `sentiment_analysis_results[i].end`        | number         | The ending time, in milliseconds, of the i-th sentence.                                                                    |
| `sentiment_analysis_results[i].sentiment`  | string         | The detected sentiment for the i-th sentence, one of `POSITIVE`, `NEUTRAL`, `NEGATIVE`.                                    |
| `sentiment_analysis_results[i].confidence` | number         | The confidence score for the detected sentiment of the i-th sentence, from 0 to 1.                                         |
| `sentiment_analysis_results[i].speaker`    | string or null | The speaker of the i-th sentence if [Speaker Diarization](/docs/speech-to-text/speaker-diarization) is enabled, else null. |

## Frequently asked questions

<Accordion title="What if the model predicts the wrong sentiment label for a sentence?" theme="dark" iconColor="white" >
  
The Sentiment Analysis model is based on the interpretation of the transcript and may not always accurately capture the intended sentiment of the speaker. It's recommended to take into account the context of the transcript and to validate the sentiment analysis results with human judgment when possible.

  </Accordion>

<Accordion title="What if the transcript contains sensitive or offensive content?" theme="dark" iconColor="white" >
  
The [Content Moderation model](/docs/audio-intelligence/content-moderation) can be used to identify and filter out sensitive or offensive content from the transcript.

  </Accordion>

<Accordion title="What if the sentiment analysis results aren't consistent with my expectations?" theme="dark" iconColor="white" >
  
It's important to ensure that the audio being analyzed is relevant to your use case. Additionally, it's recommended to take into account the context of the transcript and to evaluate the confidence score for each sentiment label.

  </Accordion>

<Accordion title="What if the sentiment analysis is taking too long to process?" theme="dark" iconColor="white" >
  
The Sentiment Analysis model is designed to be fast and efficient, but processing times may vary depending on the size of the audio file and the complexity of the language used. If you experience longer processing times than expected, don't hesitate to contact our support team.

  </Accordion>

---
title: Entity Detection
description: Extract named entities from your audio file
---

import { LanguageTable } from "../../assets/components/LanguagesTable";

<Accordion title="Supported languages">
  <LanguageTable
    languages={[
      { name: "Global English", code: "en" },
      { name: "Australian English", code: "en_au" },
      { name: "British English", code: "en_uk" },
      { name: "US English", code: "en_us" },
      { name: "Spanish", code: "es" },
      { name: "French", code: "fr" },
      { name: "German", code: "de" },
      { name: "Italian", code: "it" },
      { name: "Portuguese", code: "pt" },
      { name: "Dutch", code: "nl" },
      { name: "Hindi", code: "hi" },
      { name: "Japanese", code: "ja" },
      { name: "Chinese", code: "zh" },
      { name: "Finnish", code: "fi" },
      { name: "Korean", code: "ko" },
      { name: "Polish", code: "pl" },
      { name: "Russian", code: "ru" },
      { name: "Turkish", code: "tr" },
      { name: "Ukrainian", code: "uk" },
      { name: "Vietnamese", code: "vi" },
      { name: "Afrikaans", code: "af" },
      { name: "Arabic", code: "ar" },
      { name: "Belarusian", code: "be" },
      { name: "Bulgarian", code: "bg" },
      { name: "Burmese", code: "my" },
      { name: "Catalan", code: "ca" },
      { name: "Croatian", code: "hr" },
      { name: "Czech", code: "cs" },
      { name: "Danish", code: "da" },
      { name: "Estonian", code: "et" },
      { name: "Georgian", code: "ka" },
      { name: "Greek", code: "el" },
      { name: "Hebrew", code: "he" },
      { name: "Hungarian", code: "hu" },
      { name: "Icelandic", code: "is" },
      { name: "Indonesian", code: "id" },
      { name: "Khmer", code: "km" },
      { name: "Latvian", code: "lv" },
      { name: "Lithuanian", code: "lt" },
      { name: "Luxembourgish", code: "lb" },
      { name: "Malay", code: "ms" },
      { name: "Norwegian", code: "no" },
      { name: "Persian", code: "fa" },
      { name: "Romanian", code: "ro" },
      { name: "Slovak", code: "sk" },
      { name: "Slovenian", code: "sl" },
      { name: "Swahili", code: "sw" },
      { name: "Swedish", code: "sv" },
      { name: "Tagalog", code: "tl" },
      { name: "Tamil", code: "ta" },
    ]}
    columns={4}
  />
  <br />
</Accordion>

The Entity Detection model lets you automatically identify and categorize key information in transcribed audio content.

Here are a few examples of what you can detect:

- Names of people
- Organizations
- Addresses
- Phone numbers
- Medical data
- Social security numbers

For the full list of entities that you can detect, see [Supported entities](#supported-entities).

<Tip title="Supported Languages">
  Entity Detection is available in multiple languages. See [Supported
  languages](/docs/speech-to-text/pre-recorded-audio/supported-languages).
</Tip>

## Quickstart

<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>
  
  Enable Entity Detection by setting `entity_detection` to `True` in the transcription config.

```python {8}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(entity_detection=True)

transcript = aai.Transcriber().transcribe(audio_file, config)
print(f"Transcript ID:", transcript.id)

for entity in transcript.entities:
    print(entity.text)
    print(entity.entity_type)
    print(f"Timestamp: {entity.start} - {entity.end}\n")
```

  </Tab>
  <Tab language="python" title="Python" default>
  
  Enable Entity Detection by setting `entity_detection` to `True` in the JSON payload.

```python {19}
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./local_file.mp3", "rb") as f:
    response = requests.post(base_url + "/v2/upload",
                            headers=headers,
                            data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "entity_detection": True
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

print(f"Transcript ID:", transcript_id)

while True:
    transcription_result = requests.get(polling_endpoint, headers=headers).json()

    if transcription_result['status'] == 'completed':
      for entity in transcription_result['entities']:
        print(entity['text'])
        print(entity['entity_type'])
        print(f"Timestamp: {entity['start']} - {entity['end']}\n")
      break
    elif transcription_result['status'] == 'error':
        raise RuntimeError(f"Transcription failed: {transcription_result['error']}")
    else:
        time.sleep(3)
```

  </Tab>
  <Tab language="javascript-sdk" title="JavaScript SDK">
  
  Enable Entity Detection by setting `entity_detection` to `true` in the transcription config.

```javascript {12}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  entity_detection: true,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  for (const entity of transcript.entities) {
    console.log(entity.text);
    console.log(entity.entity_type);
    console.log(`Timestamp: ${entity.start} - ${entity.end}\n`);
  }
};

run();
```

  </Tab>
  <Tab language="javascript" title="JavaScript">
  
  Enable Entity Detection by setting `entity_detection` to `true` in the JSON payload.

```javascript {19}
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  entity_detection: true,
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
console.log("Transcript ID: ", transcriptId);

const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    for (const entity of transcriptionResult.entities) {
      console.log(entity.text);
      console.log(entity.entity_type);
      console.log(`Timestamp: ${entity.start} - ${entity.end}\n`);
    }
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  Enable Entity Detection by setting `entity_detection` to `true` in the JSON payload.

<Info>
  Most of these libraries are included by default, but on .NET Framework and
  Mono you need to reference the System.Net.Http library and install the
  [System.Net.Http.Json NuGet
  package](https://www.nuget.org/packages/System.Net.Http.Json).
</Info>

```csharp {53}
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Threading.Tasks;
using System.Text.Json.Serialization;
using System.Collections.Generic;

class Program
{
    static async Task Main(string[] args)
    {
        string baseUrl = "https://api.assemblyai.com";

        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Authorization =
                new AuthenticationHeaderValue("<YOUR_API_KEY>");

            string uploadUrl = await UploadFileAsync("./local_file.mp3", httpClient, baseUrl);

            var transcript = await CreateTranscriptWithEntityDetectionAsync(uploadUrl, httpClient, baseUrl);

            Console.WriteLine($"Transcript ID: {transcript.Id}");
            transcript = await WaitForTranscriptToProcessAndAnalyzeEntities(transcript, httpClient, baseUrl);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient, string baseUrl)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var fileContent = new StreamContent(fileStream))
        {
            fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            using (var response = await httpClient.PostAsync($"{baseUrl}/v2/upload", fileContent))
            {
                response.EnsureSuccessStatusCode();
                var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
                return jsonDoc.RootElement.GetProperty("upload_url").GetString();
            }
        }
    }

    static async Task<Transcript> CreateTranscriptWithEntityDetectionAsync(string audioUrl, HttpClient httpClient, string baseUrl)
    {
        var data = new
        {
            audio_url = audioUrl,
            entity_detection = true
        };

        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync($"{baseUrl}/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcessAndAnalyzeEntities(Transcript transcript, HttpClient httpClient, string baseUrl)
    {
        string pollingEndpoint = $"{baseUrl}/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "completed":
                    // Process entity detection results
                    if (transcript.Entities != null)
                    {
                        foreach (var entity in transcript.Entities)
                        {
                            Console.WriteLine(entity.Text);
                            Console.WriteLine(entity.EntityType);
                            Console.WriteLine($"Timestamp: {entity.Start} - {entity.End}");
                            Console.WriteLine();
                        }
                    }

                    return transcript;

                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");

                default:
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
            }
        }
    }

    public class Transcript
    {
        [JsonPropertyName("id")]
        public string Id { get; set; }

        [JsonPropertyName("status")]
        public string Status { get; set; }

        [JsonPropertyName("text")]
        public string Text { get; set; }

        [JsonPropertyName("entities")]
        public List<Entity> Entities { get; set; }

        [JsonPropertyName("error")]
        public string Error { get; set; }
    }

    public class Entity
    {
        [JsonPropertyName("text")]
        public string Text { get; set; }

        [JsonPropertyName("entity_type")]
        public string EntityType { get; set; }

        [JsonPropertyName("start")]
        public int Start { get; set; }

        [JsonPropertyName("end")]
        public int End { get; set; }
    }
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  Enable Entity Detection by setting `entity_detection` to `true` in the JSON payload.

```ruby {22}
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI.parse("#{base_url}/v2/upload")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = File.read(path)
response = http.request(request)
upload_url = JSON.parse(response.body)["upload_url"]

data = {
  "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
  "entity_detection" => true,
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'

    transcription_result['entities'].each do |entity|
      puts entity['text']
      puts entity['entity_type']
      puts "Timestamp: #{entity['start']} - #{entity['end']}"
    end
  break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    sleep(3)
  end
end
```

  </Tab>
  <Tab language="php" title="PHP">
  
  Enable Entity Detection by setting `entity_detection` to `true` in the JSON payload.

```php {30}
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "entity_detection" => true,
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {

        foreach ($transcription_result['entities'] as $entity) {
            echo $entity['text'] . "\n";
            echo $entity['entity_type'] . "\n";
            echo "Timestamp: {$entity['start']} - {$entity['end']}" . "\n";
        }
        break;
    }  else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}

```

  </Tab>
</Tabs>

### Example output

```plain
Canada
location
Timestamp: 2548 - 3130

the US
location
Timestamp: 5498 - 6350

...
```

## API reference

### Request

```bash {6}
curl https://api.assemblyai.com/v2/transcript \
--header "Authorization: <YOUR_API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "audio_url": "YOUR_AUDIO_URL",
  "entity_detection": true
}'
```

| Key                | Type    | Description              |
| ------------------ | ------- | ------------------------ |
| `entity_detection` | boolean | Enable Entity Detection. |

### Response

<Json
  json={{
    entities: [
      {
        entity_type: "location",
        text: "Canada",
        start: 2548,
        end: 3130,
      },
      {
        entity_type: "location",
        text: "the US",
        start: 5498,
        end: 6382,
      },
      {
        entity_type: "location",
        text: "Maine",
        start: 7492,
        end: 7914,
      },
      {
        entity_type: "location",
        text: "Maryland",
        start: 8212,
        end: 8634,
      },
      {
        entity_type: "location",
        text: "Minnesota",
        start: 8932,
        end: 9578,
      },
      {
        entity_type: "person_name",
        text: "Peter de Carlo",
        start: 18948,
        end: 19930,
      },
      {
        entity_type: "occupation",
        text: "associate professor",
        start: 20292,
        end: 21194,
      },
      {
        entity_type: "organization",
        text: "Department of Environmental Health and Engineering",
        start: 21508,
        end: 23706,
      },
      {
        entity_type: "organization",
        text: "Johns Hopkins University Varsity",
        start: 23972,
        end: 25490,
      },
      {
        entity_type: "occupation",
        text: "professor",
        start: 26076,
        end: 26950,
      },
      {
        entity_type: "location",
        text: "the US",
        start: 45184,
        end: 45898,
      },
      {
        entity_type: "nationality",
        text: "Canadian",
        start: 49728,
        end: 50086,
      },
      {
        entity_type: "location",
        text: "Pennsylvania",
        start: 51680,
        end: 52326,
      },
      {
        entity_type: "location",
        text: "Mid Atlantic",
        start: 52624,
        end: 53178,
      },
      {
        entity_type: "location",
        text: "Northeast",
        start: 53428,
        end: 53866,
      },
      {
        entity_type: "location",
        text: "Baltimore",
        start: 65064,
        end: 65534,
      },
      {
        entity_type: "occupation",
        text: "science",
        start: 101168,
        end: 101446,
      },
      {
        entity_type: "location",
        text: "New York City",
        start: 125768,
        end: 126274,
      },
      {
        entity_type: "medical_condition",
        text: "respiratory conditions",
        start: 152964,
        end: 153786,
      },
      {
        entity_type: "medical_condition",
        text: "heart conditions",
        start: 153988,
        end: 154506,
      },
      {
        entity_type: "location",
        text: "New York",
        start: 171448,
        end: 171938,
      },
      {
        entity_type: "location",
        text: "New York",
        start: 176008,
        end: 176322,
      },
      {
        entity_type: "location",
        text: "the US",
        start: 201824,
        end: 202202,
      },
      {
        entity_type: "location",
        text: "mid Atlantic",
        start: 209010,
        end: 209866,
      },
      {
        entity_type: "location",
        text: "Northeast region",
        start: 210196,
        end: 211082,
      },
      {
        entity_type: "location",
        text: "Western US",
        start: 257364,
        end: 258046,
      },
      {
        entity_type: "location",
        text: "eastern US",
        start: 258484,
        end: 259054,
      },
      {
        entity_type: "person_name",
        text: "Peter De Carlo",
        start: 268298,
        end: 269194,
      },
      {
        entity_type: "occupation",
        text: "associate professor",
        start: 269242,
        end: 270186,
      },
      {
        entity_type: "organization",
        text: "Department of Environmental Health and Engineering",
        start: 270404,
        end: 272762,
      },
      {
        entity_type: "organization",
        text: "Johns Hopkins University",
        start: 273156,
        end: 274850,
      },
      {
        entity_type: "occupation",
        text: "Sergeant",
        start: 274970,
        end: 275298,
      },
      {
        entity_type: "person_name",
        text: "Carlo",
        start: 275314,
        end: 275634,
      },
    ],
  }}
/>


| Key                       | Type   | Description                                                                                      |
| ------------------------- | ------ | ------------------------------------------------------------------------------------------------ |
| `entities`                | array  | An array of detected entities.                                                                   |
| `entities[i].entity_type` | string | The type of entity for the i-th detected entity.                                                 |
| `entities[i].text`        | string | The text for the i-th detected entity.                                                           |
| `entities[i].start`       | number | The starting time, in milliseconds, at which the i-th detected entity appears in the audio file. |
| `entities[i].end`         | number | The ending time, in milliseconds, for the i-th detected entity in the audio file.                |

The response also includes the request parameters used to generate the transcript.

## Supported entities

The model is designed to automatically detect and classify various types of entities within the transcription text. The detected entities and their corresponding types is listed individually in the entities key of the response object, ordered by when they first appear in the transcript.

| Entity name                 | Description                                                                                                    | Example                                                          |
| --------------------------- | -------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- |
| `account_number`            | Customer account or membership identification number                                                           | Policy No. 10042992; Member ID: HZ-5235-001                      |
| `banking_information`       | Banking information, including account and routing numbers                                                     |                                                                  |
| `blood_type`                | Blood type                                                                                                     | O-, AB positive                                                  |
| `credit_card_cvv`           | Credit card verification code                                                                                  | CVV: 080                                                         |
| `credit_card_expiration`    | Expiration date of a credit card                                                                               |                                                                  |
| `credit_card_number`        | Credit card number                                                                                             |                                                                  |
| `date`                      | Specific calendar date                                                                                         | December 18                                                      |
| `date_interval`             | Broader time periods, including date ranges, months, seasons, years, and decades                               | 2020-2021; 5-9 May; January 1984                                 |
| `date_of_birth`             | Date of birth                                                                                                  | Date of Birth: March 7, 1961                                     |
| `drivers_license`           | Driver's license number                                                                                        | DL# 356933-540                                                   |
| `drug`                      | Medications, vitamins, or supplements                                                                          | Advil, Acetaminophen, Panadol                                    |
| `duration`                  | Periods of time, specified as a number and a unit of time                                                      | 8 months; 2 years                                                |
| `email_address`             | Email address                                                                                                  | support@assemblyai.com                                           |
| `event`                     | Name of an event or holiday                                                                                    | Olympics, Yom Kippur                                             |
| `filename`                  | Names of computer files, including the extension or filepath                                                   | Taxes/2012/brad-tax-returns.pdf                                  |
| `gender_sexuality`          | Terms indicating gender identity or sexual orientation, including slang terms                                  | female; bisexual; trans                                          |
| `healthcare_number`         | Healthcare numbers and health plan beneficiary numbers                                                         | Policy No.: 5584-486-674-YM                                      |
| `injury`                    | Injuries or health issues                                                                                      | I broke my arm, I have a sprained wrist                          |
| `ip_address`                | Internet IP address, including IPv4 and IPv6 formats                                                           | 192.168.0.1                                                      |
| `language`                  | Name of a natural language                                                                                     | Spanish, French                                                  |
| `location`                  | Any Location reference including mailing address, postal code, city, state, province, country, or coordinates. | Lake Victoria, 145 Windsor St., 90210                            |
| `marital_status`            | Terms indicating marital status                                                                                | Single; common-law; ex-wife; married                             |
| `medical_condition`         | Name of a medical condition, disease, syndrome, deficit, or disorder                                           | chronic fatigue syndrome, arrhythmia, depression                 |
| `medical_process`           | Medical process, including treatments, procedures, and tests                                                   | heart surgery, CT scan                                           |
| `money_amount`              | Name and/or amount of currency                                                                                 | 15 pesos, $94.50                                                 |
| `nationality`               | Terms indicating nationality, ethnicity, or race                                                               | American, Asian, Caucasian                                       |
| `number_sequence`           | Numerical PII (including alphanumeric strings) that doesn't fall under other categories                        |                                                                  |
| `occupation`                | Job title or profession                                                                                        | professor, actors, engineer, CPA                                 |
| `organization`              | Name of an organization                                                                                        | CNN, McDonalds, University of Alaska, Northwest General Hospital |
| `passport_number`           | Passport numbers, issued by any country                                                                        | PA4568332; NU3C6L86S12                                           |
| `password`                  | Account passwords, PINs, access keys, or verification answers                                                  | 27%alfalfa, temp1234, My mother's maiden name is Smith           |
| `person_age`                | Number associated with an age                                                                                  | 27, 75                                                           |
| `person_name`               | Name of a person                                                                                               | Bob, Doug Jones, Dr. Kay Martinez, MD                            |
| `phone_number`              | Telephone or fax number                                                                                        |                                                                  |
| `physical_attribute`        | Distinctive bodily attributes, including terms indicating race                                                 | I'm 190cm tall; He belongs to the Black students' association    |
| `political_affiliation`     | Terms referring to a political party, movement, or ideology                                                    | Republican, Liberal                                              |
| `religion`                  | Terms indicating religious affiliation                                                                         | Hindu, Catholic                                                  |
| `statistics`                | Medical statistics                                                                                             | 18%, 18 percent                                                  |
| `time`                      | Expressions indicating clock times                                                                             | 19:37:28; 10pm EST                                               |
| `url`                       | Internet addresses                                                                                             | https://www.assemblyai.com/                                      |
| `us_social_security_number` | Social Security Number or equivalent                                                                           |                                                                  |
| `username`                  | Usernames, login names, or handles                                                                             | @AssemblyAI                                                      |
| `vehicle_id`                | Vehicle identification numbers (VINs), vehicle serial numbers, and license plate numbers                       | 5FNRL38918B111818; BIF7547                                       |
| `zodiac_sign`               | Names of Zodiac signs                                                                                          | Aries; Taurus                                                    |

## Frequently asked questions

<Accordion title="How does the Entity Detection model handle misspellings or variations of entities?" theme="dark" iconColor="white" >
  
The model is capable of identifying entities with variations in spelling or formatting. However, the accuracy of the detection may depend on the severity of the variation or misspelling.

  </Accordion>

<Accordion title="Can the Entity Detection model identify custom entity types?" theme="dark" iconColor="white" >
  
No, the Entity Detection model doesn't support the detection of custom entity types. However, the model is capable of detecting a wide range of predefined entity types, including people, organizations, locations, dates, times, addresses, phone numbers, medical data, and banking information, among others.

  </Accordion>

<Accordion title="How can I improve the accuracy of the Entity Detection model?" theme="dark" iconColor="white" >
  
To improve the accuracy of the Entity Detection model, it's recommended to provide high-quality audio files with clear and distinct speech. In addition, it's important to ensure that the audio content is relevant to the use case and that the entities being detected are relevant to the intended analysis. Finally, it may be helpful to review and adjust the model's configuration parameters, such as the confidence threshold for entity detection, to optimize the results.

  </Accordion>
---
title: Topic Detection
description: Label topics that are mentioned in your audio file
---

import { LanguageTable } from "../../assets/components/LanguagesTable";

<Accordion title="Supported languages">
  <LanguageTable
    languages={[
      { name: "Global English", code: "en" },
      { name: "Australian English", code: "en_au" },
      { name: "British English", code: "en_uk" },
      { name: "US English", code: "en_us" },
      { name: "Spanish", code: "es" },
      { name: "French", code: "fr" },
      { name: "German", code: "de" },
      { name: "Italian", code: "it" },
      { name: "Portuguese", code: "pt" },
    ]}
    columns={2}
  />
  <br />
</Accordion>

The Topic Detection model lets you identify different topics in the transcript. The model uses the [IAB Content Taxonomy](https://airtable.com/shr7KNXOtvfhTTS4i/tblqVLDb7YSsCMXo4?backgroundColor=purple&viewControls=on), a standardized language for content description which consists of 698 comprehensive topics.

## Quickstart

<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>
  
  Enable Topic Detection by setting `iab_categories` to `True` in the transcription parameters.

```python {8}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(iab_categories=True)

transcript = aai.Transcriber().transcribe(audio_file, config)
print(f"Transcript ID:", transcript.id)

# Get the parts of the transcript that were tagged with topics
for result in transcript.iab_categories.results:
    print(result.text)
    print(f"Timestamp: {result.timestamp.start} - {result.timestamp.end}")
    for label in result.labels:
        print(f"{label.label} ({label.relevance})")

# Get a summary of all topics in the transcript
for topic, relevance in transcript.iab_categories.summary.items():
    print(f"Audio is {relevance * 100}% relevant to {topic}")
```

  </Tab>
  <Tab language="python" title="Python" default>
  
  Enable Topic Detection by setting `iab_categories` to `true` in the JSON payload.

```python {19}
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./local_file.mp3", "rb") as f:
    response = requests.post(base_url + "/v2/upload",
                            headers=headers,
                            data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "iab_categories": True
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

print(f"Transcript ID:", transcript_id)

while True:
    transcription_result = requests.get(polling_endpoint, headers=headers).json()

    if transcription_result['status'] == 'completed':
      # Get the parts of the transcript that were tagged with topics
      for result in transcription_result['iab_categories_result']['results']:
        print(result['text'])
        print(f"Timestamp: {result['timestamp']['start']} - {result['timestamp']['end']}")

        for label in result['labels']:
          print(f"{label['label']} ({label['relevance']})")

      # Get a summary of all topics in the transcript
      for topic, relevance in transcription_result['iab_categories_result']['summary'].items():
        print(f"Audio is {relevance * 100}% relevant to {topic}")
      break
    elif transcription_result['status'] == 'error':
        raise RuntimeError(f"Transcription failed: {transcription_result['error']}")
    else:
        time.sleep(3)
```

  </Tab>
  <Tab language="javascript-sdk" title="JavaScript SDK">
  
  Enable Topic Detection by setting `iab_categories` to `true` in the transcription config.

```javascript {12}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  iab_categories: true,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);
  console.log("Transcript ID: ", transcript.id);

  // Get the parts of the transcript that were tagged with topics
  for (const result of transcript.iab_categories_result.results) {
    console.log(result.text);
    console.log(
      `Timestamp: ${result.timestamp?.start} - ${result.timestamp?.end}`
    );
    for (const label of result.labels) {
      console.log(`${label.label} (${label.relevance})`);
    }
  }

  // Get a summary of all topics in the transcript
  for (const [topic, relevance] of Object.entries(
    transcript.iab_categories_result.summary
  )) {
    console.log(`Audio is ${relevance * 100} relevant to ${topic}`);
  }
};

run();
```

  </Tab>
  <Tab language="javascript" title="JavaScript">
  
  Enable Topic Detection by setting `iab_categories` to `true` in the JSON payload.

```javascript {19}
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  iab_categories: true,
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
console.log("Transcript ID: ", transcriptId);

const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    // Get the parts of the transcript that were tagged with topics
    for (const result of transcriptionResult.iab_categories_result.results) {
      console.log(result.text);
      console.log(
        `Timestamp: ${result.timestamp.start} - ${result.timestamp.end}`
      );

      for (const label of result.labels) {
        console.log(`${label.label} (${label.relevance})`);
      }
    }
    // Get a summary of all topics in the transcript
    for (const [topic, relevance] of Object.entries(
      transcriptionResult.iab_categories_result.summary
    )) {
      console.log(`Audio is ${relevance * 100} relevant to ${topic}`);
    }
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  Enable Topic Detection by setting `iab_categories` to `true` in the JSON payload.

<Info>
  Most of these libraries are included by default, but on .NET Framework and
  Mono you need to reference the System.Net.Http library and install the
  [System.Net.Http.Json NuGet
  package](https://www.nuget.org/packages/System.Net.Http.Json).
</Info>

```csharp {53}
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Threading.Tasks;
using System.Text.Json.Serialization;
using System.Collections.Generic;

class Program
{
    static async Task Main(string[] args)
    {
        string baseUrl = "https://api.assemblyai.com";

        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Authorization =
                new AuthenticationHeaderValue("<YOUR_API_KEY>");

            string uploadUrl = await UploadFileAsync("./local_file.mp3", httpClient, baseUrl);

            var transcript = await CreateTranscriptWithIabCategoriesAsync(uploadUrl, httpClient, baseUrl);

            Console.WriteLine($"Transcript ID: {transcript.Id}");
            transcript = await WaitForTranscriptToProcessAndAnalyzeCategories(transcript, httpClient, baseUrl);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient, string baseUrl)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var fileContent = new StreamContent(fileStream))
        {
            fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            using (var response = await httpClient.PostAsync($"{baseUrl}/v2/upload", fileContent))
            {
                response.EnsureSuccessStatusCode();
                var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
                return jsonDoc.RootElement.GetProperty("upload_url").GetString();
            }
        }
    }

    static async Task<Transcript> CreateTranscriptWithIabCategoriesAsync(string audioUrl, HttpClient httpClient, string baseUrl)
    {
        var data = new
        {
            audio_url = audioUrl,
            iab_categories = true
        };

        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync($"{baseUrl}/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcessAndAnalyzeCategories(Transcript transcript, HttpClient httpClient, string baseUrl)
    {
        string pollingEndpoint = $"{baseUrl}/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "completed":
                    // Process IAB categories results
                    if (transcript.IabCategoriesResult != null)
                    {
                        // Get the parts of the transcript that were tagged with topics
                        if (transcript.IabCategoriesResult.Results != null)
                        {
                            foreach (var result in transcript.IabCategoriesResult.Results)
                            {
                                Console.WriteLine(result.Text);
                                Console.WriteLine($"Timestamp: {result.Timestamp.Start} - {result.Timestamp.End}");

                                foreach (var label in result.Labels)
                                {
                                    Console.WriteLine($"{label.Label} ({label.Relevance})");
                                }
                                Console.WriteLine();
                            }
                        }

                        // Get a summary of all topics in the transcript
                        if (transcript.IabCategoriesResult.Summary != null)
                        {
                            foreach (var topicEntry in transcript.IabCategoriesResult.Summary)
                            {
                                Console.WriteLine($"Audio is {topicEntry.Value * 100}% relevant to {topicEntry.Key}");
                            }
                        }
                    }

                    return transcript;

                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");

                default:
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
            }
        }
    }

    public class Transcript
    {
        [JsonPropertyName("id")]
        public string Id { get; set; }

        [JsonPropertyName("status")]
        public string Status { get; set; }

        [JsonPropertyName("text")]
        public string Text { get; set; }

        [JsonPropertyName("iab_categories_result")]
        public IabCategoriesResult IabCategoriesResult { get; set; }

        [JsonPropertyName("error")]
        public string Error { get; set; }
    }

    public class IabCategoriesResult
    {
        [JsonPropertyName("results")]
        public List<CategoryResult> Results { get; set; }

        [JsonPropertyName("summary")]
        public Dictionary<string, double> Summary { get; set; }
    }

    public class CategoryResult
    {
        [JsonPropertyName("text")]
        public string Text { get; set; }

        [JsonPropertyName("timestamp")]
        public TimeRange Timestamp { get; set; }

        [JsonPropertyName("labels")]
        public List<CategoryLabel> Labels { get; set; }
    }

    public class TimeRange
    {
        [JsonPropertyName("start")]
        public int Start { get; set; }

        [JsonPropertyName("end")]
        public int End { get; set; }
    }

    public class CategoryLabel
    {
        [JsonPropertyName("label")]
        public string Label { get; set; }

        [JsonPropertyName("relevance")]
        public double Relevance { get; set; }
    }
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  Enable Topic Detection by setting `iab_categories` to `true` in the JSON payload.

```ruby {23}
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "/my_audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
    "iab_categories" => true
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    # Get the parts of the transcript that were tagged with topics
    transcription_result['iab_categories_result']['results'].each do |result|
      puts result['text']
      puts "Timestamp: #{result['timestamp']['start']} - #{result['timestamp']['end']}"

      result['labels'].each do |label|
        puts "#{label['label']} - (#{label['relevance']})"
      end
    end
    # Get a summary of all topics in the transcript
    transcription_result['iab_categories_result']['summary'].each do |topic, relevance|
      puts "Audio is #{relevance * 100}% relevant to #{topic}"
    end
  break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    sleep(3)
  end
end
```

  </Tab>
  <Tab language="php" title="PHP">
  
  Enable Topic Detection by setting `iab_categories` to `true` in the JSON payload.

```php {30}
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "iab_categories" => true,
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        $content_safety_labels = $transcription_result['iab_categories_result'];

        // Get the parts of the transcript that were tagged with topics
        foreach ($content_safety_labels['results'] as $result) {
            echo $result['text'] . "\n";
            echo "Timestamp: {$result['timestamp']['start']} - {$result['timestamp']['end']}\n";

            foreach ($result['labels'] as $label) {
                echo "{$label['label']} - {$label['relevance']}\n";
            }
        }
        // Get a summary of all topics in the transcript
        foreach ($content_safety_labels['summary'] as $topic => $relevance) {
          echo "Audio is " . ($relevance * 100) . "% relevant to $topic\n";
        }
        break;
    }  else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

  </Tab>
</Tabs>

### Example output

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines...
Timestamp: 250 - 28920
Home&Garden>IndoorEnvironmentalQuality (0.9881)
NewsAndPolitics>Weather (0.5561)
MedicalHealth>DiseasesAndConditions>LungAndRespiratoryHealth (0.0042)
...
Audio is 100.0% relevant to NewsAndPolitics>Weather
Audio is 93.78% relevant to Home&Garden>IndoorEnvironmentalQuality
...
```

<Tip title="Topic Detection Using LeMUR">
  Check out this cookbook [Custom Topic
  Tags](/docs/guides/custom-topic-tags)
  for an example of how to leverage LeMUR for custom topic detection.
</Tip>

## API reference

### Request

```bash {6}
curl https://api.assemblyai.com/v2/transcript \
--header "Authorization: <YOUR_API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "audio_url": "YOUR_AUDIO_URL",
  "iab_categories": true
}'
```

| Key              | Type    | Description             |
| ---------------- | ------- | ----------------------- |
| `iab_categories` | boolean | Enable Topic Detection. |

### Response

<Json
  json={{
    iab_categories_result: {
      status: "success",
      results: [
        {
          text: "Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines from Maine to Maryland to Minnesota are gray and smoggy. And in some places, the air quality warnings include the warning to stay inside. We wanted to better understand what's happening here and why, so we called Peter de Carlo, an associate professor in the Department of Environmental Health and Engineering at Johns Hopkins University Varsity. Good morning, professor. Good morning.",
          labels: [
            {
              relevance: 0.988274097442627,
              label: "Home&Garden>IndoorEnvironmentalQuality",
            },
            {
              relevance: 0.5821335911750793,
              label: "NewsAndPolitics>Weather",
            },
            {
              relevance: 0.0042327106930315495,
              label:
                "MedicalHealth>DiseasesAndConditions>LungAndRespiratoryHealth",
            },
            {
              relevance: 0.0033971222583204508,
              label: "NewsAndPolitics>Disasters",
            },
            {
              relevance: 0.002469958271831274,
              label: "BusinessAndFinance>Business>GreenSolutions",
            },
            {
              relevance: 0.0014376690378412604,
              label: "MedicalHealth>DiseasesAndConditions>Cancer",
            },
            {
              relevance: 0.0014294233405962586,
              label: "Science>Environment",
            },
            {
              relevance: 0.001234519761055708,
              label: "Travel>TravelLocations>PolarTravel",
            },
            {
              relevance: 0.0010231725173071027,
              label: "MedicalHealth>DiseasesAndConditions>ColdAndFlu",
            },
            {
              relevance: 0.0007445293595083058,
              label: "BusinessAndFinance>Industries>PowerAndEnergyIndustry",
            },
          ],
          timestamp: {
            start: 250,
            end: 28840,
          },
        },
        {
          text: "What is it about the conditions right now that have caused this round of wildfires to affect so many people so far away? Well, there's a couple of things. The season has been pretty dry already. And then the fact that we're getting hit in the US. Is because there's a couple of weather systems that are essentially channeling the smoke from those Canadian wildfires through Pennsylvania into the Mid Atlantic and the Northeast and kind of just dropping the smoke there.",
          labels: [
            {
              relevance: 0.9667055010795593,
              label: "NewsAndPolitics>Weather",
            },
            {
              relevance: 0.19349651038646698,
              label: "NewsAndPolitics>Disasters",
            },
            {
              relevance: 0.007998290471732616,
              label: "Travel>TravelLocations>PolarTravel",
            },
            {
              relevance: 0.0007007605163380504,
              label: "Home&Garden>IndoorEnvironmentalQuality",
            },
            {
              relevance: 0.0006503399927169085,
              label: "MedicalHealth>DiseasesAndConditions>ColdAndFlu",
            },
            {
              relevance: 0.0005085167940706015,
              label: "Science>Environment",
            },
            {
              relevance: 0.0003594171430449933,
              label: "Sports>OlympicSports>WinterOlympicSports",
            },
            {
              relevance: 0.00031780285644344985,
              label: "PopCulture>CelebrityScandal",
            },
            {
              relevance: 0.00028784608002752066,
              label: "Sports>CollegeSports>CollegeBaseball",
            },
            {
              relevance: 0.0002203711192123592,
              label: "BusinessAndFinance>Industries>PowerAndEnergyIndustry",
            },
          ],
          timestamp: {
            start: 29610,
            end: 56142,
          },
        },
        {
          text: "So what is it in this haze that makes it harmful? And I'm assuming it is harmful. It is. The levels outside right now in Baltimore are considered unhealthy. And most of that is due to what's called particulate matter, which are tiny particles, microscopic smaller than the width of your hair that can get into your lungs and impact your respiratory system, your cardiovascular system, and even your neurological your brain. What makes this particularly harmful? Is it the volume of particulant?",
          labels: [
            {
              relevance: 0.9955303072929382,
              label: "Home&Garden>IndoorEnvironmentalQuality",
            },
            {
              relevance: 0.028595492243766785,
              label:
                "MedicalHealth>DiseasesAndConditions>LungAndRespiratoryHealth",
            },
            {
              relevance: 0.003357736626639962,
              label: "NewsAndPolitics>Weather",
            },
            {
              relevance: 0.0007577401702292264,
              label: "MedicalHealth>DiseasesAndConditions>ColdAndFlu",
            },
            {
              relevance: 0.0007211097399704158,
              label: "MedicalHealth>DiseasesAndConditions>Allergies",
            },
            {
              relevance: 0.0006683538085781038,
              label: "MedicalHealth>DiseasesAndConditions>Injuries",
            },
            {
              relevance: 0.0005227243527770042,
              label: "HealthyLiving",
            },
            {
              relevance: 0.00036367119173519313,
              label: "Technology&Computing>Computing>ComputerNetworking",
            },
            {
              relevance: 0.0003151895070914179,
              label: "HealthyLiving>Wellness>SmokingCessation",
            },
            {
              relevance: 0.0002955370000563562,
              label: "RealEstate>OfficeProperty",
            },
          ],
          timestamp: {
            start: 56276,
            end: 88034,
          },
        },
        {
          text: "Is it something in particular? What is it exactly? Can you just drill down on that a little bit more? Yeah. So the concentration of particulate matter I was looking at some of the monitors that we have was reaching levels of what are, in science, big 150 micrograms per meter cubed, which is more than ten times what the annual average should be and about four times higher than what you're supposed to have on a 24 hours average.",
          labels: [
            {
              relevance: 0.9881851673126221,
              label: "Home&Garden>IndoorEnvironmentalQuality",
            },
            {
              relevance: 0.0010195717914029956,
              label:
                "MedicalHealth>DiseasesAndConditions>LungAndRespiratoryHealth",
            },
            {
              relevance: 0.0005706266965717077,
              label: "MedicalHealth>DiseasesAndConditions>ColdAndFlu",
            },
            {
              relevance: 0.0003804410225711763,
              label: "NewsAndPolitics>Weather",
            },
            {
              relevance: 0.0003401130379643291,
              label:
                "MedicalHealth>DiseasesAndConditions>HeartAndCardiovascularDiseases",
            },
            {
              relevance: 0.0003019376308657229,
              label: "MedicalHealth>DiseasesAndConditions>Allergies",
            },
            {
              relevance: 0.000281211658148095,
              label: "HealthyLiving",
            },
            {
              relevance: 0.00027685757959261537,
              label: "Home&Garden>Remodeling&Construction",
            },
            {
              relevance: 0.0002556040126364678,
              label: "BusinessAndFinance>Business>ConsumerIssues>Recalls",
            },
            {
              relevance: 0.00019348932255525142,
              label: "RealEstate>OfficeProperty",
            },
          ],
          timestamp: {
            start: 88082,
            end: 113258,
          },
        },
        {
          text: "And so the concentrations of these particles in the air are just much, much higher than we typically see. And exposure to those high levels can lead to a host of health problems. And who is most vulnerable? I noticed that in New York City, for example, they're canceling outdoor activities. And so here it is in the early days of summer, and they have to keep all the kids inside. So who tends to be vulnerable in a situation like this? It's the youngest.",
          labels: [
            {
              relevance: 0.9975869655609131,
              label: "Home&Garden>IndoorEnvironmentalQuality",
            },
            {
              relevance: 0.10198913514614105,
              label: "NewsAndPolitics>Weather",
            },
            {
              relevance: 0.025256866589188576,
              label: "HealthyLiving",
            },
            {
              relevance: 0.004600986372679472,
              label:
                "MedicalHealth>DiseasesAndConditions>LungAndRespiratoryHealth",
            },
            {
              relevance: 0.004249158315360546,
              label: "MedicalHealth>DiseasesAndConditions>ColdAndFlu",
            },
            {
              relevance: 0.002624025335535407,
              label: "Travel>TravelLocations>PolarTravel",
            },
            {
              relevance: 0.0014258016599342227,
              label: "NewsAndPolitics>Disasters",
            },
            {
              relevance: 0.0014156353427097201,
              label: "MedicalHealth>DiseasesAndConditions>MentalHealth",
            },
            {
              relevance: 0.0011561078717932105,
              label: "MedicalHealth>DiseasesAndConditions>Allergies",
            },
            {
              relevance: 0.0010420052567496896,
              label: "MedicalHealth>DiseasesAndConditions>Injuries",
            },
          ],
          timestamp: {
            start: 113354,
            end: 138754,
          },
        },
        {
          text: "So children, obviously, whose bodies are still developing. The elderly, who are their bodies are more in decline and they're more susceptible to the health impacts of breathing, the poor air quality. And then people who have preexisting health conditions, people with respiratory conditions or heart conditions can be triggered by high levels of air pollution. Could this get worse? That's a good question. In some areas, it's much worse than others. And it just depends on kind of where the smoke is concentrated.",
          labels: [
            {
              relevance: 0.9996503591537476,
              label: "Home&Garden>IndoorEnvironmentalQuality",
            },
            {
              relevance: 0.6170681118965149,
              label:
                "MedicalHealth>DiseasesAndConditions>LungAndRespiratoryHealth",
            },
            {
              relevance: 0.02062302641570568,
              label: "HealthyLiving",
            },
            {
              relevance: 0.010274156928062439,
              label:
                "MedicalHealth>DiseasesAndConditions>HeartAndCardiovascularDiseases",
            },
            {
              relevance: 0.007371500600129366,
              label: "HealthyLiving>Wellness>SmokingCessation",
            },
            {
              relevance: 0.005629349499940872,
              label: "MedicalHealth>DiseasesAndConditions>Injuries",
            },
            {
              relevance: 0.0033619196619838476,
              label: "MedicalHealth>DiseasesAndConditions>Allergies",
            },
            {
              relevance: 0.0032199521083384752,
              label: "MedicalHealth>DiseasesAndConditions>Cancer",
            },
            {
              relevance: 0.002642817562445998,
              label: "MedicalHealth>DiseasesAndConditions>ColdAndFlu",
            },
            {
              relevance: 0.0024871069472283125,
              label: "NewsAndPolitics>Weather",
            },
          ],
          timestamp: {
            start: 138802,
            end: 170370,
          },
        },
        {
          text: "I think New York has some of the higher concentrations right now, but that's going to change as that air moves away from the New York area. But over the course of the next few days, we will see different areas being hit at different times with the highest concentrations. I was going to ask you about more fires start burning. I don't expect the concentrations to go up too much higher.",
          labels: [
            {
              relevance: 0.9051397442817688,
              label: "NewsAndPolitics>Weather",
            },
            {
              relevance: 0.05779021233320236,
              label: "Home&Garden>IndoorEnvironmentalQuality",
            },
            {
              relevance: 0.012258341535925865,
              label: "NewsAndPolitics>Disasters",
            },
            {
              relevance: 0.001360786729492247,
              label: "MedicalHealth>DiseasesAndConditions>ColdAndFlu",
            },
            {
              relevance: 0.0003114799619652331,
              label:
                "MedicalHealth>DiseasesAndConditions>LungAndRespiratoryHealth",
            },
            {
              relevance: 0.00030524705653078854,
              label: "Automotive>AutoParts",
            },
            {
              relevance: 0.0002636107965372503,
              label: "Travel>TravelLocations>PolarTravel",
            },
            {
              relevance: 0.00025970168644562364,
              label: "MedicalHealth>DiseasesAndConditions>Cancer",
            },
            {
              relevance: 0.00023528788005933166,
              label: "Automotive>AutoType>PerformanceCars",
            },
            {
              relevance: 0.00022414950944948941,
              label: "BusinessAndFinance>Industries>PowerAndEnergyIndustry",
            },
          ],
          timestamp: {
            start: 170950,
            end: 189030,
          },
        },
        {
          text: "I was going to ask you how and you started to answer this, but how much longer could this last? Or forgive me if I'm asking you to speculate, but what do you think? Well, I think the fires are going to burn for a little bit longer, but the key for us in the US. Is the weather system changing. And so right now, it's kind of the weather systems that are pulling that air into our mid Atlantic and Northeast region.",
          labels: [
            {
              relevance: 0.9955629110336304,
              label: "NewsAndPolitics>Weather",
            },
            {
              relevance: 0.008395846001803875,
              label: "Travel>TravelLocations>PolarTravel",
            },
            {
              relevance: 0.0024605081416666508,
              label: "NewsAndPolitics>Disasters",
            },
            {
              relevance: 0.0005194907425902784,
              label: "Science>Environment",
            },
            {
              relevance: 0.00028931227279827,
              label: "MedicalHealth>DiseasesAndConditions>ColdAndFlu",
            },
            {
              relevance: 0.0002018982922891155,
              label: "Sports>CollegeSports>CollegeBaseball",
            },
            {
              relevance: 0.00017360984929837286,
              label: "PopCulture>CelebrityScandal",
            },
            {
              relevance: 0.0001712092343950644,
              label: "Sports>OlympicSports>SummerOlympicSports",
            },
            {
              relevance: 0.00015698879724368453,
              label: "MusicAndAudio>AdultContemporaryMusic>UrbanACMusic",
            },
            {
              relevance: 0.0001443001820007339,
              label: "BusinessAndFinance>Economy>FinancialCrisis",
            },
          ],
          timestamp: {
            start: 189100,
            end: 211082,
          },
        },
        {
          text: "As those weather systems change and shift, we'll see that smoke going elsewhere and not impact us in this region as much. And so I think that's going to be the defining factor. And I think the next couple of days we're going to see a shift in that weather pattern and start to push the smoke away from where we are. And finally, with the impacts of climate change, we are seeing more wildfires.",
          labels: [
            {
              relevance: 0.9943384528160095,
              label: "NewsAndPolitics>Weather",
            },
            {
              relevance: 0.00387981696985662,
              label: "Travel>TravelLocations>PolarTravel",
            },
            {
              relevance: 0.0031615248881280422,
              label: "NewsAndPolitics>Disasters",
            },
            {
              relevance: 0.0019596796482801437,
              label: "Home&Garden>IndoorEnvironmentalQuality",
            },
            {
              relevance: 0.001271517714485526,
              label: "Science>Environment",
            },
            {
              relevance: 0.00032604511943645775,
              label: "PopCulture>CelebrityScandal",
            },
            {
              relevance: 0.00028069576364941895,
              label: "BusinessAndFinance>Industries>PowerAndEnergyIndustry",
            },
            {
              relevance: 0.00025750460918061435,
              label: "Technology&Computing>Computing",
            },
            {
              relevance: 0.0001956245250767097,
              label: "Sports>AutoRacing>MotorcycleSports",
            },
            {
              relevance: 0.00019082955259364098,
              label: "Sports>OlympicSports>SummerOlympicSports",
            },
          ],
          timestamp: {
            start: 211146,
            end: 232354,
          },
        },
        {
          text: "Will we be seeing more of these kinds of wide ranging air quality consequences or circumstances? I mean, that is one of the predictions for climate change. Looking into the future, the fire season is starting earlier and lasting longer, and we're seeing more frequent fires. So, yeah, this is probably something that we'll be seeing more frequently. This tends to be much more of an issue in the Western US. So the eastern US. Getting hit right now is a little bit new.",
          labels: [
            {
              relevance: 0.9948033690452576,
              label: "NewsAndPolitics>Weather",
            },
            {
              relevance: 0.12892243266105652,
              label: "Home&Garden>IndoorEnvironmentalQuality",
            },
            {
              relevance: 0.04659998416900635,
              label: "Travel>TravelLocations>PolarTravel",
            },
            {
              relevance: 0.023162951692938805,
              label: "NewsAndPolitics>Disasters",
            },
            {
              relevance: 0.0044741383753716946,
              label: "Science>Environment",
            },
            {
              relevance: 0.0010589624289423227,
              label: "MedicalHealth>DiseasesAndConditions>ColdAndFlu",
            },
            {
              relevance: 0.0009752486366778612,
              label:
                "MedicalHealth>DiseasesAndConditions>LungAndRespiratoryHealth",
            },
            {
              relevance: 0.0008771885768510401,
              label: "BusinessAndFinance>Industries>PowerAndEnergyIndustry",
            },
            {
              relevance: 0.0005613958346657455,
              label: "Food&Drink>Cooking",
            },
            {
              relevance: 0.0005496227531693876,
              label: "MedicalHealth>DiseasesAndConditions>Cancer",
            },
          ],
          timestamp: {
            start: 232482,
            end: 261760,
          },
        },
        {
          text: "But yeah, I think with climate change moving forward, this is something that is going to happen more frequently. That's Peter De Carlo, associate professor in the Department of Environmental Health and Engineering at Johns Hopkins University. Sergeant Carlo, thanks so much for joining us and sharing this expertise with us. Thank you for having me.",
          labels: [
            {
              relevance: 0.9194307923316956,
              label: "Science>Environment",
            },
            {
              relevance: 0.8289446234703064,
              label:
                "BusinessAndFinance>Industries>EnvironmentalServicesIndustry",
            },
            {
              relevance: 0.361684650182724,
              label: "BusinessAndFinance>Business>GreenSolutions",
            },
            {
              relevance: 0.21688857674598694,
              label: "NewsAndPolitics>Weather",
            },
            {
              relevance: 0.05395161733031273,
              label: "Home&Garden>IndoorEnvironmentalQuality",
            },
            {
              relevance: 0.05076828971505165,
              label: "NewsAndPolitics>Disasters",
            },
            {
              relevance: 0.004510390106588602,
              label: "BusinessAndFinance>Industries>PowerAndEnergyIndustry",
            },
            {
              relevance: 0.003953366074711084,
              label: "Travel>TravelLocations>PolarTravel",
            },
            {
              relevance: 0.0007138398941606283,
              label: "EventsAndAttractions>Parks&Nature",
            },
            {
              relevance: 0.0006015477702021599,
              label: "Travel>TravelType>BeachTravel",
            },
          ],
          timestamp: {
            start: 262130,
            end: 280340,
          },
        },
      ],
      summary: {
        "NewsAndPolitics>Weather": 1.0,
        "Home&Garden>IndoorEnvironmentalQuality": 0.9043831825256348,
        "Science>Environment": 0.16117265820503235,
        "BusinessAndFinance>Industries>EnvironmentalServicesIndustry": 0.14393523335456848,
        "MedicalHealth>DiseasesAndConditions>LungAndRespiratoryHealth": 0.11401086300611496,
        "BusinessAndFinance>Business>GreenSolutions": 0.06348437070846558,
        "NewsAndPolitics>Disasters": 0.05041387677192688,
        "Travel>TravelLocations>PolarTravel": 0.01308488193899393,
        HealthyLiving: 0.008222488686442375,
        "MedicalHealth>DiseasesAndConditions>ColdAndFlu": 0.0022315620444715023,
        "MedicalHealth>DiseasesAndConditions>HeartAndCardiovascularDiseases": 0.00213034451007843,
        "HealthyLiving>Wellness>SmokingCessation": 0.001540527562610805,
        "MedicalHealth>DiseasesAndConditions>Injuries": 0.0013950627762824297,
        "BusinessAndFinance>Industries>PowerAndEnergyIndustry": 0.0012570273829624057,
        "MedicalHealth>DiseasesAndConditions>Cancer": 0.001097781932912767,
        "MedicalHealth>DiseasesAndConditions>Allergies": 0.0010148967849090695,
        "MedicalHealth>DiseasesAndConditions>MentalHealth": 0.000717321818228811,
        "Style&Fashion>PersonalCare>DeodorantAndAntiperspirant": 0.0006022014422342181,
        "Technology&Computing>Computing>ComputerNetworking": 0.0005461975233629346,
        "MedicalHealth>DiseasesAndConditions>Injuries>FirstAid": 0.0004885646631009877,
      },
    },
  }}
/>


| Key                                                    | Type   | Description                                                                                                                                |
| ------------------------------------------------------ | ------ | ------------------------------------------------------------------------------------------------------------------------------------------ |
| `iab_categories_result`                                | object | The result of the Topic Detection model.                                                                                                   |
| `iab_categories_result.status`                         | string | Is either `success`, or `unavailable` in the rare case that the Content Moderation model failed.                                           |
| `iab_categories_result.results`                        | array  | An array of the Topic Detection results.                                                                                                   |
| `iab_categories_result.results[i].text`                | string | The text in the transcript in which the i-th instance of a detected topic occurs.                                                          |
| `iab_categories_result.results[i].labels[j].relevance` | number | How relevant the j-th detected topic is in the i-th instance of a detected topic.                                                          |
| `iab_categories_result.results[i].labels[j].label`     | string | The IAB taxonomical label for the j-th label of the i-th instance of a detected topic, where `>` denotes supertopic/subtopic relationship. |
| `iab_categories_result.results[i].timestamp.start`     | number | The starting time in the audio file at which the i-th detected topic instance is discussed.                                                |
| `iab_categories_result.results[i].timestamp.end`       | number | The ending time in the audio file at which the i-th detected topic instance is discussed.                                                  |
| `iab_categories_result.summary`                        | object | Summary where each property is a detected topic.                                                                                           |
| `iab_categories_result.summary.topic`                  | number | The overall relevance of <i>topic</i> to the entire audio file.                                                                            |

The response also includes the request parameters used to generate the transcript.

## Frequently asked questions

{" "}

<Accordion
  title="How does the Topic Detection model handle misspelled or unrecognized words?"
  theme="dark"
  iconColor="white"
>
  <p>
    The Topic Detection model uses natural language processing and machine
    learning to identify related words and phrases even if they are misspelled
    or unrecognized. However, the accuracy of the detection may depend on the
    severity of the misspelling or the obscurity of the word.
  </p>
</Accordion>

<Accordion
  title="Can I use the Topic Detection model to identify entities that aren't part of the IAB Taxonomy?"
  theme="dark"
  iconColor="white"
>
  <p>
    No, the Topic Detection model can only identify entities that are part of
    the IAB Taxonomy. The model is optimized for contextual targeting use cases,
    so using the predefined IAB categories ensures the most accurate results.
  </p>
</Accordion>

<Accordion
  title="Why am I not getting any topic predictions for my audio file?"
  theme="dark"
  iconColor="white"
>
  <p>
    There could be several reasons why you aren't getting any topic predictions
    for your audio file. One possible reason is that the audio file doesn't
    contain enough relevant content for the model to analyze. Additionally, the
    accuracy of the predictions may be affected by factors such as background
    noise, low-quality audio, or a low confidence threshold for topic detection.
    It's recommended to review and adjust the model's configuration parameters
    and to provide high-quality, relevant audio files for analysis.
  </p>
</Accordion>

<Accordion
  title="Why am I getting inaccurate or irrelevant topic predictions for my audio file?"
  theme="dark"
  iconColor="white"
>
  <p>
    There could be several reasons why you're getting inaccurate or irrelevant
    topic predictions for your audio file. One possible reason is that the audio
    file contains background noise or other non-relevant content that's
    interfering with the model's analysis. Additionally, the accuracy of the
    predictions may be affected by factors such as low-quality audio, a low
    confidence threshold for topic detection, or insufficient training data.
    It's recommended to review and adjust the model's configuration parameters,
    to provide high-quality, relevant audio files for analysis, and to consider
    adding additional training data to the model.
  </p>
</Accordion>

<Accordion
  title="Is AssemblyAI associated with IAB?"
  theme="dark"
  iconColor="white"
>
  <p>
    As of 2023, AssemblyAI is a partner with the Interactive Advertising Bureau
    (IAB), a certification and community for advertising across the internet.
    AssemblyAI built Topic Detection using the IAB Taxonomy, which is a
    blueprint of the approximately 700 topics used to categorize ads.
  </p>
</Accordion>


---
title: Auto Chapters
description: Automatically summarize your audio into chapters
---

import { LanguageTable } from "../../assets/components/LanguagesTable";

<Accordion title="Supported languages">
  <LanguageTable
    languages={[
      { name: "Global English", code: "en" },
      { name: "Australian English", code: "en_au" },
      { name: "British English", code: "en_uk" },
      { name: "US English", code: "en_us" },
    ]}
    columns={2}
  />
  <br />
</Accordion>

The Auto Chapters model summarizes audio data over time into chapters. Chapters makes it easy for users to navigate and find specific information.

Each chapter contains the following:

- Summary
- One-line gist
- Headline
- Start and end timestamps

<Warning title="Auto Chapters and Summarization">
  You can only enable one of the Auto Chapters and
  [Summarization](/docs/audio-intelligence/summarization) models in the same
  transcription.
</Warning>

## Quickstart

<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>
  
  Enable Auto Chapters by setting `auto_chapters` to `True` in the transcription config. `punctuate` must be enabled to use Auto Chapters (`punctuate` is enabled by default).

```python {8}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(auto_chapters=True)

transcript = aai.Transcriber().transcribe(audio_file, config)
print(f"Transcript ID:", transcript.id)

for chapter in transcript.chapters:
  print(f"{chapter.start}-{chapter.end}: {chapter.headline}")
```

  </Tab>
  <Tab language="python" title="Python" default>
  
   Enable Auto Chapters by setting `auto_chapters` to `True` in the JSON payload.

```python {19}
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./local_file.mp3", "rb") as f:
    response = requests.post(base_url + "/v2/upload",
                            headers=headers,
                            data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "auto_chapters": True
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

print(f"Transcript ID:", transcript_id)

while True:
    transcription_result = requests.get(polling_endpoint, headers=headers).json()

    if transcription_result['status'] == 'completed':

        for chapter in transcription_result['chapters']:
            print(f"{chapter['start']} - {chapter['end']}: {chapter['headline']}")
        break
    elif transcription_result['status'] == 'error':
        raise RuntimeError(f"Transcription failed: {transcription_result['error']}")
    else:
        time.sleep(3)
```

  </Tab>
  <Tab language="javascript-sdk" title="JavaScript SDK">
  
  Enable Auto Chapters by setting `auto_chapters` to `true` in the transcription config. `punctuate` must be enabled to use Auto Chapters (`punctuate` is enabled by default).

```javascript {12}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  auto_chapters: true,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);
  console.log("Transcript ID: ", transcript.id);

  for (const chapter of transcript.chapters) {
    console.log(`${chapter.start}-${chapter.end}: ${chapter.headline}`);
  }
};

run();
```

  </Tab>
  <Tab language="javascript" title="JavaScript">
  
  Enable Auto Chapters by setting `auto_chapters` to `true` in the JSON payload.

```javascript {19}
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  auto_chapters: true,
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
console.log("Transcript ID: ", transcriptId);

const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    for (const chapter of transcriptionResult.chapters) {
      console.log(`${chapter.start} - ${chapter.end}: ${chapter.headline}`);
    }
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  Enable Auto Chapters by setting `auto_chapters` to `true` in the JSON payload.

<Info>
  Most of these libraries are included by default, but on .NET Framework and
  Mono you need to reference the System.Net.Http library and install the
  [System.Net.Http.Json NuGet
  package](https://www.nuget.org/packages/System.Net.Http.Json).
</Info>

```csharp {53}
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Threading.Tasks;
using System.Text.Json.Serialization;
using System.Collections.Generic;

class Program
{
    static async Task Main(string[] args)
    {
        string baseUrl = "https://api.assemblyai.com";

        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Authorization =
                new AuthenticationHeaderValue("<YOUR_API_KEY>");

            string uploadUrl = await UploadFileAsync("./local_file.mp3", httpClient, baseUrl);

            var transcript = await CreateTranscriptWithAutoChaptersAsync(uploadUrl, httpClient, baseUrl);

            Console.WriteLine($"Transcript ID: {transcript.Id}");
            transcript = await WaitForTranscriptToProcessAndGetChapters(transcript, httpClient, baseUrl);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient, string baseUrl)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var fileContent = new StreamContent(fileStream))
        {
            fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            using (var response = await httpClient.PostAsync($"{baseUrl}/v2/upload", fileContent))
            {
                response.EnsureSuccessStatusCode();
                var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
                return jsonDoc.RootElement.GetProperty("upload_url").GetString();
            }
        }
    }

    static async Task<Transcript> CreateTranscriptWithAutoChaptersAsync(string audioUrl, HttpClient httpClient, string baseUrl)
    {
        var data = new
        {
            audio_url = audioUrl,
            auto_chapters = true
        };

        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync($"{baseUrl}/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcessAndGetChapters(Transcript transcript, HttpClient httpClient, string baseUrl)
    {
        string pollingEndpoint = $"{baseUrl}/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "completed":
                    // Process auto chapters results
                    if (transcript.Chapters != null)
                    {
                        foreach (var chapter in transcript.Chapters)
                        {
                            Console.WriteLine($"{chapter.Start} - {chapter.End}: {chapter.Headline}");
                        }
                    }

                    return transcript;

                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");

                default:
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
            }
        }
    }

    public class Transcript
    {
        [JsonPropertyName("id")]
        public string Id { get; set; }

        [JsonPropertyName("status")]
        public string Status { get; set; }

        [JsonPropertyName("text")]
        public string Text { get; set; }

        [JsonPropertyName("chapters")]
        public List<Chapter> Chapters { get; set; }

        [JsonPropertyName("error")]
        public string Error { get; set; }
    }

    public class Chapter
    {
        [JsonPropertyName("start")]
        public int Start { get; set; }

        [JsonPropertyName("end")]
        public int End { get; set; }

        [JsonPropertyName("headline")]
        public string Headline { get; set; }
    }
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  Enable Auto Chapters by setting `auto_chapters` to `true` in the JSON payload.

```ruby {23}
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "/my_audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
    "auto_chapters" => true
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    transcription_result['chapters'].each do |chapter|
      puts "#{chapter['start']} - #{chapter['end']}: #{chapter['headline']}"
    end
  break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    sleep(3)
  end
end
```

  </Tab>
 <Tab language="php" title="PHP">
  
  Enable Auto Chapters by setting `auto_chapters` to `true` in the JSON payload.

```php {30}
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "auto_chapters" => true,
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        foreach ($transcription_result['chapters'] as $chapter) {
            echo "{$chapter['start']} - {$chapter['end']}: {$chapter['headline']}\n";
        }
        break;
    }  else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

  </Tab>
</Tabs>

### Example output

```plain
250-28840: Smoke from hundreds of wildfires in Canada is triggering air quality alerts across US
29610-280340: High particulate matter in wildfire smoke can lead to serious health problems
```

<Tip title="Auto Chapters Using LeMUR">
  Check out this cookbook [Creating Chapter
  Summaries](/docs/guides/input-text-chapters)
  for an example of how to leverage LeMUR's custom text input parameter for
  chapter summaries.
</Tip>

## API reference

### Request

```bash {6}
curl https://api.assemblyai.com/v2/transcript \
--header "Authorization: <YOUR_API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "audio_url": "YOUR_AUDIO_URL",
  "auto_chapters": true
}'
```

| Key             | Type    | Description           |
| --------------- | ------- | --------------------- |
| `auto_chapters` | boolean | Enable Auto Chapters. |

### Response

<Json
  json={{
    chapters: [
      {
        summary:
          "Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines from Maine to Maryland to Minnesota are gray and smoggy. In some places, the air quality warnings include the warning to stay inside.",
        gist: "Smoggy air quality alerts across US",
        headline:
          "Smoke from hundreds of wildfires in Canada is triggering air quality alerts across US",
        start: 250,
        end: 28840,
      },
      {
        summary:
          "Air pollution levels in Baltimore are considered unhealthy. Exposure to high levels can lead to a host of health problems. With climate change, we are seeing more wildfires. Will we be seeing more of these kinds of wide ranging air quality consequences?",
        gist: "What is it about the conditions right now that have caused this round",
        headline:
          "High particulate matter in wildfire smoke can lead to serious health problems",
        start: 29610,
        end: 280340,
      },
    ],
  }}
/>


| Key                    | Type   | Description                                                                |
| ---------------------- | ------ | -------------------------------------------------------------------------- |
| `chapters`             | array  | An array of temporally sequential chapters for the audio file.             |
| `chapters[i].gist`     | string | An short summary in a few words of the content spoken in the i-th chapter. |
| `chapters[i].headline` | string | A single sentence summary of the content spoken during the i-th chapter.   |
| `chapters[i].summary`  | string | A one paragraph summary of the content spoken during the i-th chapter.     |
| `chapters[i].start`    | number | The starting time, in milliseconds, for the i-th chapter.                  |
| `chapters[i].end`      | number | The ending time, in milliseconds, for the i-th chapter.                    |

The response also includes the request parameters used to generate the transcript.

## Frequently asked questions

<Accordion
  title="Can I specify the number of chapters to be generated by the Auto Chapters model?"
  theme="dark"
  iconColor="white"
>

No, the number of chapters generated by the Auto Chapters model isn't configurable by the user. The model automatically segments the audio file into logical chapters as the topic of conversation changes.

</Accordion>

## Troubleshooting

<Accordion
  title="Why am I not getting any chapter predictions for my audio file?"
  theme="dark"
  iconColor="white"
>

One possible reason is that the audio file doesn't contain enough variety in topic or tone for the model to identify separate chapters. Another reason could be due to background noise or low-quality audio interfering with the model's analysis.

</Accordion>


---
title: Key Phrases
description: Label key phrases that are spoken in your audio
---

import { LanguageTable } from "../../assets/components/LanguagesTable";

<Accordion title="Supported languages">
  <LanguageTable
    languages={[
      { name: "Global English", code: "en" },
      { name: "Australian English", code: "en_au" },
      { name: "British English", code: "en_uk" },
      { name: "US English", code: "en_us" },
    ]}
    columns={2}
  />
  <br />
</Accordion>

The Key Phrases model identifies significant words and phrases in your transcript and lets you extract the most important concepts or highlights from your audio or video file.

## Quickstart

<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>
  
  Enable Key Phrases by setting `auto_highlights` to `True` in the transcription config.

```python {8}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(auto_highlights=True)

transcript = aai.Transcriber().transcribe(audio_file, config)
print(f"Transcript ID:", transcript.id)

for result in transcript.auto_highlights.results:
    print(f"Highlight: {result.text}, Count: {result.count}, Rank: {result.rank}, Timestamps: {result.timestamps}")
```

  </Tab>
  <Tab language="python" title="Python" default>
  
  Enable Key Phrases by setting `auto_highlights` to `True` in the JSON payload.

```python {19}
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./local_file.mp3", "rb") as f:
    response = requests.post(base_url + "/v2/upload",
                            headers=headers,
                            data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "auto_highlights": True
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

print(f"Transcript ID:", transcript_id)

while True:
    transcription_result = requests.get(polling_endpoint, headers=headers).json()

    if transcription_result['status'] == 'completed':
        for result in transcription_result['auto_highlights_result']['results']:
            print(f"Highlight: {result['text']}, Count: {result['count']}, Rank: {result['rank']}, Timestamps: {result['timestamps']}")
        break
    elif transcription_result['status'] == 'error':
        raise RuntimeError(f"Transcription failed: {transcription_result['error']}")
    else:
        time.sleep(3)
```

  </Tab>
  <Tab language="javascript-sdk" title="JavaScript SDK">
  
  Enable Key Phrases by setting `auto_highlights` to `true` in the transcription config.

```javascript {12}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  auto_highlights: true,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  for (const result of transcript.auto_highlights_result.results) {
    const timestamps = result.timestamps
      .map(({ start, end }) => `[Timestamp(start=${start}, end=${end})]`)
      .join(", ");
    console.log(
      `Highlight: ${result.text}, Count: ${result.count}, Rank ${result.rank}, Timestamps: ${timestamps}`
    );
  }
};

run();
```

  </Tab>
  <Tab language="javascript" title="JavaScript">
  
  Enable Key Phrases by setting `auto_highlights` to `true` in the JSON payload.

```javascript {19}
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  auto_highlights: true,
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
console.log("Transcript ID: ", transcriptId);

const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    for (const result of transcriptionResult.auto_highlights_result.results) {
      const timestamps = result.timestamps
        .map(({ start, end }) => `[Timestamp(start=${start}, end=${end})]`)
        .join(", ");
      console.log(
        `Highlight: ${result.text}, Count: ${result.count}, Rank: ${result.rank}, Timestamps: ${timestamps}`
      );
    }
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  Enable Key Phrases by setting `auto_highlights` to `true` in the JSON payload.

<Info>
  Most of these libraries are included by default, but on .NET Framework and
  Mono you need to reference the System.Net.Http library and install the
  [System.Net.Http.Json NuGet
  package](https://www.nuget.org/packages/System.Net.Http.Json).
</Info>

```csharp {54}
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Threading.Tasks;
using System.Text.Json.Serialization;
using System.Collections.Generic;
using System.Linq;

class Program
{
    static async Task Main(string[] args)
    {
        string baseUrl = "https://api.assemblyai.com";

        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Authorization =
                new AuthenticationHeaderValue("<YOUR_API_KEY>");

            string uploadUrl = await UploadFileAsync("./local_file.mp3", httpClient, baseUrl);

            var transcript = await CreateTranscriptWithAutoHighlightsAsync(uploadUrl, httpClient, baseUrl);

            Console.WriteLine($"Transcript ID: {transcript.Id}");
            transcript = await WaitForTranscriptToProcessAndGetHighlights(transcript, httpClient, baseUrl);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient, string baseUrl)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var fileContent = new StreamContent(fileStream))
        {
            fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            using (var response = await httpClient.PostAsync($"{baseUrl}/v2/upload", fileContent))
            {
                response.EnsureSuccessStatusCode();
                var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
                return jsonDoc.RootElement.GetProperty("upload_url").GetString();
            }
        }
    }

    static async Task<Transcript> CreateTranscriptWithAutoHighlightsAsync(string audioUrl, HttpClient httpClient, string baseUrl)
    {
        var data = new
        {
            audio_url = audioUrl,
            auto_highlights = true
        };

        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync($"{baseUrl}/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcessAndGetHighlights(Transcript transcript, HttpClient httpClient, string baseUrl)
    {
        string pollingEndpoint = $"{baseUrl}/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "completed":
                    // Process auto highlights results
                    if (transcript.AutoHighlightsResult != null && transcript.AutoHighlightsResult.Results != null)
                    {
                        foreach (var result in transcript.AutoHighlightsResult.Results)
                        {
                            var timestampInfo = result.Timestamps.Select(t => $"{t.Start}-{t.End}").ToList();
                            Console.WriteLine($"Highlight: {result.Text}, Count: {result.Count}, Rank: {result.Rank}, Timestamps: {string.Join(", ", timestampInfo)}");
                        }
                    }

                    return transcript;

                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");

                default:
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
            }
        }
    }

    public class Transcript
    {
        [JsonPropertyName("id")]
        public string Id { get; set; }

        [JsonPropertyName("status")]
        public string Status { get; set; }

        [JsonPropertyName("text")]
        public string Text { get; set; }

        [JsonPropertyName("auto_highlights_result")]
        public AutoHighlightsResult AutoHighlightsResult { get; set; }

        [JsonPropertyName("error")]
        public string Error { get; set; }
    }

    public class AutoHighlightsResult
    {
        [JsonPropertyName("status")]
        public string Status { get; set; }

        [JsonPropertyName("results")]
        public List<Highlight> Results { get; set; }
    }

    public class Highlight
    {
        [JsonPropertyName("text")]
        public string Text { get; set; }

        [JsonPropertyName("count")]
        public int Count { get; set; }

        [JsonPropertyName("rank")]
        public double Rank { get; set; }

        [JsonPropertyName("timestamps")]
        public List<TimestampRange> Timestamps { get; set; }
    }

    public class TimestampRange
    {
        [JsonPropertyName("start")]
        public int Start { get; set; }

        [JsonPropertyName("end")]
        public int End { get; set; }
    }
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  Enable Key Phrases by setting `auto_highlights` to `true` in the JSON payload.

```ruby {22}
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "/my_audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]
data = {
    "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
    "auto_highlights" => true
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    transcription_result['auto_highlights_result']['results'].each do |result|
      timestamps = (result['timestamps'].map do |timestamp|
        format(
          '[Timestamp(start=%<start>s, end=%<end>s)]',
          start: timestamp['start'],
          end: timestamp['end']
        )
      end).join(', ')

    printf(
      "Highlight: %<text>s, Count: %<count>d, Rank %<rank>.2f, Timestamps: %<timestamp>s\n",
      text: result['text'],
      count: result['count'],
      rank: result['rank'],
      timestamp: timestamps
    )
    end
  break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    sleep(3)
  end
end
```

  </Tab>
 <Tab language="php" title="PHP">
  
  Enable Key Phrases by setting `auto_highlights` to `true` in the JSON payload.

```php {30}
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "auto_highlights" => true,
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        foreach ($transcription_result["auto_highlights_result"]["results"] as $result) {
            $timestamps = array_map(function($timestamp) {
                return "[Timestamp(start={$timestamp["start"]}, end={$timestamp["end"]})]";
            }, $result["timestamps"]);

            $timestamps_string = implode(', ', $timestamps);

            echo "Highlight: {$result["text"]}, Count: {$result["count"]}, Rank: {$result["rank"]}, Timestamps: {$timestamps_string}\n";
        }
        break;
    }  else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

  </Tab>
</Tabs>

### Example output

```plain
Highlight: air quality alerts, Count: 1, Rank: 0.08, Timestamps: [Timestamp(start=3978, end=5114)]
Highlight: wide ranging air quality consequences, Count: 1, Rank: 0.08, Timestamps: [Timestamp(start=235388, end=238838)]
Highlight: more fires, Count: 1, Rank: 0.07, Timestamps: [Timestamp(start=184716, end=185186)]
...
```

## API reference

### Request

```bash {6}
curl https://api.assemblyai.com/v2/transcript \
--header "Authorization: <YOUR_API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "audio_url": "YOUR_AUDIO_URL",
  "auto_highlights": true
}'
```

| Key               | Type    | Description         |
| ----------------- | ------- | ------------------- |
| `auto_highlights` | boolean | Enable Key Phrases. |

### Response

<Json
  json={{
    auto_highlights_result: {
      status: "success",
      results: [
        {
          count: 1,
          rank: 0.08,
          text: "air quality alerts",
          timestamps: [
            {
              start: 3978,
              end: 5114,
            },
          ],
        },
        {
          count: 1,
          rank: 0.08,
          text: "wide ranging air quality consequences",
          timestamps: [
            {
              start: 235388,
              end: 238694,
            },
          ],
        },
        {
          count: 1,
          rank: 0.07,
          text: "more wildfires",
          timestamps: [
            {
              start: 230972,
              end: 232354,
            },
          ],
        },
        {
          count: 1,
          rank: 0.07,
          text: "air pollution",
          timestamps: [
            {
              start: 156004,
              end: 156910,
            },
          ],
        },
        {
          count: 3,
          rank: 0.07,
          text: "weather systems",
          timestamps: [
            {
              start: 47344,
              end: 47958,
            },
            {
              start: 205268,
              end: 205818,
            },
            {
              start: 211588,
              end: 213434,
            },
          ],
        },
        {
          count: 2,
          rank: 0.06,
          text: "high levels",
          timestamps: [
            {
              start: 121128,
              end: 121646,
            },
            {
              start: 155412,
              end: 155866,
            },
          ],
        },
        {
          count: 1,
          rank: 0.06,
          text: "health conditions",
          timestamps: [
            {
              start: 152138,
              end: 152666,
            },
          ],
        },
        {
          count: 2,
          rank: 0.06,
          text: "Peter de Carlo",
          timestamps: [
            {
              start: 18948,
              end: 19930,
            },
            {
              start: 268298,
              end: 269194,
            },
          ],
        },
        {
          count: 1,
          rank: 0.06,
          text: "New York City",
          timestamps: [
            {
              start: 125768,
              end: 126274,
            },
          ],
        },
        {
          count: 1,
          rank: 0.05,
          text: "respiratory conditions",
          timestamps: [
            {
              start: 152964,
              end: 153786,
            },
          ],
        },
        {
          count: 3,
          rank: 0.05,
          text: "New York",
          timestamps: [
            {
              start: 125768,
              end: 126034,
            },
            {
              start: 171448,
              end: 171938,
            },
            {
              start: 176008,
              end: 176322,
            },
          ],
        },
        {
          count: 3,
          rank: 0.05,
          text: "climate change",
          timestamps: [
            {
              start: 229548,
              end: 230230,
            },
            {
              start: 244576,
              end: 245162,
            },
            {
              start: 263348,
              end: 263950,
            },
          ],
        },
        {
          count: 1,
          rank: 0.05,
          text: "Johns Hopkins University Varsity",
          timestamps: [
            {
              start: 23972,
              end: 25490,
            },
          ],
        },
        {
          count: 1,
          rank: 0.05,
          text: "heart conditions",
          timestamps: [
            {
              start: 153988,
              end: 154506,
            },
          ],
        },
        {
          count: 1,
          rank: 0.05,
          text: "air quality warnings",
          timestamps: [
            {
              start: 12308,
              end: 13434,
            },
          ],
        },
      ],
    },
  }}
/>


| Key                                                     | Type   | Description                                                                                                                    |
| ------------------------------------------------------- | ------ | ------------------------------------------------------------------------------------------------------------------------------ |
| `auto_highlights_result`                                | object | The result of the Key Phrases model.                                                                                           |
| `auto_highlights_result.status`                         | string | Is either `success` or `unavailable` in the rare case that the Key Phrases model failed.                                       |
| `auto_highlights_result.results`                        | array  | A temporally-sequential array of key phrases.                                                                                  |
| `auto_highlights_result.results[i].count`               | number | The total number of times the i-th key phrase appears in the audio file.                                                       |
| `auto_highlights_result.results[i].rank`                | number | The total relevancy to the overall audio file of this key phrase. A greater number means that the key phrase is more relevant. |
| `auto_highlights_result.results[i].text`                | string | The text itself of the key phrase.                                                                                             |
| `auto_highlights_result.results[i].timestamps[j].start` | number | The starting time of the j-th appearance of the i-th key phrase.                                                               |
| `auto_highlights_result.results[i].timestamps[j].end`   | number | The ending time of the j-th appearance of the i-th key phrase.                                                                 |

The response also includes the request parameters used to generate the transcript.

## Frequently Asked Questions

<Accordion
  title="How does the Key Phrases model identify important phrases in my transcription?"
  theme="dark"
  iconColor="white"
>
  <p>
    The Key Phrases model uses natural language processing and machine learning
    algorithms to analyze the frequency and distribution of words and phrases in
    your transcription. The algorithm identifies key phrases based on their
    relevancy score, which takes into account factors such as the number of
    times a phrase occurs, the distance between occurrences, and the overall
    length of the transcription.
  </p>
</Accordion>

<Accordion
  title="What is the difference between the Key Phrases model and the Topic Detection model?"
  theme="dark"
  iconColor="white"
>
  <p>
    The Key Phrases model is designed to identify important phrases and words in
    your transcription, whereas the Topic Detection model is designed to
    categorize your transcription into predefined topics. While both models use
    natural language processing and machine learning algorithms, they have
    different goals and approaches to analyzing your text.
  </p>
</Accordion>

<Accordion
  title="Can the Key Phrases model handle misspelled or unrecognized words?"
  theme="dark"
  iconColor="white"
>
  <p>
    Yes, the Key Phrases model can handle misspelled or unrecognized words to
    some extent. However, the accuracy of the detection may depend on the
    severity of the misspelling or the obscurity of the word. It's recommended
    to provide high-quality, relevant audio files with accurate transcriptions
    for the best results.
  </p>
</Accordion>

<Accordion
  title="What are some limitations of the Key Phrases model?"
  theme="dark"
  iconColor="white"
>
  <p>
    Some limitations of the Key Phrases model include its limited understanding
    of context, which may lead to inaccuracies in identifying the most important
    phrases in certain cases, such as text with heavy use of jargon or idioms.
    Additionally, the model assigns higher scores to words or phrases that occur
    more frequently in the text, which may lead to an over-representation of
    common words and phrases that may not be as important in the context of the
    text. Finally, the Key Phrases model is a general-purpose algorithm that
    can't be easily customized or fine-tuned for specific domains, meaning it
    may not perform as well for specialized texts where certain keywords or
    concepts may be more important than others.
  </p>
</Accordion>

<Accordion
  title="How can I optimize the performance of the Key Phrases model?"
  theme="dark"
  iconColor="white"
>
  <p>
    To optimize the performance of the Key Phrases model, it's recommended to
    provide high-quality, relevant audio files with accurate transcriptions, to
    review and adjust the model's configuration parameters, such as the
    confidence threshold for key phrase detection, and to refer to the list of
    identified key phrases to guide the analysis. It may also be helpful to
    consider adding additional training data to the model or consulting with
    AssemblyAI support for further assistance.
  </p>
</Accordion>


---
title: PII Redaction
description: Redact PII that is spoken in your audio
---

import { LanguageTable } from "../../assets/components/LanguagesTable";

<Accordion title="Supported languages">
  <LanguageTable
    languages={[
      { name: "Global English", code: "en" },
      { name: "Australian English", code: "en_au" },
      { name: "British English", code: "en_uk" },
      { name: "US English", code: "en_us" },
      { name: "Spanish", code: "es" },
      { name: "French", code: "fr" },
      { name: "German", code: "de" },
      { name: "Italian", code: "it" },
      { name: "Portuguese", code: "pt" },
      { name: "Dutch", code: "nl" },
      { name: "Hindi", code: "hi" },
      { name: "Japanese", code: "ja" },
      { name: "Chinese", code: "zh" },
      { name: "Finnish", code: "fi" },
      { name: "Korean", code: "ko" },
      { name: "Polish", code: "pl" },
      { name: "Russian", code: "ru" },
      { name: "Turkish", code: "tr" },
      { name: "Ukrainian", code: "uk" },
      { name: "Vietnamese", code: "vi" },
      { name: "Afrikaans", code: "af" },
      { name: "Arabic", code: "ar" },
      { name: "Belarusian", code: "be" },
      { name: "Bulgarian", code: "bg" },
      { name: "Burmese", code: "my" },
      { name: "Catalan", code: "ca" },
      { name: "Croatian", code: "hr" },
      { name: "Czech", code: "cs" },
      { name: "Danish", code: "da" },
      { name: "Estonian", code: "et" },
      { name: "Georgian", code: "ka" },
      { name: "Greek", code: "el" },
      { name: "Hebrew", code: "he" },
      { name: "Hungarian", code: "hu" },
      { name: "Icelandic", code: "is" },
      { name: "Indonesian", code: "id" },
      { name: "Khmer", code: "km" },
      { name: "Latvian", code: "lv" },
      { name: "Lithuanian", code: "lt" },
      { name: "Luxembourgish", code: "lb" },
      { name: "Malay", code: "ms" },
      { name: "Norwegian", code: "no" },
      { name: "Persian", code: "fa" },
      { name: "Romanian", code: "ro" },
      { name: "Slovak", code: "sk" },
      { name: "Slovenian", code: "sl" },
      { name: "Swahili", code: "sw" },
      { name: "Swedish", code: "sv" },
      { name: "Tagalog", code: "tl" },
      { name: "Tamil", code: "ta" },
    ]}
    columns={4}
  />
  <br />
</Accordion>

The PII Redaction model lets you minimize sensitive information about individuals by automatically identifying and removing it from your transcript.

Personal Identifiable Information (PII) is any information that can be used to identify a person, such as a name, email address, or phone number.

When you enable the PII Redaction model, your transcript will look like this:

- With `hash` substitution: `Hi, my name is ####!`
- With `entity_name` substitution: `Hi, my name is [PERSON_NAME]!`

You can also [Create redacted audio files](#create-redacted-audio-files) to replace sensitive information with a beeping sound.

<Warning title="Redacted Properties">
  PII only redacts words in the `text` property. Properties from other features
  may still include PII, such as `entities` from [Entity
  Detection](/docs/audio-intelligence/entity-detection) or `summary` from
  [Summarization](/docs/audio-intelligence/summarization).
</Warning>

## Quickstart

<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>
  Enable PII Redaction on the `TranscriptionConfig` using the `set_redact_pii()`
method.

Set `policies` to specify the information you want to redact. For the full list of policies, see [PII policies](#pii-policies).

```python {8-15}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig().set_redact_pii(
    policies=[
        aai.PIIRedactionPolicy.person_name,
        aai.PIIRedactionPolicy.organization,
        aai.PIIRedactionPolicy.occupation,
    ],
    substitution=aai.PIISubstitutionPolicy.hash,
)

transcript = aai.Transcriber().transcribe(audio_file, config)
print(f"Transcript ID:", transcript.id)

print(transcript.text)
```

  </Tab>
  <Tab language="python" title="Python" default>
  
  Enable Topic Detection by setting `redact_pii` to `True` in the JSON payload.

Set `redact_pii_policies` to specify the information you want to redact. For the full list of policies, see [PII policies](#pii-policies).

Set `redact_pii_sub` to specify the replacement for redacted information.

```python {19-21}
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./local_file.mp3", "rb") as f:
    response = requests.post(base_url + "/v2/upload",
                            headers=headers,
                            data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "redact_pii": True,
    "redact_pii_policies": ["person_name", "organization", "occupation"],
    "redact_pii_sub": "hash"
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

print(f"Transcript ID:", transcript_id)

while True:
    transcription_result = requests.get(polling_endpoint, headers=headers).json()

    if transcription_result['status'] == 'completed':
        print(transcription_result['text'])
        break
    elif transcription_result['status'] == 'error':
        raise RuntimeError(f"Transcription failed: {transcription_result['error']}")
    else:
        time.sleep(3)
```

  </Tab>
  <Tab language="javascript-sdk" title="JavaScript SDK">
  
  Enable PII Redaction by setting `redact_pii` to `true` in the transcription
config.

Use `redact_pii_policies` to specify the information you want to
redact. For the full list of policies, see [PII policies](#pii-policies).

```javascript {12-14}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params: TranscribeParams = {
  audio: audioFile,
  redact_pii: true,
  redact_pii_policies: ["person_name", "organization", "occupation"],
  redact_pii_sub: "hash",
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  console.log(transcript.text);
};

run();
```

  </Tab>
  <Tab language="javascript" title="JavaScript">
  
    Enable Topic Detection by setting `redact_pii` to `true` in the JSON payload.

Set `redact_pii_policies` to specify the information you want to redact. For the full list of policies, see [PII policies](#pii-policies).

Set `redact_pii_sub` to specify the replacement for redacted information.

```javascript {19-21}
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  redact_pii: true,
  redact_pii_policies: ["person_name", "organization", "occupation"],
  redact_pii_sub: "hash",
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
console.log("Transcript ID: ", transcriptId);

const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  Enable Topic Detection by setting `redact_pii` to `true` in the JSON payload.

Set `redact_pii_policies` to specify the information you want to redact. For the full list of policies, see [PII policies](#pii-policies).

Set `redact_pii_sub` to specify the replacement for redacted information.

<Info>
  Most of these libraries are included by default, but on .NET Framework and
  Mono you need to reference the System.Net.Http library and install the
  [System.Net.Http.Json NuGet
  package](https://www.nuget.org/packages/System.Net.Http.Json).
</Info>

```csharp {53-55}
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Threading.Tasks;
using System.Text.Json.Serialization;
using System.Collections.Generic;

class Program
{
    static async Task Main(string[] args)
    {
        string baseUrl = "https://api.assemblyai.com";

        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Authorization =
                new AuthenticationHeaderValue("<YOUR_API_KEY>");

            string uploadUrl = await UploadFileAsync("./local_file.mp3", httpClient, baseUrl);

            var transcript = await CreateTranscriptWithPiiRedactionAsync(uploadUrl, httpClient, baseUrl);

            Console.WriteLine($"Transcript ID: {transcript.Id}");
            transcript = await WaitForTranscriptToProcessAndGetRedactedText(transcript, httpClient, baseUrl);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient, string baseUrl)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var fileContent = new StreamContent(fileStream))
        {
            fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            using (var response = await httpClient.PostAsync($"{baseUrl}/v2/upload", fileContent))
            {
                response.EnsureSuccessStatusCode();
                var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
                return jsonDoc.RootElement.GetProperty("upload_url").GetString();
            }
        }
    }

    static async Task<Transcript> CreateTranscriptWithPiiRedactionAsync(string audioUrl, HttpClient httpClient, string baseUrl)
    {
        var data = new
        {
            audio_url = audioUrl,
            redact_pii = true,
            redact_pii_policies = new[] { "person_name", "organization", "occupation" },
            redact_pii_sub = "hash"
        };

        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync($"{baseUrl}/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcessAndGetRedactedText(Transcript transcript, HttpClient httpClient, string baseUrl)
    {
        string pollingEndpoint = $"{baseUrl}/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "completed":
                    // Print the redacted transcript text
                    Console.WriteLine(transcript.Text);
                    return transcript;

                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");

                default:
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
            }
        }
    }

    public class Transcript
    {
        [JsonPropertyName("id")]
        public string Id { get; set; }

        [JsonPropertyName("status")]
        public string Status { get; set; }

        [JsonPropertyName("text")]
        public string Text { get; set; }

        [JsonPropertyName("error")]
        public string Error { get; set; }
    }
}
```

  </Tab>
<Tab language="ruby" title="Ruby">
  
  Enable Topic Detection by setting `redact_pii` to `true` in the JSON payload.

Set `redact_pii_policies` to specify the information you want to redact. For the full list of policies, see [PII policies](#pii-policies).

Set `redact_pii_sub` to specify the replacement for redacted information.

```ruby {23-25}
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "/my_audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
    "redact_pii" => true,
    "redact_pii_policies" => ["person_name", "organization", "occupation"],
    "redact_pii_sub" => "hash"
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

raise "API request failed with status #{response.code}: #{response.body}" unless response.is_a?(Net::HTTPSuccess)

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    puts transcription_result['text']
  break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    sleep(3)
  end
end
```

  </Tab>
  <Tab language="php" title="PHP">
  
    Enable Topic Detection by setting `redact_pii` to `true` in the JSON payload.

Set `redact_pii_policies` to specify the information you want to redact. For the full list of policies, see [PII policies](#pii-policies).

Set `redact_pii_sub` to specify the replacement for redacted information.

```php {30-32}
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "redact_pii" => true,
    "redact_pii_policies" => ["person_name", "organization", "occupation"],
    "redact_pii_sub" => "hash"
);
$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        echo $transcription_result['text'] . "\n";
        break;
    }  else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

  </Tab>
</Tabs>

### Example output

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts
throughout the US. Skylines from Maine to Maryland to Minnesota are gray and
smoggy. And in some places, the air quality warnings include the warning to stay
inside. We wanted to better understand what's happening here and why, so we
called ##### #######, an ######### ######### in the ########## ## #############
###### ### ########### at ##### ####### ##########. Good morning, #########.
Good morning. So what is it about the conditions right now that have caused this
round of wildfires to affect so many people so far away? Well, there's a couple
of things. The season has been pretty dry already, and then the fact that we're
getting hit in the US. Is because there's a couple of weather systems that ...
```

<Tip title="PII Redaction Using LeMUR">
  If you would like the option to use LeMUR for custom PII redaction, check out
  this guide [Redact PII from Text Using
  LeMUR](/docs/lemur/lemur-pii-redaction).
</Tip>

## Create redacted audio files

In addition to redacting sensitive information from the transcription text, you can also generate a version of the original audio file with the PII "beeped" out.

<Tip title="Redacted Audio Endpoint">

Retrieve the redacted audio file using the `transcript_id` for a transcript where `redact_pii_audio` was enabled during submission:

<EndpointRequestSnippet endpoint="GET /v2/transcript/:transcript_id/redacted-audio" />

_Note_: Redacted audio creation is not supported for the EU region. For more information refer to our [API Reference](/docs/api-reference/transcripts/get-redacted-audio)

</Tip>

<Note title="Webhooks and PII Audio Redaction">

If using webhooks with PII audio redaction enabled, you'll receive two webhook calls: the first when the redacted audio file is ready and the second when the request for transcription is completed. For more information about using webhooks, refer to our [webhook documentation](/docs/deployment/webhooks).

</Note>

<Tabs groupId="language">
  <Tab value="python-sdk" title="Python SDK" default>
    To create a redacted version of the audio file, use the `set_redact_pii()` method on the `TranscriptionConfig` with `redact_audio` to `True`.

Use `get_redacted_audio_url()` on the transcript to get the URL to the redacted audio file.

```python {7,12}
config = aai.TranscriptionConfig().set_redact_pii(
    policies=[
        aai.PIIRedactionPolicy.person_name,
        aai.PIIRedactionPolicy.organization,
        aai.PIIRedactionPolicy.occupation,
    ],
    redact_audio=True
)

transcript = aai.Transcriber().transcribe(audio_url, config)

print(transcript.get_redacted_audio_url())
```

  </Tab>
  <Tab value="python" title="Python" default>
    To create a redacted version of the audio file, set `redact_pii_audio` to `True` on the JSON payload.
    Set `redact_pii_audio_quality` to specify the quality of the redacted audio file.

Use the transcript ID to poll the [GET redacted audio endpoint](/docs/api-reference/transcripts/get-redacted-audio) every few seconds to check the status of the redacted audio. Once the status is `redacted_audio_ready`, you can retrieve the audio URL from the API response.

```python {6-7,11-22}
data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "redact_pii": True,
    "redact_pii_policies": ["person_name", "organization", "occupation"],
    "redact_pii_sub": "hash",
    "redact_pii_audio": True,
    "redact_pii_audio_quality": "wav" # Optional. Defaults to "mp3"
}

# ...
redacted_audio_polling_endpoint = base_url + "/v2/transcript/" + transcript_id + "/redacted-audio"

while True:
    redacted_audio_result = requests.get(redacted_audio_polling_endpoint, headers=headers).json()

    if redacted_audio_result['status'] == 'redacted_audio_ready':
        print(redacted_audio_result['redacted_audio_url'])
        break
    elif redacted_audio_result['status'] == 'error':
        raise RuntimeError(f"Transcription failed: {redacted_audio_result['error']}")
    else:
        time.sleep(3)
```

  </Tab>
  <Tab language="javascript-sdk" title="JavaScript SDK">

To create a redacted version of the audio file, set `redact_pii_audio` to
`true` in the transcription config. Use `redact_pii_audio_quality` to specify
the quality of the redacted audio file.

Use redactedAudio() on the transcript to get the URL to the redacted audio file.

```javascript {5-6, 12-14}
const params: TranscribeParams = {
  audio: audioUrl,
  redact_pii: true,
  redact_pii_policies: ["person_name", "organization", "occupation"],
  redact_pii_audio: true,
  redact_pii_audio_quality: "wav", // Optional. Defaults to "mp3"
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  const { status, redacted_audio_url } = await client.transcripts.redactedAudio(
    transcript.id
  );

  console.log(`Status: ${status}, Redacted audio URL: ${redacted_audio_url}`);
};

run();
```

You can also retrieve the redacted audio file itself using `redactedAudioFile()`.
The following code writes the redacted audio file to a local file, using `writeFile()` from Node.js.

```javascript
import fs from "fs/promises";

...

const audioFile = await client.transcripts.redactedAudioFile(transcript.id);
await fs.writeFile('./redacted-audio.wav', audioFile.body, 'binary');
```

  </Tab>
  <Tab value="javascript" title="JavaScript" default>
    To create a redacted version of the audio file, set `redact_pii_audio` to `true` on the JSON payload.
    Set `redact_pii_audio_quality` to specify the quality of the redacted audio file.

Use the transcript ID to poll the [GET redacted audio endpoint](/docs/api-reference/transcripts/get-redacted-audio) every few seconds to check the status of the redacted audio. Once the status is `redacted_audio_ready`, you can retrieve the audio URL from the API response.

```javascript {6-7,11,14-26}
const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  redact_pii: true,
  redact_pii_policies: ["person_name", "organization", "occupation"],
  redact_pii_sub: "hash",
  redact_pii_audio: true,
  redact_pii_audio_quality: "wav", // Optional. Defaults to "mp3"
};

// ...
const redactedAudioPollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}/redacted-audio`;

while (true) {
  const redactedAudioPollingResponse = await axios.get(
    redactedAudioPollingEndpoint,
    {
      headers: headers,
    }
  );
  const redactedAudioResult = redactedAudioPollingResponse.data;

  if (redactedAudioResult.status === "redacted_audio_ready") {
    console.log(redactedAudioResult.redacted_audio_url);
    break;
  } else if (redactedAudioResult.status === "error") {
    throw new Error(`Transcription failed: ${redactedAudioResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

  </Tab>
  <Tab value="csharp" title="C#" default>
    To create a redacted version of the audio file, set `redact_pii_audio` to `true` on the JSON payload.
    Set `redact_pii_audio_quality` to specify the quality of the redacted audio file.

Use the transcript ID to poll the [GET redacted audio endpoint](/docs/api-reference/transcripts/get-redacted-audio) every few seconds to check the status of the redacted audio. Once the status is `redacted_audio_ready`, you can retrieve the audio URL from the API response.

```csharp {7-8,29,35-63,67-77}
var data = new
{
    audio_url = audioUrl,
    redact_pii = true,
    redact_pii_policies = new[] { "person_name", "organization", "occupation" },
    redact_pii_sub = "hash",
    redact_pii_audio = true,
    redact_pii_audio_quality = "wav" // Optional. Defaults to "mp3"
};

// ...
  static async Task Main(string[] args)
  {
      string baseUrl = "https://api.assemblyai.com";

      using (var httpClient = new HttpClient())
      {
          httpClient.DefaultRequestHeaders.Authorization =
              new AuthenticationHeaderValue("<YOUR_API_KEY>");

          string uploadUrl = await UploadFileAsync("./local_file.mp3", httpClient, baseUrl);

          var transcript = await CreateTranscriptWithPiiRedactionAsync(uploadUrl, httpClient, baseUrl);

          Console.WriteLine($"Transcript ID: {transcript.Id}");
          transcript = await WaitForTranscriptToProcessAndGetRedactedText(transcript, httpClient, baseUrl);

          // Wait for the redacted audio
          string redactedAudioUrl = await WaitForRedactedAudioAsync(transcript.Id, httpClient, baseUrl);
      }
  }

// ...

static async Task<string> WaitForRedactedAudioAsync(string transcriptId, HttpClient httpClient, string baseUrl)
{
    string redactedAudioPollingEndpoint = $"{baseUrl}/v2/transcript/{transcriptId}/redacted-audio";

    while (true)
    {
        var pollingResponse = await httpClient.GetAsync(redactedAudioPollingEndpoint);
        var responseContent = await pollingResponse.Content.ReadAsStringAsync();

        var redactedAudioResult = JsonSerializer.Deserialize<RedactedAudioResult>(
            responseContent,
            new JsonSerializerOptions { PropertyNameCaseInsensitive = true }
        );

        if (redactedAudioResult.Status == "redacted_audio_ready")
        {
            Console.WriteLine(redactedAudioResult.RedactedAudioUrl);
            return redactedAudioResult.RedactedAudioUrl;
        }
        else if (redactedAudioResult.Status == "error")
        {
            throw new Exception($"Redacted audio failed: {redactedAudioResult.Error}");
        }
        else
        {
            await Task.Delay(TimeSpan.FromSeconds(3));
        }
    }
}

// ...

public class RedactedAudioResult
{
    [JsonPropertyName("status")]
    public string Status { get; set; }

    [JsonPropertyName("redacted_audio_url")]
    public string RedactedAudioUrl { get; set; }

    [JsonPropertyName("error")]
    public string Error { get; set; }
}
```

  </Tab>
  <Tab value="ruby" title="Ruby" default>
    To create a redacted version of the audio file, set `redact_pii_audio` to `true` on the JSON payload.
    Set `redact_pii_audio_quality` to specify the quality of the redacted audio file.

Use the transcript ID to poll the [GET redacted audio endpoint](/docs/api-reference/transcripts/get-redacted-audio) every few seconds to check the status of the redacted audio. Once the status is `redacted_audio_ready`, you can retrieve the audio URL from the API response.

```ruby {6-7,11,14-28}
data = {
    "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
    "redact_pii" => true,
    "redact_pii_policies" => ["person_name", "organization", "occupation"],
    "redact_pii_sub" => "hash",
    "redact_pii_audio" => true,
    "redact_pii_audio_quality" => "wav" # Optional. Defaults to "mp3"
}

# ...
redacted_audio_polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}/redacted-audio")

while true
  redacted_audio_polling_http = Net::HTTP.new(redacted_audio_polling_endpoint.host, redacted_audio_polling_endpoint.port)
  redacted_audio_polling_http.use_ssl = true
  redact_audio_polling_request = Net::HTTP::Get.new(redacted_audio_polling_endpoint.request_uri, headers)
  redacted_audio_polling_response = redacted_audio_polling_http.request(redact_audio_polling_request)

  redacted_audio_result = JSON.parse(redacted_audio_polling_response.body)

  if redacted_audio_result['status'] == 'redacted_audio_ready'
    puts redacted_audio_result['redacted_audio_url']
  break
  elsif redacted_audio_result['status'] == 'error'
    raise "Transcription failed: #{redacted_audio_result['error']}"
  else
    sleep(3)
  end
end
```

  </Tab>
    <Tab value="php" title="PHP" default>
    To create a redacted version of the audio file, set `redact_pii_audio` to `true` on the JSON payload.
    Set `redact_pii_audio_quality` to specify the quality of the redacted audio file.

Use the transcript ID to poll the [GET redacted audio endpoint](/docs/api-reference/transcripts/get-redacted-audio) every few seconds to check the status of the redacted audio. Once the status is `redacted_audio_ready`, you can retrieve the audio URL from the API response.

```php {6-7,11,14-28}
$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "redact_pii" => true,
    "redact_pii_policies" => ["person_name", "organization", "occupation"],
    "redact_pii_sub" => "hash",
    "redact_pii_audio" => true,
    "redact_pii_audio_quality" => "wav" // Optional. Defaults to "mp3"
);

# ...
$redacted_audio_polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id . "/redacted-audio";

while (true) {
    $redacted_audio_polling_response = curl_init($redacted_audio_polling_endpoint);

    curl_setopt($redacted_audio_polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($redacted_audio_polling_response, CURLOPT_RETURNTRANSFER, true);

    $redacted_audio_result = json_decode(curl_exec($redacted_audio_polling_response), true);

    if ($redacted_audio_result['status'] === "redacted_audio_ready") {
        echo $redacted_audio_result['redacted_audio_url'] . "\n";
        break;
    }  else if ($redacted_audio_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $redacted_audio_result['error']);
    } else {
        sleep(3);
    }
}
```

  </Tab>
</Tabs>

<Warning title="Maximum Audio File Size">
  You can only create redacted versions of audio files if the original file is
  smaller than 1 GB.
</Warning>

<Info title="Redacted Audio for Silent Files">
By default, audio redaction provides redacted audio URLs only when speech is detected. However, if your use-case specifically requires redacted audio files even for silent audio files without any dialogue, you can now opt to receive these URLs. Enable this by setting the optional parameter `"return_redacted_no_speech_audio": true` within `redact_pii_audio_options` in your `POST` request body.

<Accordion title="Example request body">
```json
{
    "audio_url": "YOUR_AUDIO_URL",
    "redact_pii": true,
    "redact_pii_audio": true,
    "redact_pii_audio_options": {
          "return_redacted_no_speech_audio": true
     },
     "redact_pii_policies": ["credit_card_number"]
}
```
</Accordion>

</Info>

### Example output

```plain
https://s3.us-west-2.amazonaws.com/api.assembly.ai.usw2/redacted-audio/ac06721c-d1ea-41a7-95f7-a9463421e6b1.mp3?AWSAccessKeyId=...
```

## API reference

### Request

```bash {6-10}
curl https://api.assemblyai.com/v2/transcript \
--header "Authorization: <YOUR_API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "audio_url": "YOUR_AUDIO_URL",
  "redact_pii": true,
  "redact_pii_policies": ["us_social_security_number", "credit_card_number"],
  "redact_pii_sub": "hash",
  "redact_pii_audio": true,
  "redact_pii_audio_quality": "mp3"
}'
```

| Key                        | Type    | Description                                                                      |
| -------------------------- | ------- | -------------------------------------------------------------------------------- |
| `redact_pii`               | boolean | Enable PII Redaction.                                                            |
| `redact_pii_policies`      | array   | [PII policies](#pii-policies) for what information to redact.                    |
| `redact_pii_sub`           | string  | Method used to substitute PII in the transcript. Can be `entity_name` or `hash`. |
| `redact_pii_audio`         | boolean | Create a [redacted version of the audio file](#create-redacted-audio-files).     |
| `redact_pii_audio_quality` | string  | Quality of the redacted PII audio file. Can be `mp3` or `wav`.                   |

### Response

<Json
  json={{
    text: "Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines from Maine to Maryland to Minnesota are gray and smoggy. And in some places, the air quality warnings include the warning to stay inside. We wanted to better understand what's happening here and why, so we called ##### #######, an ######### ######### in the ########## ## ############# ###### ### ########### at ##### ####### ##########. Good morning, #########. Good morning. So what is it about the conditions right now that have caused this round of wildfires to affect so many people so far away? Well, there's a couple of things. The season has been pretty dry already, and then the fact that we're getting hit in the US. Is because there's a couple of weather systems that are essentially channeling the smoke from those Canadian wildfires through Pennsylvania into the Mid Atlantic and the Northeast and kind of just dropping the smoke there. So what is it in this haze that makes it harmful? And I'm assuming it is is it is the levels outside right now in Baltimore are considered unhealthy. And most of that is due to what's called particulate matter, which are tiny particles, microscopic smaller than the width of your hair, that can get into your lungs and impact your respiratory system, your cardiovascular system, and even your neurological your brain. What makes this particularly harmful? Is it the volume of particulate? Is it something in particular? What is it exactly? Can you just drill down on that a little bit more? Yeah. So the concentration of particulate matter I was looking at some of the monitors that we have was reaching levels of what are, in ####### speak, 150 micrograms per meter cubed, which is more than ten times what the annual average should be, and about four times higher than what you're supposed to have on a 24 hours average. And so the concentrations of these particles in the air are just much, much higher than we typically see. And exposure to those high levels can lead to a host of health problems. And who is most vulnerable? I noticed that in New York City, for example, they're canceling outdoor activities, and so here it is in the early days of summer, and they have to keep all the kids inside. So who tends to be vulnerable in a situation like this? It's the youngest. So children, obviously, whose bodies are still developing. The elderly who know their bodies are more in decline, and they're more susceptible to the health impacts of breathing, the poor air quality. And then people who have preexisting health conditions, people with respiratory conditions or heart conditions can be triggered by high levels of air pollution. Could this get worse? That's a good in some areas, it's much worse than others. And it just depends on kind of where the smoke is concentrated. I think New York has some of the higher concentrations right now, but that's going to change as that air moves away from the New York area. But over the course of the next few days, we will see different areas being hit at different times with the highest concentrations. I was going to ask you, more fires start burning, I don't expect the concentrations to go up too much higher. I was going to ask you and you started to answer this, but how much longer could this last? Or forgive me if I'm asking you to speculate, but what do you think? Well, I think the fires are going to burn for a little bit longer, but the key for us in the US. Is the weather system changing. And so right now, it's kind of the weather systems that are pulling that air into our mid Atlantic and Northeast region. As those weather systems change and shift, we'll see that smoke going elsewhere and not impact us in this region as much. And so I think that's going to be the defining factor. And I think the next couple of days we're going to see a shift in that weather pattern and start to push the smoke away from where we are. And finally, with the impacts of climate change, we are seeing more wildfires. Will we be seeing more of these kinds of wide ranging air quality consequences or circumstances? I mean, that is one of the predictions for climate change. Looking into the future, the fire season is starting earlier and lasting longer and we're seeing more frequent fires. So, yeah, this is probably something that we'll be seeing more frequently. This tends to be much more of an issue in the Western US. So the Eastern US getting hit right now is a little bit new. But yeah, I think with climate change moving forward, this is something that is going to happen more frequently. That's ##### #######, ######### ######### in the ########## ## ############# ###### ### ########### at ##### ####### ##########. ######## #####, thanks so much for joining us and sharing this expertise with us. Thank you for having me.",
  }}
/>


| Key    | Type   | Description                   |
| ------ | ------ | ----------------------------- |
| `text` | string | Transcript with redacted PII. |

The response also includes the request parameters used to generate the transcript.

### Request for Redacted Audio

In the request URL, replace transcript_id with the ID of the transcript where `redact_pii_audio` is set to `true`.

```bash {6-10}
curl https://api.assemblyai.com/v2/transcript/transcript_id/redacted-audio \
--header "Authorization: <YOUR_API_KEY>"
```

### Response for Redacted Audio

<Json
  json={{
    status: "redacted_audio_ready",
    redacted_audio_url:
      "https://s3.us-west-2.amazonaws.com/api.assembly.ai.usw2/redacted-audio/785efd9e-0e20-45e1-967b-3db17770ed9f.wav?AWSAccessKeyId=aws-access-key0id&Signature=signature&x-amz-security-token=security-token&Expires=1698966551",
  }}
/>


| Key                  | Type   | Description                         |
| -------------------- | ------ | ----------------------------------- |
| `status`             | string | The status of the redacted audio.   |
| `redacted_audio_url` | string | The URL of the redacted audio file. |

### PII policies

| Policy name                 | Description                                                                                                    | Example                                                          |
| --------------------------- | -------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- |
| `account_number`            | Customer account or membership identification number                                                           | `Policy No. 10042992; Member ID: HZ-5235-001`                    |
| `banking_information`       | Banking information, including account and routing numbers                                                     |                                                                  |
| `blood_type`                | Blood type                                                                                                     | O-, AB positive                                                  |
| `credit_card_cvv`           | Credit card verification code                                                                                  | CVV: 080                                                         |
| `credit_card_expiration`    | Expiration date of a credit card                                                                               |                                                                  |
| `credit_card_number`        | Credit card number                                                                                             |                                                                  |
| `date`                      | Specific calendar date                                                                                         | December 18                                                      |
| `date_of_birth`             | Date of birth                                                                                                  | Date of Birth: March 7, 1961                                     |
| `drivers_license`           | Driver's license number                                                                                        | DL# 356933-540                                                   |
| `drug`                      | Medications, vitamins, or supplements                                                                          | Advil, Acetaminophen, Panadol                                    |
| `email_address`             | Email address                                                                                                  | support@assemblyai.com                                           |
| `event`                     | Name of an event or holiday                                                                                    | Olympics, Yom Kippur                                             |
| `gender_sexuality`          | Terms indicating gender identity or sexual orientation, including slang terms                                  | female, bisexual, trans                                          |
| `healthcare_number`         | Healthcare numbers and health plan beneficiary numbers                                                         | Policy No.: 5584-486-674-YM                                      |
| `injury`                    | Bodily injury                                                                                                  | I broke my arm, I have a sprained wrist                          |
| `ip_address`                | Internet IP address, including IPv4 and IPv6 formats                                                           | 192.168.0.1                                                      |
| `language`                  | Name of a natural language                                                                                     | Spanish, French                                                  |
| `location`                  | Any Location reference including mailing address, postal code, city, state, province, country, or coordinates. | Lake Victoria, 145 Windsor St., 90210                            |
| `medical_condition`         | Name of a medical condition, disease, syndrome, deficit, or disorder                                           | chronic fatigue syndrome, arrhythmia, depression                 |
| `medical_process`           | Medical process, including treatments, procedures, and tests                                                   | heart surgery, CT scan                                           |
| `money_amount`              | Name and/or amount of currency                                                                                 | 15 pesos, $94.50                                                 |
| `nationality`               | Terms indicating nationality, ethnicity, or race                                                               | American, Asian, Caucasian                                       |
| `number_sequence`           | Numerical PII (including alphanumeric strings) that doesn't fall under other categories                        |                                                                  |
| `occupation`                | Job title or profession                                                                                        | professor, actors, engineer, CPA                                 |
| `organization`              | Name of an organization                                                                                        | CNN, McDonalds, University of Alaska, Northwest General Hospital |
| `passport_number`           | Passport numbers, issued by any country                                                                        | PA4568332, NU3C6L86S12                                           |
| `password`                  | Account passwords, PINs, access keys, or verification answers                                                  | 27%alfalfa, temp1234, My mother's maiden name is Smith           |
| `person_age`                | Number associated with an age                                                                                  | 27, 75                                                           |
| `person_name`               | Name of a person                                                                                               | Bob, Doug Jones, Dr. Kay Martinez, MD                            |
| `phone_number`              | Telephone or fax number                                                                                        |                                                                  |
| `political_affiliation`     | Terms referring to a political party, movement, or ideology                                                    | Republican, Liberal                                              |
| `religion`                  | Terms indicating religious affiliation                                                                         | Hindu, Catholic                                                  |
| `url`                       | Internet addresses                                                                                             | https://www.assemblyai.com/                                      |
| `us_social_security_number` | Social Security Number or equivalent                                                                           |                                                                  |
| `username`                  | Usernames, login names, or handles                                                                             | @AssemblyAI                                                      |
| `vehicle_id`                | Vehicle identification numbers (VINs), vehicle serial numbers, and license plate numbers                       | 5FNRL38918B111818, BIF7547                                       |

## Troubleshooting

<Accordion
  title="Why is the PII not redacted in my transcription?"
  theme="dark"
  iconColor="white"
>
  Make sure that at least one [PII policy](#pii-policies) has been specified in
  your request, using the `redact_pii_policies` parameter. If you're still
  experiencing issues, please reach out to our support team for assistance.
</Accordion>
<Accordion
  title="Why is my webhook not being sent?"
  theme="dark"
  iconColor="white"
>
  There could be several reasons why your webhook isn't being sent, such as a
  misconfigured URL, an unreachable endpoint, or an issue with the
  authentication headers. Double-check your request and ensure that the{" "}
  `webhook_url` parameter is included with a valid URL that can be reached by
  AssemblyAI's API. If you're using custom authentication headers, ensure that
  the `webhook_auth_header_name` and `webhook_auth_header_value` parameters are
  included and are correct. If you're still having issues, please contact our
  support team for assistance.
</Accordion>
<Accordion
  title="Why does my redacted audio file sound worse than the original?"
  theme="dark"
  iconColor="white"
>
  By default, the API returns redacted audio files in MP3 format, a lossy
  format. Lossy formats remove audio information to reduce file size, which may
  cause a reduction in quality. The difference may be particularly noticeable if
  the submitted audio is in a lossless file format. To retain as much quality as
  possible, you can instead return your redacted audio files in a lossless
  format, by setting `redact_pii_audio_quality` to `wav`.
</Accordion>


---
title: Overview
---

This page describes how to perform common operations with the REST API. Each endpoint is documented individually and grouped by the resource it interacts with, such as [Transcripts](/docs/api-reference/transcripts) and [LeMUR](/docs/api-reference/lemur).

The AssemblyAI API uses [REST](https://en.wikipedia.org/wiki/REST) with [JSON-encoded](https://www.json.org/json-en.html) request bodies and responses, and is available at the following URL:

```plain title="Base URL"
https://api.assemblyai.com
```

<Note>
  To use our EU servers, replace `api.assemblyai.com` with
  `api.eu.assemblyai.com`. The EU endpoint is available for **Async STT** and **LeMUR**. **Streaming STT** is not currently supported.
</Note>

<Info title="Streaming Speech-to-Text">
  This page explains the AssemblyAI REST API. If you want to use Streaming
  Speech-to-Text, see [Streaming API reference](/docs/api-reference/streaming-api/streaming-api).
</Info>

## Client SDKs

AssemblyAI provides official SDKs for popular programming languages, that make it simpler to interact with the API.

To get started using the SDKs, see the following resources:

- [Transcribe an audio file](https://www.assemblyai.com/docs/getting-started/transcribe-an-audio-file)
- [Apply LLMs to audio files using LeMUR](https://www.assemblyai.com/docs/lemur/apply-llms-to-audio-files).

## Authorization

To make authorized calls the REST API, your app must provide an authorization header with an API key. You can find your API key in the [AssemblyAI dashboard](https://www.assemblyai.com/app/api-keys).

```bash title="Authenticated request"
curl https://api.assemblyai.com/v2/transcript \
  --header 'Authorization: <YOUR_API_KEY>'
```

<Info title="Your API key">
The examples here contain a placeholder, `<YOUR_API_KEY>`, that you need to replace with your actual API key.
</Info>

## Errors

The AssemblyAI API uses HTTP response codes to indicate whether a request was successful.

The response codes generally fall into the following ranges:

- `2xx` indicates the request was successful.
- `4xx` indicates the request may have omitted a required parameter, or have invalid information.
- `5xx` indicates an error on AssemblyAI's end.

Below is a summary of the HTTP response codes you may encounter:

| Code          | Status            | Description                                                                                  |
| ------------- | ----------------- | -------------------------------------------------------------------------------------------- |
| 200           | OK                | Request was successful.                                                                      |
| 400           | Bad request       | The request failed due to an invalid request.                                                |
| 401           | Unauthorized      | Missing or invalid API key.                                                                  |
| 404           | Not found         | The requested resource doesn't exist.                                                        |
| 429           | Too many requests | Too many request were sent to the API. See [Rate limits](#rate-limits) for more information. |
| 500, 503, 504 | Server error      | Something went wrong on AssemblyAI's end.                                                    |

```json title="Response with error"
{
  "error": "Authentication error, API token missing/invalid"
}
```

<Tip title="API status">
  To stay up-to-date with any known service disruptions, subscribe to updates on
  the [Status](https://status.assemblyai.com) page.
</Tip>

### Failed transcriptions

Transcriptions may fail due to errors while processing the audio data.

When you query a transcription that has failed, the response will have a `200` code, along with `status` set to `error` and an `error` property with more details.

```json title="Failed transcription"
{
    "status": "error",
    "error": "Download error to https://foo.bar, 403 Client Error: Forbidden for url: https://foo.bar",
    ...
}
```

Common reasons why a transcription may fail include:

- Audio data is corrupted or in an unsupported format. See [FAQ](https://www.assemblyai.com/docs/concepts/faq) for supported formats.
- Audio URL is a webpage rather than a file.
- Audio URL isn't accessible from AssemblyAI's servers.
- Audio duration is too short (less than 160ms).

In the rare event of a transcription failure due to a server error, you may resubmit the file for transcription. If the problems persist after resubmitting, [let us know](mailto:support@assemblyai.com).

## Rate limits

To ensure the LeMUR API remains available for all users, you can only make a limited number of requests within a 60-second time window. Only LeMUR requests are rate limited.

If you exceed the limit, the API will respond with a `429` status code.

To see your remaining quota, check the following response headers:

| Header                  | Description                                                                                |
| ----------------------- | ------------------------------------------------------------------------------------------ |
| `X-RateLimit-Limit`     | Maximum number of allowed requests in a 60 second window.                                  |
| `X-RateLimit-Remaining` | Number of remaining requests in the current time window.                                   |
| `X-RateLimit-Reset`     | Number of seconds until the remaining requests resets to the value of `X-RateLimit-Limit`. |

If the response doesn't include `X-RateLimit` headers, the endpoint doesn't have rate limits.

<Info title="Increasing rate limits">
  If you want to increase the rate limit for your account, [contact
  us](mailto:support@assemblyai.com).
</Info>

## Pagination

Endpoints that support listing multiple resources use pagination to limit the number of results returned in a single response.

Paginated responses include a `page_details` JSON object with information about the results and links to navigate between pages.

| Property                       | Description                            |
| ------------------------------ | -------------------------------------- |
| `page_details[i].limit`        | Maximum number of resources in a page. |
| `page_details[i].result_count` | Total number of available resources.   |
| `page_details[i].current_url`  | URL to the current page.               |
| `page_details[i].prev_url`     | URL to the previous page.              |
| `page_details[i].next_url`     | URL to the next page.                  |

```json title="Paginated response"
{
  "page_details": {
    "limit": 1,
    "result_count": 1,
    "current_url": "https://api.assemblyai.com/v2/transcript?limit=1",
    "prev_url": "https://api.assemblyai.com/v2/transcript?limit=1&before_id=bfc3622e-8c69-4497-9a84-fb65b30dcb07",
    "next_url": "https://api.assemblyai.com/v2/transcript?limit=1&after_id=bfc3622e-8c69-4497-9a84-fb65b30dcb07"
  },
  "transcripts": [
    {
      // ...
    }
  ]
}
```

## Versioning

When AssemblyAI makes backwards-incompatible changes to the API, we release a new version. For information on API updates, see [Changelog](https://www.assemblyai.com/changelog).

Endpoints are versioned using a path prefix, such as `/v2`.
